{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10ff97e1",
   "metadata": {},
   "source": [
    "## Import everything that we need here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b3d2a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e2d2117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aadf0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Using cuda\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "print(use_cuda)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75989e7",
   "metadata": {},
   "source": [
    "## Create Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70d4b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.multiprocessing_env import SubprocVecEnv\n",
    "\n",
    "num_envs = 16\n",
    "env_name = \"Pendulum-v1\"\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name, render_mode=\"human\")\n",
    "        return env \n",
    "    return _thunk()\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "#envs = SubprocVecEnv(envs)\n",
    "\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6205948b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PassiveEnvChecker<PendulumEnv<Pendulum-v1>>>>>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86916544",
   "metadata": {},
   "source": [
    "## Defining the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85073036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    # print(f\"This is m : {m}\")\n",
    "    # print(f\"This is the type of m : {type(m)}\")\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_inputs,\n",
    "            num_outputs,\n",
    "            hidden_size,\n",
    "            std=0.0\n",
    "    ):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_outputs),\n",
    "        )\n",
    "\n",
    "        self.log_std = nn.Parameter(torch.ones(1, num_outputs) * std)\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x\n",
    "    ):\n",
    "        value = self.critic(x)\n",
    "        mu = self.actor(x)\n",
    "        std = self.log_std.exp().expand_as(mu)\n",
    "        dist = Normal(mu, std)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4d1ddfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1\n",
      "log_std: torch.Size([1, 1])\n",
      "critic.0.weight: torch.Size([256, 3])\n",
      "critic.0.bias: torch.Size([256])\n",
      "critic.2.weight: torch.Size([1, 256])\n",
      "critic.2.bias: torch.Size([1])\n",
      "actor.0.weight: torch.Size([256, 3])\n",
      "actor.0.bias: torch.Size([256])\n",
      "actor.2.weight: torch.Size([1, 256])\n",
      "actor.2.bias: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "num_input = env.observation_space.shape[0]\n",
    "num_output = env.action_space.shape[0]\n",
    "\n",
    "print(num_input, num_output)\n",
    "\n",
    "hidden_size      = 256\n",
    "lr               = 3e-4\n",
    "num_steps        = 20\n",
    "mini_batch_size  = 5\n",
    "ppo_epochs       = 4\n",
    "threshold_reward = -200\n",
    "\n",
    "model = ActorCritic(num_input, num_output, hidden_size).to(device)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dcc92a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1495, -0.9888, -0.0220]], device='cuda:0'), torch.Size([1, 3]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state, info = env.reset()\n",
    "env.render()\n",
    "state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "\n",
    "state, state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4e04b40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_experience(\n",
    "    env,\n",
    "    model,\n",
    "    num_steps\n",
    "):\n",
    "    pass\n",
    "\n",
    "    frames = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    values  = []\n",
    "    masks = []\n",
    "    states = []\n",
    "    log_probs = []\n",
    "    entropy = 0\n",
    "\n",
    "    state, info = env.reset()\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, value = model(state)\n",
    "\n",
    "        action = dist.sample()\n",
    "        next_state, reward, done, _, _ = env.step(action.cpu().numpy()) #if you pass in action.cpy().numpy()[0] we will get a scalar value or single value tuple (3,)\n",
    "\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy += dist.entropy().mean()\n",
    "\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        \n",
    "        state = np.array(next_state).flatten()\n",
    "\n",
    "        print('-' * 50)\n",
    "        print(f\"Step : {step}\")\n",
    "        print(f\"Current State : {state}\")\n",
    "        print(f\"Currrent Action : {action}\")\n",
    "        print(f\"Next State : {next_state}\")\n",
    "        \n",
    "        # The interesting part is how the reward is calculated\n",
    "        # reward is -(angle_cost + velocity_cost + action_cost)\n",
    "\n",
    "        # angle_cost = angle**2                      Penalty for being away from upright (0Â°)\n",
    "        # velocity_cost = 0.1 * angular_velocity**2  Penalty for moving too fast\n",
    "        # action_cost = 0.001 * action**2            Penalty for using large torques\n",
    "        print(f\"Reward : {reward}\")\n",
    "        \n",
    "        #frame = env.render()\n",
    "        #frames.append(frame)\n",
    "    \n",
    "    return (\n",
    "        frames,\n",
    "        rewards,\n",
    "        actions,\n",
    "        values,\n",
    "        masks,\n",
    "        states,\n",
    "        log_probs,\n",
    "        entropy,\n",
    "        next_state\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "be8becc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalized advtange exstimate is the total advantage that is expected at any particular time. Advtange is the reward that will be used to train the RL network\n",
    "def compute_gae(next_value, \n",
    "            rewards,\n",
    "            values, \n",
    "            masks,\n",
    "            gamma=0.9, \n",
    "            tau=0.95\n",
    "        ):\n",
    "    \n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    \n",
    "    for step in reversed(range(len(rewards))):\n",
    "\n",
    "        advantage_current_step = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "\n",
    "        # multi step advantage\n",
    "        gae = advantage_current_step + gamma * tau * masks[step] * gae\n",
    "\n",
    "        return_value = gae + values[step]\n",
    "\n",
    "        returns.insert(0, return_value)\n",
    "    \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "87b5727f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Step : 0\n",
      "Current State : [ 0.5180702 -0.8553381 -0.8166661]\n",
      "Currrent Action : tensor([[0.3133]], device='cuda:0')\n",
      "Next State : [[ 0.5180702]\n",
      " [-0.8553381]\n",
      " [-0.8166661]]\n",
      "Reward : [-0.976742]\n",
      "--------------------------------------------------\n",
      "Step : 1\n",
      "Current State : [ 0.45955065 -0.8881516  -1.3420826 ]\n",
      "Currrent Action : tensor([[0.7739]], device='cuda:0')\n",
      "Next State : [[ 0.45955065]\n",
      " [-0.8881516 ]\n",
      " [-1.3420826 ]]\n",
      "Reward : [-1.1203858]\n",
      "--------------------------------------------------\n",
      "Step : 2\n",
      "Current State : [ 0.37543204 -0.9268499  -1.8525264 ]\n",
      "Currrent Action : tensor([[1.0378]], device='cuda:0')\n",
      "Next State : [[ 0.37543204]\n",
      " [-0.9268499 ]\n",
      " [-1.8525264 ]]\n",
      "Reward : [-1.3765163]\n",
      "--------------------------------------------------\n",
      "Step : 3\n",
      "Current State : [ 0.26140964 -0.96522796 -2.407611  ]\n",
      "Currrent Action : tensor([[0.9337]], device='cuda:0')\n",
      "Next State : [[ 0.26140964]\n",
      " [-0.96522796]\n",
      " [-2.407611  ]]\n",
      "Reward : [-1.7504954]\n",
      "--------------------------------------------------\n",
      "Step : 4\n",
      "Current State : [ 0.10192695 -0.99479187 -3.2475603 ]\n",
      "Currrent Action : tensor([[-0.7735]], device='cuda:0')\n",
      "Next State : [[ 0.10192695]\n",
      " [-0.99479187]\n",
      " [-3.2475603 ]]\n",
      "Reward : [-2.2867138]\n",
      "--------------------------------------------------\n",
      "Step : 5\n",
      "Current State : [-0.10669855 -0.9942914  -4.180125  ]\n",
      "Currrent Action : tensor([[-1.2431]], device='cuda:0')\n",
      "Next State : [[-0.10669855]\n",
      " [-0.9942914 ]\n",
      " [-4.180125  ]]\n",
      "Reward : [-3.2132666]\n",
      "--------------------------------------------------\n",
      "Step : 6\n",
      "Current State : [-0.34591657 -0.93826526 -4.926269  ]\n",
      "Currrent Action : tensor([[-0.0028]], device='cuda:0')\n",
      "Next State : [[-0.34591657]\n",
      " [-0.93826526]\n",
      " [-4.926269  ]]\n",
      "Reward : [-4.5620165]\n",
      "--------------------------------------------------\n",
      "Step : 7\n",
      "Current State : [-0.6029712  -0.79776293 -5.8800964 ]\n",
      "Currrent Action : tensor([[-1.6675]], device='cuda:0')\n",
      "Next State : [[-0.6029712 ]\n",
      " [-0.79776293]\n",
      " [-5.8800964 ]]\n",
      "Reward : [-6.131415]\n",
      "--------------------------------------------------\n",
      "Step : 8\n",
      "Current State : [-0.8248069 -0.5654144 -6.452809 ]\n",
      "Currrent Action : tensor([[0.1707]], device='cuda:0')\n",
      "Next State : [[-0.8248069]\n",
      " [-0.5654144]\n",
      " [-6.452809 ]]\n",
      "Reward : [-8.377181]\n",
      "--------------------------------------------------\n",
      "Step : 9\n",
      "Current State : [-0.96636456 -0.25717607 -6.816737  ]\n",
      "Currrent Action : tensor([[0.4009]], device='cuda:0')\n",
      "Next State : [[-0.96636456]\n",
      " [-0.25717607]\n",
      " [-6.816737  ]]\n",
      "Reward : [-10.618974]\n",
      "--------------------------------------------------\n",
      "Step : 10\n",
      "Current State : [-0.99716014  0.07531071 -6.709619  ]\n",
      "Currrent Action : tensor([[2.5885]], device='cuda:0')\n",
      "Next State : [[-0.99716014]\n",
      " [ 0.07531071]\n",
      " [-6.709619  ]]\n",
      "Reward : [-12.953796]\n",
      "--------------------------------------------------\n",
      "Step : 11\n",
      "Current State : [-0.9206581   0.39036983 -6.513022  ]\n",
      "Currrent Action : tensor([[0.9341]], device='cuda:0')\n",
      "Next State : [[-0.9206581 ]\n",
      " [ 0.39036983]\n",
      " [-6.513022  ]]\n",
      "Reward : [-13.904422]\n",
      "--------------------------------------------------\n",
      "Step : 12\n",
      "Current State : [-0.7547912   0.65596515 -6.2885456 ]\n",
      "Currrent Action : tensor([[-0.4553]], device='cuda:0')\n",
      "Next State : [[-0.7547912 ]\n",
      " [ 0.65596515]\n",
      " [-6.2885456 ]]\n",
      "Reward : [-11.752821]\n",
      "--------------------------------------------------\n",
      "Step : 13\n",
      "Current State : [-0.53058     0.84763485 -5.921019  ]\n",
      "Currrent Action : tensor([[-0.8296]], device='cuda:0')\n",
      "Next State : [[-0.53058   ]\n",
      " [ 0.84763485]\n",
      " [-5.921019  ]]\n",
      "Reward : [-9.841388]\n",
      "--------------------------------------------------\n",
      "Step : 14\n",
      "Current State : [-0.2907413  0.9568017 -5.2856655]\n",
      "Currrent Action : tensor([[-0.0025]], device='cuda:0')\n",
      "Next State : [[-0.2907413]\n",
      " [ 0.9568017]\n",
      " [-5.2856655]]\n",
      "Reward : [-8.043094]\n",
      "--------------------------------------------------\n",
      "Step : 15\n",
      "Current State : [-0.07362182  0.99728626 -4.4262567 ]\n",
      "Currrent Action : tensor([[0.9454]], device='cuda:0')\n",
      "Next State : [[-0.07362182]\n",
      " [ 0.99728626]\n",
      " [-4.4262567 ]]\n",
      "Reward : [-6.275923]\n",
      "--------------------------------------------------\n",
      "Step : 16\n",
      "Current State : [ 0.11486963  0.99338055 -3.7762437 ]\n",
      "Currrent Action : tensor([[-0.6530]], device='cuda:0')\n",
      "Next State : [[ 0.11486963]\n",
      " [ 0.99338055]\n",
      " [-3.7762437 ]]\n",
      "Reward : [-4.6639314]\n",
      "--------------------------------------------------\n",
      "Step : 17\n",
      "Current State : [ 0.26537314  0.9641458  -3.0693483 ]\n",
      "Currrent Action : tensor([[-0.2543]], device='cuda:0')\n",
      "Next State : [[ 0.26537314]\n",
      " [ 0.9641458 ]\n",
      " [-3.0693483 ]]\n",
      "Reward : [-3.545049]\n",
      "--------------------------------------------------\n",
      "Step : 18\n",
      "Current State : [ 0.3624572  0.9320004 -2.046239 ]\n",
      "Currrent Action : tensor([[2.6420]], device='cuda:0')\n",
      "Next State : [[ 0.3624572]\n",
      " [ 0.9320004]\n",
      " [-2.046239 ]]\n",
      "Reward : [-2.6418285]\n",
      "--------------------------------------------------\n",
      "Step : 19\n",
      "Current State : [ 0.42983258  0.9029086  -1.4680876 ]\n",
      "Currrent Action : tensor([[-0.8057]], device='cuda:0')\n",
      "Next State : [[ 0.42983258]\n",
      " [ 0.9029086 ]\n",
      " [-1.4680876 ]]\n",
      "Reward : [-1.8591022]\n",
      "--------------------------------------------------\n",
      "Step : 20\n",
      "Current State : [ 0.46977475  0.88278633 -0.8945656 ]\n",
      "Currrent Action : tensor([[-0.6911]], device='cuda:0')\n",
      "Next State : [[ 0.46977475]\n",
      " [ 0.88278633]\n",
      " [-0.8945656 ]]\n",
      "Reward : [-1.484983]\n",
      "--------------------------------------------------\n",
      "Step : 21\n",
      "Current State : [ 0.4841839   0.87496626 -0.32788903]\n",
      "Currrent Action : tensor([[-0.6361]], device='cuda:0')\n",
      "Next State : [[ 0.4841839 ]\n",
      " [ 0.87496626]\n",
      " [-0.32788903]]\n",
      "Reward : [-1.2506354]\n",
      "--------------------------------------------------\n",
      "Step : 22\n",
      "Current State : [0.47788695 0.87842137 0.14364746]\n",
      "Currrent Action : tensor([[-1.2313]], device='cuda:0')\n",
      "Next State : [[0.47788695]\n",
      " [0.87842137]\n",
      " [0.14364746]]\n",
      "Reward : [-1.1472719]\n",
      "--------------------------------------------------\n",
      "Step : 23\n",
      "Current State : [0.43072227 0.90248454 1.0590936 ]\n",
      "Currrent Action : tensor([[1.7109]], device='cuda:0')\n",
      "Next State : [[0.43072227]\n",
      " [0.90248454]\n",
      " [1.0590936 ]]\n",
      "Reward : [-1.1553521]\n",
      "--------------------------------------------------\n",
      "Step : 24\n",
      "Current State : [0.36167312 0.932305   1.504621  ]\n",
      "Currrent Action : tensor([[-1.5422]], device='cuda:0')\n",
      "Next State : [[0.36167312]\n",
      " [0.932305  ]\n",
      " [1.504621  ]]\n",
      "Reward : [-1.3813052]\n",
      "--------------------------------------------------\n",
      "Step : 25\n",
      "Current State : [0.2610991 0.965312  2.1180284]\n",
      "Currrent Action : tensor([[-0.5721]], device='cuda:0')\n",
      "Next State : [[0.2610991]\n",
      " [0.965312 ]\n",
      " [2.1180284]]\n",
      "Reward : [-1.6684788]\n",
      "--------------------------------------------------\n",
      "Step : 26\n",
      "Current State : [0.1253423  0.99211353 2.769753  ]\n",
      "Currrent Action : tensor([[-0.4817]], device='cuda:0')\n",
      "Next State : [[0.1253423 ]\n",
      " [0.99211353]\n",
      " [2.769753  ]]\n",
      "Reward : [-2.1561344]\n",
      "--------------------------------------------------\n",
      "Step : 27\n",
      "Current State : [-0.05413411  0.99853367  3.5966716 ]\n",
      "Currrent Action : tensor([[0.5522]], device='cuda:0')\n",
      "Next State : [[-0.05413411]\n",
      " [ 0.99853367]\n",
      " [ 3.5966716 ]]\n",
      "Reward : [-2.8558414]\n",
      "--------------------------------------------------\n",
      "Step : 28\n",
      "Current State : [-0.2636749  0.9646116  4.2533875]\n",
      "Currrent Action : tensor([[-0.6146]], device='cuda:0')\n",
      "Next State : [[-0.2636749]\n",
      " [ 0.9646116]\n",
      " [ 4.2533875]]\n",
      "Reward : [-3.9344687]\n",
      "--------------------------------------------------\n",
      "Step : 29\n",
      "Current State : [-0.4930849   0.86998117  4.976042  ]\n",
      "Currrent Action : tensor([[-0.0054]], device='cuda:0')\n",
      "Next State : [[-0.4930849 ]\n",
      " [ 0.86998117]\n",
      " [ 4.976042  ]]\n",
      "Reward : [-5.1860023]\n",
      "--------------------------------------------------\n",
      "Step : 30\n",
      "Current State : [-0.7056947   0.70851606  5.35542   ]\n",
      "Currrent Action : tensor([[-1.8207]], device='cuda:0')\n",
      "Next State : [[-0.7056947 ]\n",
      " [ 0.70851606]\n",
      " [ 5.35542   ]]\n",
      "Reward : [-6.8325996]\n",
      "--------------------------------------------------\n",
      "Step : 31\n",
      "Current State : [-0.88041556  0.4742029   5.8666945 ]\n",
      "Currrent Action : tensor([[-0.1341]], device='cuda:0')\n",
      "Next State : [[-0.88041556]\n",
      " [ 0.4742029 ]\n",
      " [ 5.8666945 ]]\n",
      "Reward : [-8.410327]\n",
      "--------------------------------------------------\n",
      "Step : 32\n",
      "Current State : [-0.9829496   0.18387508  6.1826224 ]\n",
      "Currrent Action : tensor([[-0.2648]], device='cuda:0')\n",
      "Next State : [[-0.9829496 ]\n",
      " [ 0.18387508]\n",
      " [ 6.1826224 ]]\n",
      "Reward : [-10.451321]\n",
      "--------------------------------------------------\n",
      "Step : 33\n",
      "Current State : [-0.9932681  -0.11583839  6.020529  ]\n",
      "Currrent Action : tensor([[-2.0299]], device='cuda:0')\n",
      "Next State : [[-0.9932681 ]\n",
      " [-0.11583839]\n",
      " [ 6.020529  ]]\n",
      "Reward : [-12.568352]\n",
      "--------------------------------------------------\n",
      "Step : 34\n",
      "Current State : [-0.9207352  -0.39018813  5.6947355 ]\n",
      "Currrent Action : tensor([[-1.5928]], device='cuda:0')\n",
      "Next State : [[-0.9207352 ]\n",
      " [-0.39018813]\n",
      " [ 5.6947355 ]]\n",
      "Reward : [-12.780825]\n",
      "--------------------------------------------------\n",
      "Step : 35\n",
      "Current State : [-0.78865147 -0.6148405   5.2269545 ]\n",
      "Currrent Action : tensor([[-1.1676]], device='cuda:0')\n",
      "Next State : [[-0.78865147]\n",
      " [-0.6148405 ]\n",
      " [ 5.2269545 ]]\n",
      "Reward : [-10.756112]\n",
      "--------------------------------------------------\n",
      "Step : 36\n",
      "Current State : [-0.63292193 -0.7742156   4.465824  ]\n",
      "Currrent Action : tensor([[-3.0005]], device='cuda:0')\n",
      "Next State : [[-0.63292193]\n",
      " [-0.7742156 ]\n",
      " [ 4.465824  ]]\n",
      "Reward : [-8.883574]\n",
      "--------------------------------------------------\n",
      "Step : 37\n",
      "Current State : [-0.47453523 -0.8802365   3.817713  ]\n",
      "Currrent Action : tensor([[-0.4497]], device='cuda:0')\n",
      "Next State : [[-0.47453523]\n",
      " [-0.8802365 ]\n",
      " [ 3.817713  ]]\n",
      "Reward : [-7.084628]\n",
      "--------------------------------------------------\n",
      "Step : 38\n",
      "Current State : [-0.33766907 -0.94126487  2.9999309 ]\n",
      "Currrent Action : tensor([[-1.0507]], device='cuda:0')\n",
      "Next State : [[-0.33766907]\n",
      " [-0.94126487]\n",
      " [ 2.9999309 ]]\n",
      "Reward : [-5.7237816]\n",
      "--------------------------------------------------\n",
      "Step : 39\n",
      "Current State : [-0.23549193 -0.9718763   2.1342955 ]\n",
      "Currrent Action : tensor([[-1.0646]], device='cuda:0')\n",
      "Next State : [[-0.23549193]\n",
      " [-0.9718763 ]\n",
      " [ 2.1342955 ]]\n",
      "Reward : [-4.56922]\n",
      "--------------------------------------------------\n",
      "Step : 40\n",
      "Current State : [-0.1652131 -0.9862579  1.4350133]\n",
      "Currrent Action : tensor([[0.1975]], device='cuda:0')\n",
      "Next State : [[-0.1652131]\n",
      " [-0.9862579]\n",
      " [ 1.4350133]]\n",
      "Reward : [-3.7263093]\n",
      "--------------------------------------------------\n",
      "Step : 41\n",
      "Current State : [-0.13083799 -0.99140376  0.6951966 ]\n",
      "Currrent Action : tensor([[-0.0008]], device='cuda:0')\n",
      "Next State : [[-0.13083799]\n",
      " [-0.99140376]\n",
      " [ 0.6951966 ]]\n",
      "Reward : [-3.222298]\n",
      "--------------------------------------------------\n",
      "Step : 42\n",
      "Current State : [-0.14395985 -0.98958355 -0.26495218]\n",
      "Currrent Action : tensor([[-1.4440]], device='cuda:0')\n",
      "Next State : [[-0.14395985]\n",
      " [-0.98958355]\n",
      " [-0.26495218]]\n",
      "Reward : [-2.9472547]\n",
      "--------------------------------------------------\n",
      "Step : 43\n",
      "Current State : [-0.17885129 -0.9838761  -0.70713985]\n",
      "Currrent Action : tensor([[2.2448]], device='cuda:0')\n",
      "Next State : [[-0.17885129]\n",
      " [-0.9838761 ]\n",
      " [-0.70713985]]\n",
      "Reward : [-2.9531305]\n",
      "--------------------------------------------------\n",
      "Step : 44\n",
      "Current State : [-0.23676291 -0.97156745 -1.184277  ]\n",
      "Currrent Action : tensor([[1.7385]], device='cuda:0')\n",
      "Next State : [[-0.23676291]\n",
      " [-0.97156745]\n",
      " [-1.184277  ]]\n",
      "Reward : [-3.1176803]\n",
      "--------------------------------------------------\n",
      "Step : 45\n",
      "Current State : [-0.3225624 -0.9465482 -1.7880542]\n",
      "Currrent Action : tensor([[0.8327]], device='cuda:0')\n",
      "Next State : [[-0.3225624]\n",
      " [-0.9465482]\n",
      " [-1.7880542]]\n",
      "Reward : [-3.4164255]\n",
      "--------------------------------------------------\n",
      "Step : 46\n",
      "Current State : [-0.4360371 -0.8999287 -2.455103 ]\n",
      "Currrent Action : tensor([[0.2857]], device='cuda:0')\n",
      "Next State : [[-0.4360371]\n",
      " [-0.8999287]\n",
      " [-2.455103 ]]\n",
      "Reward : [-3.9268763]\n",
      "--------------------------------------------------\n",
      "Step : 47\n",
      "Current State : [-0.57306117 -0.8195126  -3.1809187 ]\n",
      "Currrent Action : tensor([[-0.3391]], device='cuda:0')\n",
      "Next State : [[-0.57306117]\n",
      " [-0.8195126 ]\n",
      " [-3.1809187 ]]\n",
      "Reward : [-4.6912985]\n",
      "--------------------------------------------------\n",
      "Step : 48\n",
      "Current State : [-0.7153291  -0.69878775 -3.7371743 ]\n",
      "Currrent Action : tensor([[0.3892]], device='cuda:0')\n",
      "Next State : [[-0.7153291 ]\n",
      " [-0.69878775]\n",
      " [-3.7371743 ]]\n",
      "Reward : [-5.76888]\n",
      "--------------------------------------------------\n",
      "Step : 49\n",
      "Current State : [-0.85068834 -0.5256704  -4.403969  ]\n",
      "Currrent Action : tensor([[-0.9514]], device='cuda:0')\n",
      "Next State : [[-0.85068834]\n",
      " [-0.5256704 ]\n",
      " [-4.403969  ]]\n",
      "Reward : [-7.0044613]\n",
      "--------------------------------------------------\n",
      "Step : 50\n",
      "Current State : [-0.953251   -0.30217966 -4.9304895 ]\n",
      "Currrent Action : tensor([[-0.8818]], device='cuda:0')\n",
      "Next State : [[-0.953251  ]\n",
      " [-0.30217966]\n",
      " [-4.9304895 ]]\n",
      "Reward : [-8.63848]\n",
      "--------------------------------------------------\n",
      "Step : 51\n",
      "Current State : [-0.9986022  -0.05285461 -5.081982  ]\n",
      "Currrent Action : tensor([[0.5009]], device='cuda:0')\n",
      "Next State : [[-0.9986022 ]\n",
      " [-0.05285461]\n",
      " [-5.081982  ]]\n",
      "Reward : [-10.466262]\n",
      "--------------------------------------------------\n",
      "Step : 52\n",
      "Current State : [-0.9799875   0.19905896 -5.065536  ]\n",
      "Currrent Action : tensor([[0.3739]], device='cuda:0')\n",
      "Next State : [[-0.9799875 ]\n",
      " [ 0.19905896]\n",
      " [-5.065536  ]]\n",
      "Reward : [-12.122945]\n",
      "--------------------------------------------------\n",
      "Step : 53\n",
      "Current State : [-0.9045934   0.42627543 -4.7994776 ]\n",
      "Currrent Action : tensor([[0.7784]], device='cuda:0')\n",
      "Next State : [[-0.9045934 ]\n",
      " [ 0.42627543]\n",
      " [-4.7994776 ]]\n",
      "Reward : [-11.2172]\n",
      "--------------------------------------------------\n",
      "Step : 54\n",
      "Current State : [-0.7920175   0.61049837 -4.326367  ]\n",
      "Currrent Action : tensor([[1.0227]], device='cuda:0')\n",
      "Next State : [[-0.7920175 ]\n",
      " [ 0.61049837]\n",
      " [-4.326367  ]]\n",
      "Reward : [-9.601144]\n",
      "--------------------------------------------------\n",
      "Step : 55\n",
      "Current State : [-0.6603977  0.750916  -3.8551643]\n",
      "Currrent Action : tensor([[0.0889]], device='cuda:0')\n",
      "Next State : [[-0.6603977]\n",
      " [ 0.750916 ]\n",
      " [-3.8551643]]\n",
      "Reward : [-8.046495]\n",
      "--------------------------------------------------\n",
      "Step : 56\n",
      "Current State : [-0.52149093  0.8532568  -3.455023  ]\n",
      "Currrent Action : tensor([[-1.0870]], device='cuda:0')\n",
      "Next State : [[-0.52149093]\n",
      " [ 0.8532568 ]\n",
      " [-3.455023  ]]\n",
      "Reward : [-6.7413397]\n",
      "--------------------------------------------------\n",
      "Step : 57\n",
      "Current State : [-0.4060705  0.9138418 -2.6089454]\n",
      "Currrent Action : tensor([[1.3742]], device='cuda:0')\n",
      "Next State : [[-0.4060705]\n",
      " [ 0.9138418]\n",
      " [-2.6089454]]\n",
      "Reward : [-5.6874366]\n",
      "--------------------------------------------------\n",
      "Step : 58\n",
      "Current State : [-0.32026643  0.9473275  -1.8427812 ]\n",
      "Currrent Action : tensor([[0.5386]], device='cuda:0')\n",
      "Next State : [[-0.32026643]\n",
      " [ 0.9473275 ]\n",
      " [-1.8427812 ]]\n",
      "Reward : [-4.6368566]\n",
      "--------------------------------------------------\n",
      "Step : 59\n",
      "Current State : [-0.26298812  0.96479905 -1.1978496 ]\n",
      "Currrent Action : tensor([[-0.4371]], device='cuda:0')\n",
      "Next State : [[-0.26298812]\n",
      " [ 0.96479905]\n",
      " [-1.1978496 ]]\n",
      "Reward : [-3.9376519]\n",
      "--------------------------------------------------\n",
      "Step : 60\n",
      "Current State : [-0.24597883  0.9692752  -0.35177344]\n",
      "Currrent Action : tensor([[0.8165]], device='cuda:0')\n",
      "Next State : [[-0.24597883]\n",
      " [ 0.9692752 ]\n",
      " [-0.35177344]]\n",
      "Reward : [-3.5184052]\n",
      "--------------------------------------------------\n",
      "Step : 61\n",
      "Current State : [-0.25006792  0.9682283   0.08441851]\n",
      "Currrent Action : tensor([[-1.9384]], device='cuda:0')\n",
      "Next State : [[-0.25006792]\n",
      " [ 0.9682283 ]\n",
      " [ 0.08441851]]\n",
      "Reward : [-3.326078]\n",
      "--------------------------------------------------\n",
      "Step : 62\n",
      "Current State : [-0.2949942   0.95549905  0.9339787 ]\n",
      "Currrent Action : tensor([[0.8226]], device='cuda:0')\n",
      "Next State : [[-0.2949942 ]\n",
      " [ 0.95549905]\n",
      " [ 0.9339787 ]]\n",
      "Reward : [-3.3267117]\n",
      "--------------------------------------------------\n",
      "Step : 63\n",
      "Current State : [-0.36866507  0.9295623   1.5624642 ]\n",
      "Currrent Action : tensor([[-0.5876]], device='cuda:0')\n",
      "Next State : [[-0.36866507]\n",
      " [ 0.9295623 ]\n",
      " [ 1.5624642 ]]\n",
      "Reward : [-3.5853977]\n",
      "--------------------------------------------------\n",
      "Step : 64\n",
      "Current State : [-0.4842924   0.87490624  2.5596359 ]\n",
      "Currrent Action : tensor([[2.1489]], device='cuda:0')\n",
      "Next State : [[-0.4842924 ]\n",
      " [ 0.87490624]\n",
      " [ 2.5596359 ]]\n",
      "Reward : [-4.0442724]\n",
      "--------------------------------------------------\n",
      "Step : 65\n",
      "Current State : [-0.6179806  0.7861933  3.212343 ]\n",
      "Currrent Action : tensor([[-0.0232]], device='cuda:0')\n",
      "Next State : [[-0.6179806]\n",
      " [ 0.7861933]\n",
      " [ 3.212343 ]]\n",
      "Reward : [-4.9664054]\n",
      "--------------------------------------------------\n",
      "Step : 66\n",
      "Current State : [-0.7502365  0.6611696  3.644964 ]\n",
      "Currrent Action : tensor([[-1.0468]], device='cuda:0')\n",
      "Next State : [[-0.7502365]\n",
      " [ 0.6611696]\n",
      " [ 3.644964 ]]\n",
      "Reward : [-6.0370374]\n",
      "--------------------------------------------------\n",
      "Step : 67\n",
      "Current State : [-0.86503035  0.50171953  3.9358246 ]\n",
      "Currrent Action : tensor([[-1.3668]], device='cuda:0')\n",
      "Next State : [[-0.86503035]\n",
      " [ 0.50171953]\n",
      " [ 3.9358246 ]]\n",
      "Reward : [-7.183052]\n",
      "--------------------------------------------------\n",
      "Step : 68\n",
      "Current State : [-0.94858015  0.3165371   4.070175  ]\n",
      "Currrent Action : tensor([[-1.6129]], device='cuda:0')\n",
      "Next State : [[-0.94858015]\n",
      " [ 0.3165371 ]\n",
      " [ 4.070175  ]]\n",
      "Reward : [-8.395166]\n",
      "--------------------------------------------------\n",
      "Step : 69\n",
      "Current State : [-0.9942853   0.10675585  4.3023386 ]\n",
      "Currrent Action : tensor([[-0.0349]], device='cuda:0')\n",
      "Next State : [[-0.9942853 ]\n",
      " [ 0.10675585]\n",
      " [ 4.3023386 ]]\n",
      "Reward : [-9.606306]\n",
      "--------------------------------------------------\n",
      "Step : 70\n",
      "Current State : [-0.9938973  -0.11030916  4.349876  ]\n",
      "Currrent Action : tensor([[-0.2169]], device='cuda:0')\n",
      "Next State : [[-0.9938973 ]\n",
      " [-0.11030916]\n",
      " [ 4.349876  ]]\n",
      "Reward : [-11.060055]\n",
      "--------------------------------------------------\n",
      "Step : 71\n",
      "Current State : [-0.95140845 -0.30793184  4.0496893 ]\n",
      "Currrent Action : tensor([[-1.4497]], device='cuda:0')\n",
      "Next State : [[-0.95140845]\n",
      " [-0.30793184]\n",
      " [ 4.0496893 ]]\n",
      "Reward : [-11.08156]\n",
      "--------------------------------------------------\n",
      "Step : 72\n",
      "Current State : [-0.882824   -0.46970394  3.5187404 ]\n",
      "Currrent Action : tensor([[-2.0397]], device='cuda:0')\n",
      "Next State : [[-0.882824  ]\n",
      " [-0.46970394]\n",
      " [ 3.5187404 ]]\n",
      "Reward : [-9.64483]\n",
      "--------------------------------------------------\n",
      "Step : 73\n",
      "Current State : [-0.79916   -0.6011183  3.1188867]\n",
      "Currrent Action : tensor([[-0.3172]], device='cuda:0')\n",
      "Next State : [[-0.79916  ]\n",
      " [-0.6011183]\n",
      " [ 3.1188867]]\n",
      "Reward : [-8.274737]\n",
      "--------------------------------------------------\n",
      "Step : 74\n",
      "Current State : [-0.71383137 -0.7003176   2.6188538 ]\n",
      "Currrent Action : tensor([[-0.3280]], device='cuda:0')\n",
      "Next State : [[-0.71383137]\n",
      " [-0.7003176 ]\n",
      " [ 2.6188538 ]]\n",
      "Reward : [-7.2063284]\n",
      "--------------------------------------------------\n",
      "Step : 75\n",
      "Current State : [-0.63225794 -0.774758    2.2097971 ]\n",
      "Currrent Action : tensor([[0.7745]], device='cuda:0')\n",
      "Next State : [[-0.63225794]\n",
      " [-0.774758  ]\n",
      " [ 2.2097971 ]]\n",
      "Reward : [-6.283214]\n",
      "--------------------------------------------------\n",
      "Step : 76\n",
      "Current State : [-0.5661737 -0.824286   1.6521554]\n",
      "Currrent Action : tensor([[0.1562]], device='cuda:0')\n",
      "Next State : [[-0.5661737]\n",
      " [-0.824286 ]\n",
      " [ 1.6521554]]\n",
      "Reward : [-5.5745444]\n",
      "--------------------------------------------------\n",
      "Step : 77\n",
      "Current State : [-0.5163219 -0.8563946  1.186117 ]\n",
      "Currrent Action : tensor([[1.0145]], device='cuda:0')\n",
      "Next State : [[-0.5163219]\n",
      " [-0.8563946]\n",
      " [ 1.186117 ]]\n",
      "Reward : [-4.9944105]\n",
      "--------------------------------------------------\n",
      "Step : 78\n",
      "Current State : [-0.50584346 -0.8626253   0.24382108]\n",
      "Currrent Action : tensor([[-2.7426]], device='cuda:0')\n",
      "Next State : [[-0.50584346]\n",
      " [-0.8626253 ]\n",
      " [ 0.24382108]]\n",
      "Reward : [-4.6109223]\n",
      "--------------------------------------------------\n",
      "Step : 79\n",
      "Current State : [-0.51751506 -0.8556741  -0.27169484]\n",
      "Currrent Action : tensor([[0.8764]], device='cuda:0')\n",
      "Next State : [[-0.51751506]\n",
      " [-0.8556741 ]\n",
      " [-0.27169484]]\n",
      "Reward : [-4.4215684]\n",
      "--------------------------------------------------\n",
      "Step : 80\n",
      "Current State : [-0.56043154 -0.82820076 -1.0192499 ]\n",
      "Currrent Action : tensor([[-0.7053]], device='cuda:0')\n",
      "Next State : [[-0.56043154]\n",
      " [-0.82820076]\n",
      " [-1.0192499 ]]\n",
      "Reward : [-4.480007]\n",
      "--------------------------------------------------\n",
      "Step : 81\n",
      "Current State : [-0.6267282 -0.7792379 -1.6488122]\n",
      "Currrent Action : tensor([[-0.0561]], device='cuda:0')\n",
      "Next State : [[-0.6267282]\n",
      " [-0.7792379]\n",
      " [-1.6488122]]\n",
      "Reward : [-4.7941604]\n",
      "--------------------------------------------------\n",
      "Step : 82\n",
      "Current State : [-0.70187354 -0.71230155 -2.013541  ]\n",
      "Currrent Action : tensor([[1.4647]], device='cuda:0')\n",
      "Next State : [[-0.70187354]\n",
      " [-0.71230155]\n",
      " [-2.013541  ]]\n",
      "Reward : [-5.3281536]\n",
      "--------------------------------------------------\n",
      "Step : 83\n",
      "Current State : [-0.77956253 -0.6263244  -2.3188546 ]\n",
      "Currrent Action : tensor([[1.5261]], device='cuda:0')\n",
      "Next State : [[-0.77956253]\n",
      " [-0.6263244 ]\n",
      " [-2.3188546 ]]\n",
      "Reward : [-5.924722]\n",
      "--------------------------------------------------\n",
      "Step : 84\n",
      "Current State : [-0.853157  -0.5216542 -2.5608099]\n",
      "Currrent Action : tensor([[1.5186]], device='cuda:0')\n",
      "Next State : [[-0.853157 ]\n",
      " [-0.5216542]\n",
      " [-2.5608099]]\n",
      "Reward : [-6.615073]\n",
      "--------------------------------------------------\n",
      "Step : 85\n",
      "Current State : [-0.9249324  -0.38013154 -3.1770039 ]\n",
      "Currrent Action : tensor([[-1.4997]], device='cuda:0')\n",
      "Next State : [[-0.9249324 ]\n",
      " [-0.38013154]\n",
      " [-3.1770039 ]]\n",
      "Reward : [-7.3806562]\n",
      "--------------------------------------------------\n",
      "Step : 86\n",
      "Current State : [-0.9749157  -0.22257464 -3.3096802 ]\n",
      "Currrent Action : tensor([[1.0161]], device='cuda:0')\n",
      "Next State : [[-0.9749157 ]\n",
      " [-0.22257464]\n",
      " [-3.3096802 ]]\n",
      "Reward : [-8.581968]\n",
      "--------------------------------------------------\n",
      "Step : 87\n",
      "Current State : [-0.99799    -0.06337176 -3.220808  ]\n",
      "Currrent Action : tensor([[1.7054]], device='cuda:0')\n",
      "Next State : [[-0.99799   ]\n",
      " [-0.06337176]\n",
      " [-3.220808  ]]\n",
      "Reward : [-9.608002]\n",
      "--------------------------------------------------\n",
      "Step : 88\n",
      "Current State : [-0.99521214  0.09773824 -3.226174  ]\n",
      "Currrent Action : tensor([[0.2811]], device='cuda:0')\n",
      "Next State : [[-0.99521214]\n",
      " [ 0.09773824]\n",
      " [-3.226174  ]]\n",
      "Reward : [-10.512622]\n",
      "--------------------------------------------------\n",
      "Step : 89\n",
      "Current State : [-0.97120994  0.23822515 -2.8528705 ]\n",
      "Currrent Action : tensor([[2.2468]], device='cuda:0')\n",
      "Next State : [[-0.97120994]\n",
      " [ 0.23822515]\n",
      " [-2.8528705 ]]\n",
      "Reward : [-10.308921]\n",
      "--------------------------------------------------\n",
      "Step : 90\n",
      "Current State : [-0.93504673  0.3545244  -2.4373488 ]\n",
      "Currrent Action : tensor([[1.5790]], device='cuda:0')\n",
      "Next State : [[-0.93504673]\n",
      " [ 0.3545244 ]\n",
      " [-2.4373488 ]]\n",
      "Reward : [-9.232497]\n",
      "--------------------------------------------------\n",
      "Step : 91\n",
      "Current State : [-0.89576423  0.44452944 -1.9648714 ]\n",
      "Currrent Action : tensor([[1.3772]], device='cuda:0')\n",
      "Next State : [[-0.89576423]\n",
      " [ 0.44452944]\n",
      " [-1.9648714 ]]\n",
      "Reward : [-8.319846]\n",
      "--------------------------------------------------\n",
      "Step : 92\n",
      "Current State : [-0.8587305   0.51242745 -1.5472088 ]\n",
      "Currrent Action : tensor([[0.5618]], device='cuda:0')\n",
      "Next State : [[-0.8587305 ]\n",
      " [ 0.51242745]\n",
      " [-1.5472088 ]]\n",
      "Reward : [-7.5738473]\n",
      "--------------------------------------------------\n",
      "Step : 93\n",
      "Current State : [-0.835376    0.5496789  -0.87941015]\n",
      "Currrent Action : tensor([[1.8899]], device='cuda:0')\n",
      "Next State : [[-0.835376  ]\n",
      " [ 0.5496789 ]\n",
      " [-0.87941015]]\n",
      "Reward : [-7.0216045]\n",
      "--------------------------------------------------\n",
      "Step : 94\n",
      "Current State : [-0.8208555  0.5711359 -0.5181856]\n",
      "Currrent Action : tensor([[-0.3402]], device='cuda:0')\n",
      "Next State : [[-0.8208555]\n",
      " [ 0.5711359]\n",
      " [-0.5181856]]\n",
      "Reward : [-6.6290708]\n",
      "--------------------------------------------------\n",
      "Step : 95\n",
      "Current State : [-0.8206945   0.5713672  -0.00563675]\n",
      "Currrent Action : tensor([[0.5613]], device='cuda:0')\n",
      "Next State : [[-0.8206945 ]\n",
      " [ 0.5713672 ]\n",
      " [-0.00563675]]\n",
      "Reward : [-6.446823]\n",
      "--------------------------------------------------\n",
      "Step : 96\n",
      "Current State : [-0.8341681   0.55151033  0.47994354]\n",
      "Currrent Action : tensor([[0.3804]], device='cuda:0')\n",
      "Next State : [[-0.8341681 ]\n",
      " [ 0.55151033]\n",
      " [ 0.47994354]]\n",
      "Reward : [-6.4183736]\n",
      "--------------------------------------------------\n",
      "Step : 97\n",
      "Current State : [-0.85451776  0.5194222   0.75998163]\n",
      "Currrent Action : tensor([[-0.8906]], device='cuda:0')\n",
      "Next State : [[-0.85451776]\n",
      " [ 0.5194222 ]\n",
      " [ 0.75998163]]\n",
      "Reward : [-6.5642204]\n",
      "--------------------------------------------------\n",
      "Step : 98\n",
      "Current State : [-0.87826174  0.4781802   0.95186436]\n",
      "Currrent Action : tensor([[-1.3179]], device='cuda:0')\n",
      "Next State : [[-0.87826174]\n",
      " [ 0.4781802 ]\n",
      " [ 0.95186436]]\n",
      "Reward : [-6.7956896]\n",
      "--------------------------------------------------\n",
      "Step : 99\n",
      "Current State : [-0.91068536  0.41310066  1.4545081 ]\n",
      "Currrent Action : tensor([[0.9601]], device='cuda:0')\n",
      "Next State : [[-0.91068536]\n",
      " [ 0.41310066]\n",
      " [ 1.4545081 ]]\n",
      "Reward : [-7.0770354]\n",
      "tensor([[-620.8011]], device='cuda:0')\n",
      "[tensor([[5.8143e+19]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[1.2824e+20]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[2.4910e+20]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[1.8284e+20]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[5.0622e+19]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[9.8247e+19]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[1.7847e+20]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[2.8100e+20]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[3.9857e+20]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[4.9337e+20]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[5.5151e+20]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[2.7520e+20]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[2.9268e+20]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[3.1350e+20]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[3.4050e+20]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[3.7794e+20]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[4.4596e+20]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[2.7640e+20]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[1.7958e+20]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[6.1918e+19]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[1.1247e+20]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[5.8476e+18]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[1.2831e+19]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[3.3191e+19]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[9.4686e+19]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[0.]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[25192.6914]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[100255.6328]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[415266.7500]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[1787502.2500]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[9054979.]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[53936264.]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[3.9722e+08]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[3.2346e+09]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[4587520.]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[33266398.]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[1.9989e+08]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[9.6026e+08]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[3.9861e+09]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[1.3982e+10]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[8.1638e+09]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[2.3760e+08]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[8.2270e+08]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[3.1186e+09]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[1.1428e+10]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[1.8330e+08]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[5.5741e+08]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[1.5667e+09]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[3.8788e+09]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[8.0012e+09]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[1.4760e+10]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[2.9842e+08]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[4.3128e+08]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[6.0011e+08]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[8.1296e+08]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[1.1364e+09]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[1.7003e+09]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[2.7235e+09]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[4.6216e+09]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[9.2819e+09]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[0.]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[-4.3751]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[-4.1778]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[-4.0374]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[-3.4951]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[5.2856]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[106.1086]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[1270.4783]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[16866.4062]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[284133.2188]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[6143157.5000]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[1.2391e+08]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[1.8778e+09]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[0.]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[-7.9282]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[-7.0488]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[-6.3871]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[-5.7246]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[-4.9720]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[-2.5208]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[12.8754]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[108.9582]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[519.8659]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[1903.3917]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[6186.0879]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[17725.1543]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[46776.0703]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[101978.7969]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[212091.7969]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[446451.4062]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[922838.5625]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[2073964.]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[-176.]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[-489.1516]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[-1646.5486]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[-8271.3818]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[-54838.4609]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[-542878.8125]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[-20075744.]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[0.]], device='cuda:0', grad_fn=<AddBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "\n",
    "def test_env(\n",
    "    env,\n",
    "    model,\n",
    "    total_steps = 100,\n",
    "    vis=False,\n",
    "):\n",
    "    \n",
    "    frames, rewards, actions, values, masks, states, log_probs, entropy, next_state = collect_experience(\n",
    "        env,\n",
    "        model,\n",
    "        total_steps\n",
    "    )\n",
    "\n",
    "    next_state = np.array(next_state).flatten()\n",
    "    total_reward = sum(rewards)\n",
    "\n",
    "    next_state = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "    _, next_value = model(next_state)\n",
    "    returns = compute_gae(next_value, rewards, masks, values)\n",
    "\n",
    "    return (\n",
    "        total_reward,\n",
    "        returns\n",
    "    )\n",
    "\n",
    "# This is how the environment works \n",
    "final_reward,returns = test_env(\n",
    "    env,\n",
    "    model,\n",
    "    vis=True\n",
    ")\n",
    "\n",
    "print(final_reward)\n",
    "print(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b81d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b13817",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
