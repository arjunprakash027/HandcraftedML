{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10ff97e1",
   "metadata": {},
   "source": [
    "## Import everything that we need here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b3d2a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e2d2117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aadf0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Using cuda\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "print(use_cuda)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75989e7",
   "metadata": {},
   "source": [
    "## Create Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70d4b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.multiprocessing_env import SubprocVecEnv\n",
    "\n",
    "num_envs = 16\n",
    "env_name = \"Pendulum-v1\"\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name, render_mode=\"human\")\n",
    "        return env \n",
    "    return _thunk()\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "#envs = SubprocVecEnv(envs)\n",
    "\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6205948b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PassiveEnvChecker<PendulumEnv<Pendulum-v1>>>>>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86916544",
   "metadata": {},
   "source": [
    "## Defining the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85073036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    # print(f\"This is m : {m}\")\n",
    "    # print(f\"This is the type of m : {type(m)}\")\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_inputs,\n",
    "            num_outputs,\n",
    "            hidden_size,\n",
    "            std=0.0\n",
    "    ):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_outputs),\n",
    "        )\n",
    "\n",
    "        self.log_std = nn.Parameter(torch.ones(1, num_outputs) * std)\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x\n",
    "    ):\n",
    "        value = self.critic(x)\n",
    "        mu = self.actor(x)\n",
    "        std = self.log_std.exp().expand_as(mu)\n",
    "        dist = Normal(mu, std)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4d1ddfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1\n",
      "log_std: torch.Size([1, 1])\n",
      "critic.0.weight: torch.Size([256, 3])\n",
      "critic.0.bias: torch.Size([256])\n",
      "critic.2.weight: torch.Size([1, 256])\n",
      "critic.2.bias: torch.Size([1])\n",
      "actor.0.weight: torch.Size([256, 3])\n",
      "actor.0.bias: torch.Size([256])\n",
      "actor.2.weight: torch.Size([1, 256])\n",
      "actor.2.bias: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "num_input = env.observation_space.shape[0]\n",
    "num_output = env.action_space.shape[0]\n",
    "\n",
    "print(num_input, num_output)\n",
    "\n",
    "hidden_size      = 256\n",
    "lr               = 3e-4\n",
    "num_steps        = 20\n",
    "mini_batch_size  = 5\n",
    "ppo_epochs       = 4\n",
    "threshold_reward = -200\n",
    "\n",
    "model = ActorCritic(num_input, num_output, hidden_size).to(device)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e04b40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_experience(\n",
    "    env,\n",
    "    model,\n",
    "    num_steps\n",
    "):\n",
    "    pass\n",
    "\n",
    "    frames = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    values  = []\n",
    "    masks = []\n",
    "    states = []\n",
    "    log_probs = []\n",
    "    entropy = 0\n",
    "\n",
    "    state, info = env.reset()\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, value = model(state)\n",
    "\n",
    "        action = dist.sample()\n",
    "        next_state, reward, done, _, _ = env.step(action.cpu().numpy()) #if you pass in action.cpy().numpy()[0] we will get a scalar value or single value tuple (3,)\n",
    "\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy += dist.entropy().mean()\n",
    "\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        \n",
    "        state = np.array(next_state).flatten()\n",
    "\n",
    "        print('-' * 50)\n",
    "        print(f\"Step : {step}\")\n",
    "        print(f\"Current State : {state}\")\n",
    "        print(f\"Currrent Action : {action}\")\n",
    "        print(f\"Next State : {next_state}\")\n",
    "        \n",
    "        # The interesting part is how the reward is calculated\n",
    "        # reward is -(angle_cost + velocity_cost + action_cost)\n",
    "\n",
    "        # angle_cost = angle**2                      Penalty for being away from upright (0Â°)\n",
    "        # velocity_cost = 0.1 * angular_velocity**2  Penalty for moving too fast\n",
    "        # action_cost = 0.001 * action**2            Penalty for using large torques\n",
    "        print(f\"Reward : {reward}\")\n",
    "        \n",
    "        #frame = env.render()\n",
    "        #frames.append(frame)\n",
    "    \n",
    "    return (\n",
    "        frames,\n",
    "        rewards,\n",
    "        actions,\n",
    "        values,\n",
    "        masks,\n",
    "        states,\n",
    "        log_probs,\n",
    "        entropy,\n",
    "        next_state\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be8becc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalized advtange exstimate is the total advantage that is expected at any particular time. Advtange is the reward that will be used to train the RL network\n",
    "def compute_gae(next_value, \n",
    "            rewards,\n",
    "            values, \n",
    "            masks,\n",
    "            gamma=0.9, \n",
    "            tau=0.95\n",
    "        ):\n",
    "    \n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    \n",
    "    for step in reversed(range(len(rewards))):\n",
    "\n",
    "        if masks[step] == 0:\n",
    "            gae = rewards[step]\n",
    "        else:\n",
    "            gae = rewards[step] + gamma * (tau * gae + (1 - tau) * values[step + 1])\n",
    "\n",
    "        returns.insert(0, gae)\n",
    "    \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87b5727f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Step : 0\n",
      "Current State : [-0.9914563  -0.13043961  0.55495125]\n",
      "Currrent Action : tensor([[1.2146]])\n",
      "Next State : [[-0.9914563 ]\n",
      " [-0.13043961]\n",
      " [ 0.55495125]]\n",
      "Reward : [-9.25436987]\n",
      "--------------------------------------------------\n",
      "Step : 1\n",
      "Current State : [-0.98816854 -0.15337192  0.46334615]\n",
      "Currrent Action : tensor([[0.0415]])\n",
      "Next State : [[-0.98816854]\n",
      " [-0.15337192]\n",
      " [ 0.46334615]]\n",
      "Reward : [-9.0955968]\n",
      "--------------------------------------------------\n",
      "Step : 2\n",
      "Current State : [-0.9847349  -0.17406097  0.41944867]\n",
      "Currrent Action : tensor([[0.4742]])\n",
      "Next State : [[-0.9847349 ]\n",
      " [-0.17406097]\n",
      " [ 0.41944867]]\n",
      "Reward : [-8.94752521]\n",
      "--------------------------------------------------\n",
      "Step : 3\n",
      "Current State : [-0.98250175 -0.18625337  0.24790601]\n",
      "Currrent Action : tensor([[-0.2733]])\n",
      "Next State : [[-0.98250175]\n",
      " [-0.18625337]\n",
      " [ 0.24790601]]\n",
      "Reward : [-8.8186246]\n",
      "--------------------------------------------------\n",
      "Step : 4\n",
      "Current State : [-0.98311776 -0.18297406 -0.06673338]\n",
      "Currrent Action : tensor([[-1.1663]])\n",
      "Next State : [[-0.98311776]\n",
      " [-0.18297406]\n",
      " [-0.06673338]]\n",
      "Reward : [-8.73507109]\n",
      "--------------------------------------------------\n",
      "Step : 5\n",
      "Current State : [-0.98469806 -0.1742693  -0.1769413 ]\n",
      "Currrent Action : tensor([[0.1802]])\n",
      "Next State : [[-0.98469806]\n",
      " [-0.1742693 ]\n",
      " [-0.1769413 ]]\n",
      "Reward : [-8.74776862]\n",
      "--------------------------------------------------\n",
      "Step : 6\n",
      "Current State : [-0.9882816  -0.15264174 -0.43845746]\n",
      "Currrent Action : tensor([[-0.8721]])\n",
      "Next State : [[-0.9882816 ]\n",
      " [-0.15264174]\n",
      " [-0.43845746]]\n",
      "Reward : [-8.80359232]\n",
      "--------------------------------------------------\n",
      "Step : 7\n",
      "Current State : [-0.99382764 -0.11093546 -0.84153056]\n",
      "Currrent Action : tensor([[-1.9239]])\n",
      "Next State : [[-0.99382764]\n",
      " [-0.11093546]\n",
      " [-0.84153056]]\n",
      "Reward : [-8.95317291]\n",
      "--------------------------------------------------\n",
      "Step : 8\n",
      "Current State : [-0.9978681  -0.06526262 -0.9171045 ]\n",
      "Currrent Action : tensor([[0.0509]])\n",
      "Next State : [[-0.9978681 ]\n",
      " [-0.06526262]\n",
      " [-0.9171045 ]]\n",
      "Reward : [-9.25431617]\n",
      "--------------------------------------------------\n",
      "Step : 9\n",
      "Current State : [-0.99980754 -0.01961839 -0.9137879 ]\n",
      "Currrent Action : tensor([[0.3484]])\n",
      "Next State : [[-0.99980754]\n",
      " [-0.01961839]\n",
      " [-0.9137879 ]]\n",
      "Reward : [-9.54775032]\n",
      "--------------------------------------------------\n",
      "Step : 10\n",
      "Current State : [-0.9993501   0.03604818 -1.1135128 ]\n",
      "Currrent Action : tensor([[-1.2334]])\n",
      "Next State : [[-0.9993501 ]\n",
      " [ 0.03604818]\n",
      " [-1.1135128 ]]\n",
      "Reward : [-9.83173757]\n",
      "--------------------------------------------------\n",
      "Step : 11\n",
      "Current State : [-0.99569374  0.09270341 -1.1356143 ]\n",
      "Currrent Action : tensor([[-0.3276]])\n",
      "Next State : [[-0.99569374]\n",
      " [ 0.09270341]\n",
      " [-1.1356143 ]]\n",
      "Reward : [-9.76845635]\n",
      "--------------------------------------------------\n",
      "Step : 12\n",
      "Current State : [-0.98857784  0.15071122 -1.1690195 ]\n",
      "Currrent Action : tensor([[-0.6862]])\n",
      "Next State : [[-0.98857784]\n",
      " [ 0.15071122]\n",
      " [-1.1690195 ]]\n",
      "Reward : [-9.42434572]\n",
      "--------------------------------------------------\n",
      "Step : 13\n",
      "Current State : [-0.9767064   0.21458006 -1.2994835 ]\n",
      "Currrent Action : tensor([[-1.6233]])\n",
      "Next State : [[-0.9767064 ]\n",
      " [ 0.21458006]\n",
      " [-1.2994835 ]]\n",
      "Reward : [-9.08121967]\n",
      "--------------------------------------------------\n",
      "Step : 14\n",
      "Current State : [-0.9639099   0.26622865 -1.06433   ]\n",
      "Currrent Action : tensor([[0.4948]])\n",
      "Next State : [[-0.9639099 ]\n",
      " [ 0.26622865]\n",
      " [-1.06433   ]]\n",
      "Reward : [-8.72667087]\n",
      "--------------------------------------------------\n",
      "Step : 15\n",
      "Current State : [-0.9506055  0.3104017 -0.9227445]\n",
      "Currrent Action : tensor([[-0.3872]])\n",
      "Next State : [[-0.9506055]\n",
      " [ 0.3104017]\n",
      " [-0.9227445]]\n",
      "Reward : [-8.36247037]\n",
      "--------------------------------------------------\n",
      "Step : 16\n",
      "Current State : [-0.9414607   0.33712274 -0.56486917]\n",
      "Currrent Action : tensor([[0.8338]])\n",
      "Next State : [[-0.9414607 ]\n",
      " [ 0.33712274]\n",
      " [-0.56486917]]\n",
      "Reward : [-8.07198746]\n",
      "--------------------------------------------------\n",
      "Step : 17\n",
      "Current State : [-0.93496746  0.35473356 -0.37540066]\n",
      "Currrent Action : tensor([[-0.4225]])\n",
      "Next State : [[-0.93496746]\n",
      " [ 0.35473356]\n",
      " [-0.37540066]]\n",
      "Reward : [-7.85939961]\n",
      "--------------------------------------------------\n",
      "Step : 18\n",
      "Current State : [-0.9299215   0.36775813 -0.27935925]\n",
      "Currrent Action : tensor([[-1.1334]])\n",
      "Next State : [[-0.9299215 ]\n",
      " [ 0.36775813]\n",
      " [-0.27935925]]\n",
      "Reward : [-7.73801575]\n",
      "--------------------------------------------------\n",
      "Step : 19\n",
      "Current State : [-0.92423314  0.38182867 -0.30354068]\n",
      "Currrent Action : tensor([[-2.3643]])\n",
      "Next State : [[-0.92423314]\n",
      " [ 0.38182867]\n",
      " [-0.30354068]]\n",
      "Reward : [-7.65700495]\n",
      "--------------------------------------------------\n",
      "Step : 20\n",
      "Current State : [-0.9197603   0.39248058 -0.23105901]\n",
      "Currrent Action : tensor([[-1.4259]])\n",
      "Next State : [[-0.9197603 ]\n",
      " [ 0.39248058]\n",
      " [-0.23105901]]\n",
      "Reward : [-7.57274924]\n",
      "--------------------------------------------------\n",
      "Step : 21\n",
      "Current State : [-0.9184032  0.3956457 -0.0688764]\n",
      "Currrent Action : tensor([[-0.8812]])\n",
      "Next State : [[-0.9184032]\n",
      " [ 0.3956457]\n",
      " [-0.0688764]]\n",
      "Reward : [-7.50421401]\n",
      "--------------------------------------------------\n",
      "Step : 22\n",
      "Current State : [-0.92245907  0.38609496  0.20752625]\n",
      "Currrent Action : tensor([[-0.1355]])\n",
      "Next State : [[-0.92245907]\n",
      " [ 0.38609496]\n",
      " [ 0.20752625]]\n",
      "Reward : [-7.47974313]\n",
      "--------------------------------------------------\n",
      "Step : 23\n",
      "Current State : [-0.931597    0.36349282  0.48760173]\n",
      "Currrent Action : tensor([[-0.0633]])\n",
      "Next State : [[-0.931597  ]\n",
      " [ 0.36349282]\n",
      " [ 0.48760173]]\n",
      "Reward : [-7.54042349]\n",
      "--------------------------------------------------\n",
      "Step : 24\n",
      "Current State : [-0.94453233  0.32841846  0.7477153 ]\n",
      "Currrent Action : tensor([[-0.0834]])\n",
      "Next State : [[-0.94453233]\n",
      " [ 0.32841846]\n",
      " [ 0.7477153 ]]\n",
      "Reward : [-7.69434598]\n",
      "--------------------------------------------------\n",
      "Step : 25\n",
      "Current State : [-0.96094924  0.2767246   1.084895  ]\n",
      "Currrent Action : tensor([[0.6058]])\n",
      "Next State : [[-0.96094924]\n",
      " [ 0.2767246 ]\n",
      " [ 1.084895  ]]\n",
      "Reward : [-7.93532156]\n",
      "--------------------------------------------------\n",
      "Step : 26\n",
      "Current State : [-0.9781478   0.20791069  1.4189087 ]\n",
      "Currrent Action : tensor([[0.8431]])\n",
      "Next State : [[-0.9781478 ]\n",
      " [ 0.20791069]\n",
      " [ 1.4189087 ]]\n",
      "Reward : [-8.30492602]\n",
      "--------------------------------------------------\n",
      "Step : 27\n",
      "Current State : [-0.9908137   0.13523392  1.4757786 ]\n",
      "Currrent Action : tensor([[-0.6604]])\n",
      "Next State : [[-0.9908137 ]\n",
      " [ 0.13523392]\n",
      " [ 1.4757786 ]]\n",
      "Reward : [-8.79929445]\n",
      "--------------------------------------------------\n",
      "Step : 28\n",
      "Current State : [-0.9983494   0.05743185  1.5637218 ]\n",
      "Currrent Action : tensor([[-0.0899]])\n",
      "Next State : [[-0.9983494 ]\n",
      " [ 0.05743185]\n",
      " [ 1.5637218 ]]\n",
      "Reward : [-9.25349422]\n",
      "--------------------------------------------------\n",
      "Step : 29\n",
      "Current State : [-0.9998706  -0.01608802  1.4710436 ]\n",
      "Currrent Action : tensor([[-0.9050]])\n",
      "Next State : [[-0.9998706 ]\n",
      " [-0.01608802]\n",
      " [ 1.4710436 ]]\n",
      "Reward : [-9.75719444]\n",
      "--------------------------------------------------\n",
      "Step : 30\n",
      "Current State : [-0.99490345 -0.10083219  1.6983025 ]\n",
      "Currrent Action : tensor([[1.5955]])\n",
      "Next State : [[-0.99490345]\n",
      " [-0.10083219]\n",
      " [ 1.6983025 ]]\n",
      "Reward : [-9.98771744]\n",
      "--------------------------------------------------\n",
      "Step : 31\n",
      "Current State : [-0.98257923 -0.1858441   1.7185404 ]\n",
      "Currrent Action : tensor([[0.6391]])\n",
      "Next State : [[-0.98257923]\n",
      " [-0.1858441 ]\n",
      " [ 1.7185404 ]]\n",
      "Reward : [-9.53401193]\n",
      "--------------------------------------------------\n",
      "Step : 32\n",
      "Current State : [-0.96508396 -0.26194078  1.5620357 ]\n",
      "Currrent Action : tensor([[-0.1141]])\n",
      "Next State : [[-0.96508396]\n",
      " [-0.26194078]\n",
      " [ 1.5620357 ]]\n",
      "Reward : [-9.02537747]\n",
      "--------------------------------------------------\n",
      "Step : 33\n",
      "Current State : [-0.9460066  -0.32414737  1.3015529 ]\n",
      "Currrent Action : tensor([[-0.4268]])\n",
      "Next State : [[-0.9460066 ]\n",
      " [-0.32414737]\n",
      " [ 1.3015529 ]]\n",
      "Reward : [-8.51877525]\n",
      "--------------------------------------------------\n",
      "Step : 34\n",
      "Current State : [-0.9318285 -0.3628989  0.8253342]\n",
      "Currrent Action : tensor([[-1.5541]])\n",
      "Next State : [[-0.9318285]\n",
      " [-0.3628989]\n",
      " [ 0.8253342]]\n",
      "Reward : [-8.07625219]\n",
      "--------------------------------------------------\n",
      "Step : 35\n",
      "Current State : [-0.92014617 -0.391575    0.61931354]\n",
      "Currrent Action : tensor([[0.4410]])\n",
      "Next State : [[-0.92014617]\n",
      " [-0.391575  ]\n",
      " [ 0.61931354]]\n",
      "Reward : [-7.74240695]\n",
      "--------------------------------------------------\n",
      "Step : 36\n",
      "Current State : [-0.91197664 -0.41024208  0.4075367 ]\n",
      "Currrent Action : tensor([[0.5460]])\n",
      "Next State : [[-0.91197664]\n",
      " [-0.41024208]\n",
      " [ 0.4075367 ]]\n",
      "Reward : [-7.54214353]\n",
      "--------------------------------------------------\n",
      "Step : 37\n",
      "Current State : [-0.9151722  -0.40306303 -0.1571633 ]\n",
      "Currrent Action : tensor([[-1.7135]])\n",
      "Next State : [[-0.9151722 ]\n",
      " [-0.40306303]\n",
      " [-0.1571633 ]]\n",
      "Reward : [-7.41181574]\n",
      "--------------------------------------------------\n",
      "Step : 38\n",
      "Current State : [-0.92981434 -0.36802897 -0.75946057]\n",
      "Currrent Action : tensor([[-2.0774]])\n",
      "Next State : [[-0.92981434]\n",
      " [-0.36802897]\n",
      " [-0.75946057]]\n",
      "Reward : [-7.44153368]\n",
      "--------------------------------------------------\n",
      "Step : 39\n",
      "Current State : [-0.9468527  -0.32166755 -0.987964  ]\n",
      "Currrent Action : tensor([[0.3168]])\n",
      "Next State : [[-0.9468527 ]\n",
      " [-0.32166755]\n",
      " [-0.987964  ]]\n",
      "Reward : [-7.70136849]\n",
      "--------------------------------------------------\n",
      "Step : 40\n",
      "Current State : [-0.9655836  -0.26009277 -1.287437  ]\n",
      "Currrent Action : tensor([[-0.3881]])\n",
      "Next State : [[-0.9655836 ]\n",
      " [-0.26009277]\n",
      " [-1.287437  ]]\n",
      "Reward : [-8.01693106]\n",
      "--------------------------------------------------\n",
      "Step : 41\n",
      "Current State : [-0.98311406 -0.18299389 -1.5817473 ]\n",
      "Currrent Action : tensor([[-0.6616]])\n",
      "Next State : [[-0.98311406]\n",
      " [-0.18299389]\n",
      " [-1.5817473 ]]\n",
      "Reward : [-8.45180191]\n",
      "--------------------------------------------------\n",
      "Step : 42\n",
      "Current State : [-0.99563515 -0.09333105 -1.8112764 ]\n",
      "Currrent Action : tensor([[-0.6152]])\n",
      "Next State : [[-0.99563515]\n",
      " [-0.09333105]\n",
      " [-1.8112764 ]]\n",
      "Reward : [-8.99774248]\n",
      "--------------------------------------------------\n",
      "Step : 43\n",
      "Current State : [-9.9999988e-01 -5.3581479e-04 -1.8586253e+00]\n",
      "Currrent Action : tensor([[0.1510]])\n",
      "Next State : [[-9.9999988e-01]\n",
      " [-5.3581479e-04]\n",
      " [-1.8586253e+00]]\n",
      "Reward : [-9.61916455]\n",
      "--------------------------------------------------\n",
      "Step : 44\n",
      "Current State : [-0.9963038   0.08589974 -1.7308309 ]\n",
      "Currrent Action : tensor([[0.8546]])\n",
      "Next State : [[-0.9963038 ]\n",
      " [ 0.08589974]\n",
      " [-1.7308309 ]]\n",
      "Reward : [-10.21241727]\n",
      "--------------------------------------------------\n",
      "Step : 45\n",
      "Current State : [-0.98358434  0.18044904 -1.9087445 ]\n",
      "Currrent Action : tensor([[-1.6156]])\n",
      "Next State : [[-0.98358434]\n",
      " [ 0.18044904]\n",
      " [-1.9087445 ]]\n",
      "Reward : [-9.63879914]\n",
      "--------------------------------------------------\n",
      "Step : 46\n",
      "Current State : [-0.9616636   0.27423185 -1.9269577 ]\n",
      "Currrent Action : tensor([[-1.0237]])\n",
      "Next State : [[-0.9616636 ]\n",
      " [ 0.27423185]\n",
      " [-1.9269577 ]]\n",
      "Reward : [-9.12786467]\n",
      "--------------------------------------------------\n",
      "Step : 47\n",
      "Current State : [-0.9330934   0.35963407 -1.8016969 ]\n",
      "Currrent Action : tensor([[-0.5361]])\n",
      "Next State : [[-0.9330934 ]\n",
      " [ 0.35963407]\n",
      " [-1.8016969 ]]\n",
      "Reward : [-8.5729648]\n",
      "--------------------------------------------------\n",
      "Step : 48\n",
      "Current State : [-0.8996745  0.4365614 -1.6779474]\n",
      "Currrent Action : tensor([[-0.9732]])\n",
      "Next State : [[-0.8996745]\n",
      " [ 0.4365614]\n",
      " [-1.6779474]]\n",
      "Reward : [-8.01906404]\n",
      "--------------------------------------------------\n",
      "Step : 49\n",
      "Current State : [-0.86834174  0.49596635 -1.3434856 ]\n",
      "Currrent Action : tensor([[0.0469]])\n",
      "Next State : [[-0.86834174]\n",
      " [ 0.49596635]\n",
      " [-1.3434856 ]]\n",
      "Reward : [-7.51668238]\n",
      "--------------------------------------------------\n",
      "Step : 50\n",
      "Current State : [-0.842223   0.5391294 -1.0091134]\n",
      "Currrent Action : tensor([[-0.2507]])\n",
      "Next State : [[-0.842223 ]\n",
      " [ 0.5391294]\n",
      " [-1.0091134]]\n",
      "Reward : [-7.05882662]\n",
      "--------------------------------------------------\n",
      "Step : 51\n",
      "Current State : [-0.817224   0.5763202 -0.8963126]\n",
      "Currrent Action : tensor([[-1.9436]])\n",
      "Next State : [[-0.817224 ]\n",
      " [ 0.5763202]\n",
      " [-0.8963126]]\n",
      "Reward : [-6.72176826]\n",
      "--------------------------------------------------\n",
      "Step : 52\n",
      "Current State : [-0.80057126  0.59923756 -0.5665946 ]\n",
      "Currrent Action : tensor([[-0.6835]])\n",
      "Next State : [[-0.80057126]\n",
      " [ 0.59923756]\n",
      " [-0.5665946 ]]\n",
      "Reward : [-6.46842413]\n",
      "--------------------------------------------------\n",
      "Step : 53\n",
      "Current State : [-0.79669636  0.6043797  -0.12877426]\n",
      "Currrent Action : tensor([[-0.0774]])\n",
      "Next State : [[-0.79669636]\n",
      " [ 0.6043797 ]\n",
      " [-0.12877426]]\n",
      "Reward : [-6.27733123]\n",
      "--------------------------------------------------\n",
      "Step : 54\n",
      "Current State : [-0.80375177  0.5949648   0.23530495]\n",
      "Currrent Action : tensor([[-0.5947]])\n",
      "Next State : [[-0.80375177]\n",
      " [ 0.5949648 ]\n",
      " [ 0.23530495]]\n",
      "Reward : [-6.21509445]\n",
      "--------------------------------------------------\n",
      "Step : 55\n",
      "Current State : [-0.8198175  0.5726249  0.5503548]\n",
      "Currrent Action : tensor([[-0.8745]])\n",
      "Next State : [[-0.8198175]\n",
      " [ 0.5726249]\n",
      " [ 0.5503548]]\n",
      "Reward : [-6.27817474]\n",
      "--------------------------------------------------\n",
      "Step : 56\n",
      "Current State : [-0.84324676  0.53752667  0.8440572 ]\n",
      "Currrent Action : tensor([[-0.9051]])\n",
      "Next State : [[-0.84324676]\n",
      " [ 0.53752667]\n",
      " [ 0.8440572 ]]\n",
      "Reward : [-6.44156788]\n",
      "--------------------------------------------------\n",
      "Step : 57\n",
      "Current State : [-0.87647474  0.48144788  1.3039067 ]\n",
      "Currrent Action : tensor([[0.3780]])\n",
      "Next State : [[-0.87647474]\n",
      " [ 0.48144788]\n",
      " [ 1.3039067 ]]\n",
      "Reward : [-6.69733275]\n",
      "--------------------------------------------------\n",
      "Step : 58\n",
      "Current State : [-0.9108343   0.41277218  1.5362073 ]\n",
      "Currrent Action : tensor([[-0.8586]])\n",
      "Next State : [[-0.9108343 ]\n",
      " [ 0.41277218]\n",
      " [ 1.5362073 ]]\n",
      "Reward : [-7.13658891]\n",
      "--------------------------------------------------\n",
      "Step : 59\n",
      "Current State : [-0.9465552   0.32254183  1.9416388 ]\n",
      "Currrent Action : tensor([[0.6390]])\n",
      "Next State : [[-0.9465552 ]\n",
      " [ 0.32254183]\n",
      " [ 1.9416388 ]]\n",
      "Reward : [-7.61358517]\n",
      "--------------------------------------------------\n",
      "Step : 60\n",
      "Current State : [-0.9740391  0.2263799  2.0010822]\n",
      "Currrent Action : tensor([[-1.2164]])\n",
      "Next State : [[-0.9740391]\n",
      " [ 0.2263799]\n",
      " [ 2.0010822]]\n",
      "Reward : [-8.29245216]\n",
      "--------------------------------------------------\n",
      "Step : 61\n",
      "Current State : [-0.9928009   0.11977637  2.1658974 ]\n",
      "Currrent Action : tensor([[-0.0331]])\n",
      "Next State : [[-0.9928009 ]\n",
      " [ 0.11977637]\n",
      " [ 2.1658974 ]]\n",
      "Reward : [-8.88736159]\n",
      "--------------------------------------------------\n",
      "Step : 62\n",
      "Current State : [-0.9998415   0.01780471  2.0451796 ]\n",
      "Currrent Action : tensor([[-1.4037]])\n",
      "Next State : [[-0.9998415 ]\n",
      " [ 0.01780471]\n",
      " [ 2.0451796 ]]\n",
      "Reward : [-9.60071302]\n",
      "--------------------------------------------------\n",
      "Step : 63\n",
      "Current State : [-0.99640584 -0.08470793  2.0523043 ]\n",
      "Currrent Action : tensor([[-0.0415]])\n",
      "Next State : [[-0.99640584]\n",
      " [-0.08470793]\n",
      " [ 2.0523043 ]]\n",
      "Reward : [-10.17632293]\n",
      "--------------------------------------------------\n",
      "Step : 64\n",
      "Current State : [-0.98408186 -0.17771575  1.877104  ]\n",
      "Currrent Action : tensor([[-0.7445]])\n",
      "Next State : [[-0.98408186]\n",
      " [-0.17771575]\n",
      " [ 1.877104  ]]\n",
      "Reward : [-9.76567233]\n",
      "--------------------------------------------------\n",
      "Step : 65\n",
      "Current State : [-0.9650013  -0.26224515  1.7336656 ]\n",
      "Currrent Action : tensor([[-0.0677]])\n",
      "Next State : [[-0.9650013 ]\n",
      " [-0.26224515]\n",
      " [ 1.7336656 ]]\n",
      "Reward : [-9.13129822]\n",
      "--------------------------------------------------\n",
      "Step : 66\n",
      "Current State : [-0.9446296  -0.32813853  1.3796854 ]\n",
      "Currrent Action : tensor([[-1.0486]])\n",
      "Next State : [[-0.9446296 ]\n",
      " [-0.32813853]\n",
      " [ 1.3796854 ]]\n",
      "Reward : [-8.57444235]\n",
      "--------------------------------------------------\n",
      "Step : 67\n",
      "Current State : [-0.92822975 -0.3720074   0.9367679 ]\n",
      "Currrent Action : tensor([[-1.3121]])\n",
      "Next State : [[-0.92822975]\n",
      " [-0.3720074 ]\n",
      " [ 0.9367679 ]]\n",
      "Reward : [-8.07278538]\n",
      "--------------------------------------------------\n",
      "Step : 68\n",
      "Current State : [-0.9163893  -0.40028822  0.6132125 ]\n",
      "Currrent Action : tensor([[-0.2970]])\n",
      "Next State : [[-0.9163893 ]\n",
      " [-0.40028822]\n",
      " [ 0.6132125 ]]\n",
      "Reward : [-7.70777093]\n",
      "--------------------------------------------------\n",
      "Step : 69\n",
      "Current State : [-0.90812284 -0.41870382  0.40372375]\n",
      "Currrent Action : tensor([[0.6048]])\n",
      "Next State : [[-0.90812284]\n",
      " [-0.41870382]\n",
      " [ 0.40372375]]\n",
      "Reward : [-7.48956565]\n",
      "--------------------------------------------------\n",
      "Step : 70\n",
      "Current State : [-0.8997926  -0.4363178   0.38969588]\n",
      "Currrent Action : tensor([[2.3774]])\n",
      "Next State : [[-0.8997926 ]\n",
      " [-0.4363178 ]\n",
      " [ 0.38969588]]\n",
      "Reward : [-7.36209667]\n",
      "--------------------------------------------------\n",
      "Step : 71\n",
      "Current State : [-0.8942452  -0.44757742  0.25104213]\n",
      "Currrent Action : tensor([[1.2572]])\n",
      "Next State : [[-0.8942452 ]\n",
      " [-0.44757742]\n",
      " [ 0.25104213]]\n",
      "Reward : [-7.25335293]\n",
      "--------------------------------------------------\n",
      "Step : 72\n",
      "Current State : [-0.8972737  -0.44147465 -0.13625862]\n",
      "Currrent Action : tensor([[-0.3441]])\n",
      "Next State : [[-0.8972737 ]\n",
      " [-0.44147465]\n",
      " [-0.13625862]]\n",
      "Reward : [-7.17563159]\n",
      "--------------------------------------------------\n",
      "Step : 73\n",
      "Current State : [-0.9135478 -0.4067314 -0.7673646]\n",
      "Currrent Action : tensor([[-2.4223]])\n",
      "Next State : [[-0.9135478]\n",
      " [-0.4067314]\n",
      " [-0.7673646]]\n",
      "Reward : [-7.21159778]\n",
      "--------------------------------------------------\n",
      "Step : 74\n",
      "Current State : [-0.9323595  -0.36153242 -0.9792461 ]\n",
      "Currrent Action : tensor([[0.6211]])\n",
      "Next State : [[-0.9323595 ]\n",
      " [-0.36153242]\n",
      " [-0.9792461 ]]\n",
      "Reward : [-7.4724715]\n",
      "--------------------------------------------------\n",
      "Step : 75\n",
      "Current State : [-0.9526575  -0.30404538 -1.219495  ]\n",
      "Currrent Action : tensor([[0.2060]])\n",
      "Next State : [[-0.9526575 ]\n",
      " [-0.30404538]\n",
      " [-1.219495  ]]\n",
      "Reward : [-7.77815412]\n",
      "--------------------------------------------------\n",
      "Step : 76\n",
      "Current State : [-0.9724458  -0.23312926 -1.4728367 ]\n",
      "Currrent Action : tensor([[-0.1687]])\n",
      "Next State : [[-0.9724458 ]\n",
      " [-0.23312926]\n",
      " [-1.4728367 ]]\n",
      "Reward : [-8.1726878]\n",
      "--------------------------------------------------\n",
      "Step : 77\n",
      "Current State : [-0.98987484 -0.1419431  -1.8574051 ]\n",
      "Currrent Action : tensor([[-1.3981]])\n",
      "Next State : [[-0.98987484]\n",
      " [-0.1419431 ]\n",
      " [-1.8574051 ]]\n",
      "Reward : [-8.66544931]\n",
      "--------------------------------------------------\n",
      "Step : 78\n",
      "Current State : [-0.99903    -0.04403523 -1.9674929 ]\n",
      "Currrent Action : tensor([[-0.0242]])\n",
      "Next State : [[-0.99903   ]\n",
      " [-0.04403523]\n",
      " [-1.9674929 ]]\n",
      "Reward : [-9.34000786]\n",
      "--------------------------------------------------\n",
      "Step : 79\n",
      "Current State : [-0.9983174   0.05798608 -2.0413618 ]\n",
      "Currrent Action : tensor([[-0.2723]])\n",
      "Next State : [[-0.9983174 ]\n",
      " [ 0.05798608]\n",
      " [-2.0413618 ]]\n",
      "Reward : [-9.98195077]\n",
      "--------------------------------------------------\n",
      "Step : 80\n",
      "Current State : [-0.9850879   0.17205188 -2.2978723 ]\n",
      "Currrent Action : tensor([[-3.0561]])\n",
      "Next State : [[-0.9850879 ]\n",
      " [ 0.17205188]\n",
      " [-2.2978723 ]]\n",
      "Reward : [-9.92914462]\n",
      "--------------------------------------------------\n",
      "Step : 81\n",
      "Current State : [-0.95840955  0.28539643 -2.3301556 ]\n",
      "Currrent Action : tensor([[-1.0755]])\n",
      "Next State : [[-0.95840955]\n",
      " [ 0.28539643]\n",
      " [-2.3301556 ]]\n",
      "Reward : [-9.34224179]\n",
      "--------------------------------------------------\n",
      "Step : 82\n",
      "Current State : [-0.91703093  0.39881608 -2.4161084 ]\n",
      "Currrent Action : tensor([[-2.1723]])\n",
      "Next State : [[-0.91703093]\n",
      " [ 0.39881608]\n",
      " [-2.4161084 ]]\n",
      "Reward : [-8.68185129]\n",
      "--------------------------------------------------\n",
      "Step : 83\n",
      "Current State : [-0.86965936  0.49365225 -2.1211796 ]\n",
      "Currrent Action : tensor([[-0.0279]])\n",
      "Next State : [[-0.86965936]\n",
      " [ 0.49365225]\n",
      " [-2.1211796 ]]\n",
      "Reward : [-8.04412562]\n",
      "--------------------------------------------------\n",
      "Step : 84\n",
      "Current State : [-0.81756365  0.57583827 -1.9468933 ]\n",
      "Currrent Action : tensor([[-1.3064]])\n",
      "Next State : [[-0.81756365]\n",
      " [ 0.57583827]\n",
      " [-1.9468933 ]]\n",
      "Reward : [-7.34389017]\n",
      "--------------------------------------------------\n",
      "Step : 85\n",
      "Current State : [-0.7648412   0.64421886 -1.7274498 ]\n",
      "Currrent Action : tensor([[-1.4162]])\n",
      "Next State : [[-0.7648412 ]\n",
      " [ 0.64421886]\n",
      " [-1.7274498 ]]\n",
      "Reward : [-6.77164494]\n",
      "--------------------------------------------------\n",
      "Step : 86\n",
      "Current State : [-0.7236312   0.69018686 -1.2349135 ]\n",
      "Currrent Action : tensor([[0.0625]])\n",
      "Next State : [[-0.7236312 ]\n",
      " [ 0.69018686]\n",
      " [-1.2349135 ]]\n",
      "Reward : [-6.25977924]\n",
      "--------------------------------------------------\n",
      "Step : 87\n",
      "Current State : [-0.7016581   0.7125138  -0.62654227]\n",
      "Currrent Action : tensor([[0.6049]])\n",
      "Next State : [[-0.7016581 ]\n",
      " [ 0.7125138 ]\n",
      " [-0.62654227]]\n",
      "Reward : [-5.8165312]\n",
      "--------------------------------------------------\n",
      "Step : 88\n",
      "Current State : [-0.7007877   0.7133699  -0.02441769]\n",
      "Currrent Action : tensor([[0.4516]])\n",
      "Next State : [[-0.7007877 ]\n",
      " [ 0.7133699 ]\n",
      " [-0.02441769]]\n",
      "Reward : [-5.55499766]\n",
      "--------------------------------------------------\n",
      "Step : 89\n",
      "Current State : [-0.7156803   0.698428    0.42193225]\n",
      "Currrent Action : tensor([[-0.5912]])\n",
      "Next State : [[-0.7156803 ]\n",
      " [ 0.698428  ]\n",
      " [ 0.42193225]]\n",
      "Reward : [-5.51021428]\n",
      "--------------------------------------------------\n",
      "Step : 90\n",
      "Current State : [-0.7501827  0.6612306  1.0148126]\n",
      "Currrent Action : tensor([[0.4604]])\n",
      "Next State : [[-0.7501827]\n",
      " [ 0.6612306]\n",
      " [ 1.0148126]]\n",
      "Reward : [-5.62730492]\n",
      "--------------------------------------------------\n",
      "Step : 91\n",
      "Current State : [-0.7936183   0.60841596  1.3678976 ]\n",
      "Currrent Action : tensor([[-0.9523]])\n",
      "Next State : [[-0.7936183 ]\n",
      " [ 0.60841596]\n",
      " [ 1.3678976 ]]\n",
      "Reward : [-5.95610371]\n",
      "--------------------------------------------------\n",
      "Step : 92\n",
      "Current State : [-0.8446578  0.5353067  1.7838459]\n",
      "Currrent Action : tensor([[-0.2691]])\n",
      "Next State : [[-0.8446578]\n",
      " [ 0.5353067]\n",
      " [ 1.7838459]]\n",
      "Reward : [-6.37499003]\n",
      "--------------------------------------------------\n",
      "Step : 93\n",
      "Current State : [-0.900984    0.43385237  2.3221333 ]\n",
      "Currrent Action : tensor([[0.9120]])\n",
      "Next State : [[-0.900984  ]\n",
      " [ 0.43385237]\n",
      " [ 2.3221333 ]]\n",
      "Reward : [-6.95853785]\n",
      "--------------------------------------------------\n",
      "Step : 94\n",
      "Current State : [-0.9533357   0.30191237  2.8413234 ]\n",
      "Currrent Action : tensor([[1.2920]])\n",
      "Next State : [[-0.9533357 ]\n",
      " [ 0.30191237]\n",
      " [ 2.8413234 ]]\n",
      "Reward : [-7.79222492]\n",
      "--------------------------------------------------\n",
      "Step : 95\n",
      "Current State : [-0.98610926  0.16609783  2.7965357 ]\n",
      "Currrent Action : tensor([[-1.8081]])\n",
      "Next State : [[-0.98610926]\n",
      " [ 0.16609783]\n",
      " [ 2.7965357 ]]\n",
      "Reward : [-8.84720899]\n",
      "--------------------------------------------------\n",
      "Step : 96\n",
      "Current State : [-0.9998995   0.01417569  3.0539007 ]\n",
      "Currrent Action : tensor([[0.8853]])\n",
      "Next State : [[-0.9998995 ]\n",
      " [ 0.01417569]\n",
      " [ 3.0539007 ]]\n",
      "Reward : [-9.63181261]\n",
      "--------------------------------------------------\n",
      "Step : 97\n",
      "Current State : [-0.9911476  -0.13276476  2.9466817 ]\n",
      "Currrent Action : tensor([[-0.7857]])\n",
      "Next State : [[-0.9911476 ]\n",
      " [-0.13276476]\n",
      " [ 2.9466817 ]]\n",
      "Reward : [-10.71398217]\n",
      "--------------------------------------------------\n",
      "Step : 98\n",
      "Current State : [-0.9602637 -0.2790941  2.9938536]\n",
      "Currrent Action : tensor([[0.9783]])\n",
      "Next State : [[-0.9602637]\n",
      " [-0.2790941]\n",
      " [ 2.9938536]]\n",
      "Reward : [-9.91992991]\n",
      "--------------------------------------------------\n",
      "Step : 99\n",
      "Current State : [-0.9153816  -0.40258732  2.6298196 ]\n",
      "Currrent Action : tensor([[-1.0314]])\n",
      "Next State : [[-0.9153816 ]\n",
      " [-0.40258732]\n",
      " [ 2.6298196 ]]\n",
      "Reward : [-9.06978588]\n",
      "tensor([[-818.8426]])\n",
      "[tensor([[1.3960e+29]], grad_fn=<AddBackward0>), tensor([[1.6327e+29]], grad_fn=<AddBackward0>), tensor([[1.9096e+29]], grad_fn=<AddBackward0>), tensor([[4.1061e+29]], grad_fn=<AddBackward0>), tensor([[4.8025e+29]], grad_fn=<AddBackward0>), tensor([[5.6170e+29]], grad_fn=<AddBackward0>), tensor([[6.5696e+29]], grad_fn=<AddBackward0>), tensor([[9.6524e+29]], grad_fn=<AddBackward0>), tensor([[1.1289e+30]], grad_fn=<AddBackward0>), tensor([[1.3204e+30]], grad_fn=<AddBackward0>), tensor([[1.5443e+30]], grad_fn=<AddBackward0>), tensor([[1.8062e+30]], grad_fn=<AddBackward0>), tensor([[2.1125e+30]], grad_fn=<AddBackward0>), tensor([[2.4708e+30]], grad_fn=<AddBackward0>), tensor([[2.8898e+30]], grad_fn=<AddBackward0>), tensor([[3.3799e+30]], grad_fn=<AddBackward0>), tensor([[3.9531e+30]], grad_fn=<AddBackward0>), tensor([[4.6235e+30]], grad_fn=<AddBackward0>), tensor([[5.4076e+30]], grad_fn=<AddBackward0>), tensor([[6.3247e+30]], grad_fn=<AddBackward0>), tensor([[7.3973e+30]], grad_fn=<AddBackward0>), tensor([[8.6518e+30]], grad_fn=<AddBackward0>), tensor([[1.0119e+31]], grad_fn=<AddBackward0>), tensor([[1.2022e+31]], grad_fn=<AddBackward0>), tensor([[1.3129e+31]], grad_fn=<AddBackward0>), tensor([[1.5356e+31]], grad_fn=<AddBackward0>), tensor([[1.7960e+31]], grad_fn=<AddBackward0>), tensor([[2.1006e+31]], grad_fn=<AddBackward0>), tensor([[2.4569e+31]], grad_fn=<AddBackward0>), tensor([[2.8961e+31]], grad_fn=<AddBackward0>), tensor([[3.3872e+31]], grad_fn=<AddBackward0>), tensor([[3.9617e+31]], grad_fn=<AddBackward0>), tensor([[-2.0563e+29]], grad_fn=<AddBackward0>), tensor([[-2.4057e+29]], grad_fn=<AddBackward0>), tensor([[-2.8137e+29]], grad_fn=<AddBackward0>), tensor([[-3.2908e+29]], grad_fn=<AddBackward0>), tensor([[-3.8489e+29]], grad_fn=<AddBackward0>), tensor([[-4.5017e+29]], grad_fn=<AddBackward0>), tensor([[-5.2651e+29]], grad_fn=<AddBackward0>), tensor([[-3.7843e+29]], grad_fn=<AddBackward0>), tensor([[-4.4261e+29]], grad_fn=<AddBackward0>), tensor([[-3.4699e+29]], grad_fn=<AddBackward0>), tensor([[-2.0699e+29]], grad_fn=<AddBackward0>), tensor([[-2.4209e+29]], grad_fn=<AddBackward0>), tensor([[-2.8315e+29]], grad_fn=<AddBackward0>), tensor([[-9.9818e+28]], grad_fn=<AddBackward0>), tensor([[-1.1675e+29]], grad_fn=<AddBackward0>), tensor([[-1.3655e+29]], grad_fn=<AddBackward0>), tensor([[-1.5970e+29]], grad_fn=<AddBackward0>), tensor([[-1.8679e+29]], grad_fn=<AddBackward0>), tensor([[-2.1846e+29]], grad_fn=<AddBackward0>), tensor([[-2.5551e+29]], grad_fn=<AddBackward0>), tensor([[-2.9885e+29]], grad_fn=<AddBackward0>), tensor([[-3.4953e+29]], grad_fn=<AddBackward0>), tensor([[-2.0910e+29]], grad_fn=<AddBackward0>), tensor([[-2.4456e+29]], grad_fn=<AddBackward0>), tensor([[-2.8604e+29]], grad_fn=<AddBackward0>), tensor([[-3.3455e+29]], grad_fn=<AddBackward0>), tensor([[-2.1158e+29]], grad_fn=<AddBackward0>), tensor([[-2.4746e+29]], grad_fn=<AddBackward0>), tensor([[-2.8943e+29]], grad_fn=<AddBackward0>), tensor([[-1.3484e+29]], grad_fn=<AddBackward0>), tensor([[-1.5771e+29]], grad_fn=<AddBackward0>), tensor([[-1.8446e+29]], grad_fn=<AddBackward0>), tensor([[-4.8175e+28]], grad_fn=<AddBackward0>), tensor([[-5.6345e+28]], grad_fn=<AddBackward0>), tensor([[-6.5900e+28]], grad_fn=<AddBackward0>), tensor([[-7.7076e+28]], grad_fn=<AddBackward0>), tensor([[-9.0148e+28]], grad_fn=<AddBackward0>), tensor([[-1.0544e+29]], grad_fn=<AddBackward0>), tensor([[-1.2332e+29]], grad_fn=<AddBackward0>), tensor([[-1.4423e+29]], grad_fn=<AddBackward0>), tensor([[-1.6869e+29]], grad_fn=<AddBackward0>), tensor([[-1.9730e+29]], grad_fn=<AddBackward0>), tensor([[-2.3076e+29]], grad_fn=<AddBackward0>), tensor([[-2.6989e+29]], grad_fn=<AddBackward0>), tensor([[-3.1566e+29]], grad_fn=<AddBackward0>), tensor([[-1.2381e+29]], grad_fn=<AddBackward0>), tensor([[-1.4481e+29]], grad_fn=<AddBackward0>), tensor([[-1.6937e+29]], grad_fn=<AddBackward0>), tensor([[-1.9809e+29]], grad_fn=<AddBackward0>), tensor([[-4.3499e+28]], grad_fn=<AddBackward0>), tensor([[-5.0876e+28]], grad_fn=<AddBackward0>), tensor([[-5.9504e+28]], grad_fn=<AddBackward0>), tensor([[-6.9596e+28]], grad_fn=<AddBackward0>), tensor([[-8.1398e+28]], grad_fn=<AddBackward0>), tensor([[-9.5203e+28]], grad_fn=<AddBackward0>), tensor([[-1.1135e+29]], grad_fn=<AddBackward0>), tensor([[-1.3023e+29]], grad_fn=<AddBackward0>), tensor([[-1.5232e+29]], grad_fn=<AddBackward0>), tensor([[-1.7815e+29]], grad_fn=<AddBackward0>), tensor([[-2.0836e+29]], grad_fn=<AddBackward0>), tensor([[-2.4370e+29]], grad_fn=<AddBackward0>), tensor([[-1.0884e+29]], grad_fn=<AddBackward0>), tensor([[-1.2729e+29]], grad_fn=<AddBackward0>), tensor([[-1.4888e+29]], grad_fn=<AddBackward0>), tensor([[-1.7413e+29]], grad_fn=<AddBackward0>), tensor([[-2.0366e+29]], grad_fn=<AddBackward0>), tensor([[-17.6830]], grad_fn=<AddBackward0>), tensor([[-9.0797]], grad_fn=<AddBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "\n",
    "def test_env(\n",
    "    env,\n",
    "    model,\n",
    "    total_steps = 100,\n",
    "    vis=False,\n",
    "):\n",
    "    \n",
    "    frames, rewards, actions, values, masks, states, log_probs, entropy, next_state = collect_experience(\n",
    "        env,\n",
    "        model,\n",
    "        total_steps\n",
    "    )\n",
    "\n",
    "    next_state = np.array(next_state).flatten()\n",
    "    total_reward = sum(rewards)\n",
    "\n",
    "    next_state = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "    _, next_value = model(next_state)\n",
    "    returns = compute_gae(next_value, rewards, masks, values)\n",
    "\n",
    "    return (\n",
    "        total_reward,\n",
    "        returns\n",
    "    )\n",
    "\n",
    "# This is how the environment works \n",
    "final_reward,returns = test_env(\n",
    "    env,\n",
    "    model,\n",
    "    vis=True\n",
    ")\n",
    "\n",
    "print(final_reward)\n",
    "print(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ed2549",
   "metadata": {},
   "source": [
    "## The real training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "10b81d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_randomizer(\n",
    "    mini_batch_size,\n",
    "    states,\n",
    "    actions,\n",
    "    log_probs,\n",
    "    returns,\n",
    "    advantages\n",
    ") :\n",
    "\n",
    "    batch_size = states.size(0)\n",
    "\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_indexes = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield states[rand_indexes], actions[rand_indexes], log_probs[rand_indexes], returns[rand_indexes], advantages[rand_indexes]\n",
    "\n",
    "def ppo_trainer(\n",
    "    model,\n",
    "    optimizer,\n",
    "    ppo_epochs,\n",
    "    mini_batch_size,\n",
    "    states,\n",
    "    actions,\n",
    "    log_probs,\n",
    "    returns,\n",
    "    advantages,\n",
    "    clip_param = 0.2\n",
    "):\n",
    "\n",
    "    batch_loss = []\n",
    "    for _ in range(ppo_epochs):\n",
    "        for state, action, old_log_probs, return_vals, advantage in ppo_randomizer(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
    "            dist, value = model(state) # Action space and value chosen by the new policy\n",
    "            entropy = dist.entropy().mean() #what is the entropy of my new policy distribution\n",
    "            new_log_probs = dist.log_prob(action) #what is the likelihood that my new policy is to take the old action?\n",
    "\n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "\n",
    "            surrogate_unclipped = ratio * advantage\n",
    "            surrogate_clipped = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
    "\n",
    "            actor_loss = -torch.min(surrogate_unclipped,surrogate_clipped).mean()\n",
    "            critic_loss = (return_vals - value).pow(2).mean()\n",
    "\n",
    "            loss = 0.5 * critic_loss + actor_loss - 0.001* entropy\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_loss.append(loss.item())\n",
    "    \n",
    "    return np.mean(batch_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9f196a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_stats(model, iteration):\n",
    "\n",
    "    all_params = []\n",
    "\n",
    "    for param in model.parameters():\n",
    "        all_params.append(param.data.flatten())\n",
    "    \n",
    "    all_params = torch.cat(all_params)\n",
    "\n",
    "    stats = {\n",
    "        'iteration': iteration,\n",
    "        'total_params': all_params.numel(),\n",
    "        'mean': all_params.mean().item(),\n",
    "        'std': all_params.std().item(),\n",
    "        'min': all_params.min().item(),\n",
    "        'max': all_params.max().item(),\n",
    "        'norm': all_params.norm().item()\n",
    "    }\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "07b13817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iteration': 0, 'total_params': 2563, 'mean': 0.021769622340798378, 'std': 0.0985417440533638, 'min': -0.43270736932754517, 'max': 0.32318469882011414, 'norm': 5.108118534088135}\n",
      "learning iteration : 0\n",
      "--------------------------------------------------\n",
      "Step : 0\n",
      "Current State : [ 0.6919735 -0.7219229 -0.5213363]\n",
      "Currrent Action : tensor([[0.2794]])\n",
      "Next State : [[ 0.6919735]\n",
      " [-0.7219229]\n",
      " [-0.5213363]]\n",
      "Reward : [-0.60940066]\n",
      "--------------------------------------------------\n",
      "Step : 1\n",
      "Current State : [ 0.6565972 -0.7542414 -0.9584169]\n",
      "Currrent Action : tensor([[0.6957]])\n",
      "Next State : [[ 0.6565972]\n",
      " [-0.7542414]\n",
      " [-0.9584169]]\n",
      "Reward : [-0.67822992]\n",
      "--------------------------------------------------\n",
      "Step : 2\n",
      "Current State : [ 0.5972377 -0.8020643 -1.5249134]\n",
      "Currrent Action : tensor([[-0.0054]])\n",
      "Next State : [[ 0.5972377]\n",
      " [-0.8020643]\n",
      " [-1.5249134]]\n",
      "Reward : [-0.82202315]\n",
      "--------------------------------------------------\n",
      "Step : 3\n",
      "Current State : [ 0.5079634 -0.8613786 -2.1446767]\n",
      "Currrent Action : tensor([[-0.1214]])\n",
      "Next State : [[ 0.5079634]\n",
      " [-0.8613786]\n",
      " [-2.1446767]]\n",
      "Reward : [-1.09883461]\n",
      "--------------------------------------------------\n",
      "Step : 4\n",
      "Current State : [ 0.36984682 -0.92909276 -3.0794935 ]\n",
      "Currrent Action : tensor([[-1.9252]])\n",
      "Next State : [[ 0.36984682]\n",
      " [-0.92909276]\n",
      " [-3.0794935 ]]\n",
      "Reward : [-1.54106754]\n",
      "--------------------------------------------------\n",
      "Step : 5\n",
      "Current State : [ 0.20055091 -0.9796833  -3.5384784 ]\n",
      "Currrent Action : tensor([[1.5856]])\n",
      "Next State : [[ 0.20055091]\n",
      " [-0.9796833 ]\n",
      " [-3.5384784 ]]\n",
      "Reward : [-2.37159211]\n",
      "--------------------------------------------------\n",
      "Step : 6\n",
      "Current State : [-0.00898751 -0.9999596  -4.2181573 ]\n",
      "Currrent Action : tensor([[0.3672]])\n",
      "Next State : [[-0.00898751]\n",
      " [-0.9999596 ]\n",
      " [-4.2181573 ]]\n",
      "Reward : [-3.12603945]\n",
      "--------------------------------------------------\n",
      "Step : 7\n",
      "Current State : [-0.25506064 -0.9669251  -4.978456  ]\n",
      "Currrent Action : tensor([[-0.0689]])\n",
      "Next State : [[-0.25506064]\n",
      " [-0.9669251 ]\n",
      " [-4.978456  ]]\n",
      "Reward : [-4.27500719]\n",
      "--------------------------------------------------\n",
      "Step : 8\n",
      "Current State : [-0.50629777 -0.8623587  -5.459516  ]\n",
      "Currrent Action : tensor([[1.6276]])\n",
      "Next State : [[-0.50629777]\n",
      " [-0.8623587 ]\n",
      " [-5.459516  ]]\n",
      "Reward : [-5.82531954]\n",
      "--------------------------------------------------\n",
      "Step : 9\n",
      "Current State : [-0.73911494 -0.67357934 -6.017385  ]\n",
      "Currrent Action : tensor([[0.5927]])\n",
      "Next State : [[-0.73911494]\n",
      " [-0.67357934]\n",
      " [-6.017385  ]]\n",
      "Reward : [-7.39805227]\n",
      "--------------------------------------------------\n",
      "Step : 10\n",
      "Current State : [-0.909834   -0.41497234 -6.2225695 ]\n",
      "Currrent Action : tensor([[2.0451]])\n",
      "Next State : [[-0.909834  ]\n",
      " [-0.41497234]\n",
      " [-6.2225695 ]]\n",
      "Reward : [-9.39714712]\n",
      "--------------------------------------------------\n",
      "Step : 11\n",
      "Current State : [-0.9955287 -0.0944595 -6.6662374]\n",
      "Currrent Action : tensor([[-0.8829]])\n",
      "Next State : [[-0.9955287]\n",
      " [-0.0944595]\n",
      " [-6.6662374]]\n",
      "Reward : [-11.23687697]\n",
      "--------------------------------------------------\n",
      "Step : 12\n",
      "Current State : [-0.97029257  0.24193458 -6.7791944 ]\n",
      "Currrent Action : tensor([[-0.2808]])\n",
      "Next State : [[-0.97029257]\n",
      " [ 0.24193458]\n",
      " [-6.7791944 ]]\n",
      "Reward : [-13.72811148]\n",
      "--------------------------------------------------\n",
      "Step : 13\n",
      "Current State : [-0.84199494  0.5394854  -6.509333  ]\n",
      "Currrent Action : tensor([[0.5894]])\n",
      "Next State : [[-0.84199494]\n",
      " [ 0.5394854 ]\n",
      " [-6.509333  ]]\n",
      "Reward : [-12.99005686]\n",
      "--------------------------------------------------\n",
      "Step : 14\n",
      "Current State : [-0.6472379   0.76228803 -5.9403014 ]\n",
      "Currrent Action : tensor([[1.0961]])\n",
      "Next State : [[-0.6472379 ]\n",
      " [ 0.76228803]\n",
      " [-5.9403014 ]]\n",
      "Reward : [-10.85232785]\n",
      "--------------------------------------------------\n",
      "Step : 15\n",
      "Current State : [-0.42208588  0.90655583 -5.3642087 ]\n",
      "Currrent Action : tensor([[0.0292]])\n",
      "Next State : [[-0.42208588]\n",
      " [ 0.90655583]\n",
      " [-5.3642087 ]]\n",
      "Reward : [-8.70321451]\n",
      "--------------------------------------------------\n",
      "Step : 16\n",
      "Current State : [-0.20301661  0.97917527 -4.626146  ]\n",
      "Currrent Action : tensor([[0.3876]])\n",
      "Next State : [[-0.20301661]\n",
      " [ 0.97917527]\n",
      " [-4.626146  ]]\n",
      "Reward : [-6.90383186]\n",
      "--------------------------------------------------\n",
      "Step : 17\n",
      "Current State : [-0.01517106  0.9998849  -3.7853212 ]\n",
      "Currrent Action : tensor([[0.7096]])\n",
      "Next State : [[-0.01517106]\n",
      " [ 0.9998849 ]\n",
      " [-3.7853212 ]]\n",
      "Reward : [-5.29208175]\n",
      "--------------------------------------------------\n",
      "Step : 18\n",
      "Current State : [ 0.13477878  0.9908757  -3.007237  ]\n",
      "Currrent Action : tensor([[0.1878]])\n",
      "Next State : [[ 0.13477878]\n",
      " [ 0.9908757 ]\n",
      " [-3.007237  ]]\n",
      "Reward : [-3.94819532]\n",
      "--------------------------------------------------\n",
      "Step : 19\n",
      "Current State : [ 0.23749664  0.97138834 -2.0919547 ]\n",
      "Currrent Action : tensor([[1.1475]])\n",
      "Next State : [[ 0.23749664]\n",
      " [ 0.97138834]\n",
      " [-2.0919547 ]]\n",
      "Reward : [-2.96662908]\n",
      "--------------------------------------------------\n",
      "Step : 20\n",
      "Current State : [ 0.3062268  0.9519586 -1.4287783]\n",
      "Currrent Action : tensor([[-0.4358]])\n",
      "Next State : [[ 0.3062268]\n",
      " [ 0.9519586]\n",
      " [-1.4287783]]\n",
      "Reward : [-2.20940069]\n",
      "--------------------------------------------------\n",
      "Step : 21\n",
      "Current State : [ 0.3388919   0.94082534 -0.6902395 ]\n",
      "Currrent Action : tensor([[0.1638]])\n",
      "Next State : [[ 0.3388919 ]\n",
      " [ 0.94082534]\n",
      " [-0.6902395 ]]\n",
      "Reward : [-1.79068282]\n",
      "--------------------------------------------------\n",
      "Step : 22\n",
      "Current State : [ 0.3473359   0.9377408  -0.17979567]\n",
      "Currrent Action : tensor([[-1.3012]])\n",
      "Next State : [[ 0.3473359 ]\n",
      " [ 0.9377408 ]\n",
      " [-0.17979567]]\n",
      "Reward : [-1.55010195]\n",
      "--------------------------------------------------\n",
      "Step : 23\n",
      "Current State : [0.30844048 0.95124364 0.82350993]\n",
      "Currrent Action : tensor([[2.9406]])\n",
      "Next State : [[0.30844048]\n",
      " [0.95124364]\n",
      " [0.82350993]]\n",
      "Reward : [-1.48605332]\n",
      "--------------------------------------------------\n",
      "Step : 24\n",
      "Current State : [0.23458333 0.972096   1.5352641 ]\n",
      "Currrent Action : tensor([[-0.0112]])\n",
      "Next State : [[0.23458333]\n",
      " [0.972096  ]\n",
      " [1.5352641 ]]\n",
      "Reward : [-1.64847745]\n",
      "--------------------------------------------------\n",
      "Step : 25\n",
      "Current State : [0.129913  0.9915254 2.1301737]\n",
      "Currrent Action : tensor([[-0.8944]])\n",
      "Next State : [[0.129913 ]\n",
      " [0.9915254]\n",
      " [2.1301737]]\n",
      "Reward : [-2.01607668]\n",
      "--------------------------------------------------\n",
      "Step : 26\n",
      "Current State : [-0.01465291  0.99989265  2.8986933 ]\n",
      "Currrent Action : tensor([[0.1658]])\n",
      "Next State : [[-0.01465291]\n",
      " [ 0.99989265]\n",
      " [ 2.8986933 ]]\n",
      "Reward : [-2.52887518]\n",
      "--------------------------------------------------\n",
      "Step : 27\n",
      "Current State : [-0.19096152  0.98159754  3.5497634 ]\n",
      "Currrent Action : tensor([[-0.6590]])\n",
      "Next State : [[-0.19096152]\n",
      " [ 0.98159754]\n",
      " [ 3.5497634 ]]\n",
      "Reward : [-3.35432757]\n",
      "--------------------------------------------------\n",
      "Step : 28\n",
      "Current State : [-0.3953125  0.9185467  4.2853284]\n",
      "Currrent Action : tensor([[-0.0042]])\n",
      "Next State : [[-0.3953125]\n",
      " [ 0.9185467]\n",
      " [ 4.2853284]]\n",
      "Reward : [-4.3680323]\n",
      "--------------------------------------------------\n",
      "Step : 29\n",
      "Current State : [-0.6070504  0.7946633  4.91871  ]\n",
      "Currrent Action : tensor([[-0.3702]])\n",
      "Next State : [[-0.6070504]\n",
      " [ 0.7946633]\n",
      " [ 4.91871  ]]\n",
      "Reward : [-5.74587796]\n",
      "--------------------------------------------------\n",
      "Step : 30\n",
      "Current State : [-0.7945365  0.6072164  5.3180056]\n",
      "Currrent Action : tensor([[-1.3113]])\n",
      "Next State : [[-0.7945365]\n",
      " [ 0.6072164]\n",
      " [ 5.3180056]]\n",
      "Reward : [-7.36344149]\n",
      "--------------------------------------------------\n",
      "Step : 31\n",
      "Current State : [-0.9397556  0.341847   6.0734177]\n",
      "Currrent Action : tensor([[2.0702]])\n",
      "Next State : [[-0.9397556]\n",
      " [ 0.341847 ]\n",
      " [ 6.0734177]]\n",
      "Reward : [-9.02743913]\n",
      "--------------------------------------------------\n",
      "Step : 32\n",
      "Current State : [-0.9997881   0.02058601  6.565883  ]\n",
      "Currrent Action : tensor([[1.5739]])\n",
      "Next State : [[-0.9997881 ]\n",
      " [ 0.02058601]\n",
      " [ 6.565883  ]]\n",
      "Reward : [-11.49035246]\n",
      "--------------------------------------------------\n",
      "Step : 33\n",
      "Current State : [-0.9541122  -0.29944935  6.4940586 ]\n",
      "Currrent Action : tensor([[-0.5818]])\n",
      "Next State : [[-0.9541122 ]\n",
      " [-0.29944935]\n",
      " [ 6.4940586 ]]\n",
      "Reward : [-14.05209378]\n",
      "--------------------------------------------------\n",
      "Step : 34\n",
      "Current State : [-0.8132853 -0.5818652  6.3380947]\n",
      "Currrent Action : tensor([[0.4575]])\n",
      "Next State : [[-0.8132853]\n",
      " [-0.5818652]\n",
      " [ 6.3380947]]\n",
      "Reward : [-12.26876581]\n",
      "--------------------------------------------------\n",
      "Step : 35\n",
      "Current State : [-0.6104714 -0.7920383  5.86241  ]\n",
      "Currrent Action : tensor([[-0.2619]])\n",
      "Next State : [[-0.6104714]\n",
      " [-0.7920383]\n",
      " [ 5.86241  ]]\n",
      "Reward : [-10.37049862]\n",
      "--------------------------------------------------\n",
      "Step : 36\n",
      "Current State : [-0.37714666 -0.92615354  5.3988423 ]\n",
      "Currrent Action : tensor([[0.8697]])\n",
      "Next State : [[-0.37714666]\n",
      " [-0.92615354]\n",
      " [ 5.3988423 ]]\n",
      "Reward : [-8.39908366]\n",
      "--------------------------------------------------\n",
      "Step : 37\n",
      "Current State : [-0.1460867 -0.9892718  4.802043 ]\n",
      "Currrent Action : tensor([[0.6521]])\n",
      "Next State : [[-0.1460867]\n",
      " [-0.9892718]\n",
      " [ 4.802043 ]]\n",
      "Reward : [-6.74701995]\n",
      "--------------------------------------------------\n",
      "Step : 38\n",
      "Current State : [ 0.05970621 -0.998216    4.1270623 ]\n",
      "Currrent Action : tensor([[0.4465]])\n",
      "Next State : [[ 0.05970621]\n",
      " [-0.998216  ]\n",
      " [ 4.1270623 ]]\n",
      "Reward : [-5.25565018]\n",
      "--------------------------------------------------\n",
      "Step : 39\n",
      "Current State : [ 0.21914263 -0.9756928   3.2238784 ]\n",
      "Currrent Action : tensor([[-1.0301]])\n",
      "Next State : [[ 0.21914263]\n",
      " [-0.9756928 ]\n",
      " [ 3.2238784 ]]\n",
      "Reward : [-3.98761141]\n",
      "--------------------------------------------------\n",
      "Step : 40\n",
      "Current State : [ 0.35210004 -0.9359624   2.7775645 ]\n",
      "Currrent Action : tensor([[1.9030]])\n",
      "Next State : [[ 0.35210004]\n",
      " [-0.9359624 ]\n",
      " [ 2.7775645 ]]\n",
      "Reward : [-2.86508451]\n",
      "--------------------------------------------------\n",
      "Step : 41\n",
      "Current State : [ 0.44631356 -0.89487666  2.0565538 ]\n",
      "Currrent Action : tensor([[-0.1269]])\n",
      "Next State : [[ 0.44631356]\n",
      " [-0.89487666]\n",
      " [ 2.0565538 ]]\n",
      "Reward : [-2.23798107]\n",
      "--------------------------------------------------\n",
      "Step : 42\n",
      "Current State : [ 0.5041368 -0.8636238  1.3148116]\n",
      "Currrent Action : tensor([[-0.4706]])\n",
      "Next State : [[ 0.5041368]\n",
      " [-0.8636238]\n",
      " [ 1.3148116]]\n",
      "Reward : [-1.65116972]\n",
      "--------------------------------------------------\n",
      "Step : 43\n",
      "Current State : [ 0.52451324 -0.8514023   0.47522336]\n",
      "Currrent Action : tensor([[-1.2791]])\n",
      "Next State : [[ 0.52451324]\n",
      " [-0.8514023 ]\n",
      " [ 0.47522336]]\n",
      "Reward : [-1.26113643]\n",
      "--------------------------------------------------\n",
      "Step : 44\n",
      "Current State : [ 0.5113495  -0.85937285 -0.30777898]\n",
      "Currrent Action : tensor([[-0.9630]])\n",
      "Next State : [[ 0.5113495 ]\n",
      " [-0.85937285]\n",
      " [-0.30777898]]\n",
      "Reward : [-1.06116503]\n",
      "--------------------------------------------------\n",
      "Step : 45\n",
      "Current State : [ 0.46127892 -0.8872552  -1.1463661 ]\n",
      "Currrent Action : tensor([[-1.2937]])\n",
      "Next State : [[ 0.46127892]\n",
      " [-0.8872552 ]\n",
      " [-1.1463661 ]]\n",
      "Reward : [-1.08038924]\n",
      "--------------------------------------------------\n",
      "Step : 46\n",
      "Current State : [ 0.37418455 -0.9273543  -1.9183757 ]\n",
      "Currrent Action : tensor([[-0.7105]])\n",
      "Next State : [[ 0.37418455]\n",
      " [-0.9273543 ]\n",
      " [-1.9183757 ]]\n",
      "Reward : [-1.32298747]\n",
      "--------------------------------------------------\n",
      "Step : 47\n",
      "Current State : [ 0.24829212 -0.9686852  -2.6520104 ]\n",
      "Currrent Action : tensor([[-0.2541]])\n",
      "Next State : [[ 0.24829212]\n",
      " [-0.9686852 ]\n",
      " [-2.6520104 ]]\n",
      "Reward : [-1.77771261]\n",
      "--------------------------------------------------\n",
      "Step : 48\n",
      "Current State : [ 0.08043671 -0.9967597  -3.4078612 ]\n",
      "Currrent Action : tensor([[-0.1956]])\n",
      "Next State : [[ 0.08043671]\n",
      " [-0.9967597 ]\n",
      " [-3.4078612 ]]\n",
      "Reward : [-2.44543623]\n",
      "--------------------------------------------------\n",
      "Step : 49\n",
      "Current State : [-0.11297385 -0.993598   -3.8747854 ]\n",
      "Currrent Action : tensor([[1.8710]])\n",
      "Next State : [[-0.11297385]\n",
      " [-0.993598  ]\n",
      " [-3.8747854 ]]\n",
      "Reward : [-3.38576483]\n",
      "--------------------------------------------------\n",
      "Step : 50\n",
      "Current State : [-0.32330006 -0.9462965  -4.319984  ]\n",
      "Currrent Action : tensor([[2.1834]])\n",
      "Next State : [[-0.32330006]\n",
      " [-0.9462965 ]\n",
      " [-4.319984  ]]\n",
      "Reward : [-4.34129224]\n",
      "--------------------------------------------------\n",
      "Step : 51\n",
      "Current State : [-0.5479886 -0.8364858 -5.01486  ]\n",
      "Currrent Action : tensor([[0.0990]])\n",
      "Next State : [[-0.5479886]\n",
      " [-0.8364858]\n",
      " [-5.01486  ]]\n",
      "Reward : [-5.47627799]\n",
      "--------------------------------------------------\n",
      "Step : 52\n",
      "Current State : [-0.7599316 -0.650003  -5.665004 ]\n",
      "Currrent Action : tensor([[-0.1519]])\n",
      "Next State : [[-0.7599316]\n",
      " [-0.650003 ]\n",
      " [-5.665004 ]]\n",
      "Reward : [-7.14064863]\n",
      "--------------------------------------------------\n",
      "Step : 53\n",
      "Current State : [-0.919893   -0.39216942 -6.0919924 ]\n",
      "Currrent Action : tensor([[0.4034]])\n",
      "Next State : [[-0.919893  ]\n",
      " [-0.39216942]\n",
      " [-6.0919924 ]]\n",
      "Reward : [-9.13376616]\n",
      "--------------------------------------------------\n",
      "Step : 54\n",
      "Current State : [-0.99522    -0.09765802 -6.1034966 ]\n",
      "Currrent Action : tensor([[1.8842]])\n",
      "Next State : [[-0.99522   ]\n",
      " [-0.09765802]\n",
      " [-6.1034966 ]]\n",
      "Reward : [-11.21473855]\n",
      "--------------------------------------------------\n",
      "Step : 55\n",
      "Current State : [-0.9792397   0.20270558 -6.0386796 ]\n",
      "Currrent Action : tensor([[0.9204]])\n",
      "Next State : [[-0.9792397 ]\n",
      " [ 0.20270558]\n",
      " [-6.0386796 ]]\n",
      "Reward : [-12.99070341]\n",
      "--------------------------------------------------\n",
      "Step : 56\n",
      "Current State : [-0.87636393  0.48164958 -5.968319  ]\n",
      "Currrent Action : tensor([[-0.5445]])\n",
      "Next State : [[-0.87636393]\n",
      " [ 0.48164958]\n",
      " [-5.968319  ]]\n",
      "Reward : [-12.27560692]\n",
      "--------------------------------------------------\n",
      "Step : 57\n",
      "Current State : [-0.7047971   0.70940894 -5.722464  ]\n",
      "Currrent Action : tensor([[-0.7692]])\n",
      "Next State : [[-0.7047971 ]\n",
      " [ 0.70940894]\n",
      " [-5.722464  ]]\n",
      "Reward : [-10.52729479]\n",
      "--------------------------------------------------\n",
      "Step : 58\n",
      "Current State : [-0.49583736  0.8684154  -5.266766  ]\n",
      "Currrent Action : tensor([[-0.5091]])\n",
      "Next State : [[-0.49583736]\n",
      " [ 0.8684154 ]\n",
      " [-5.266766  ]]\n",
      "Reward : [-8.81121433]\n",
      "--------------------------------------------------\n",
      "Step : 59\n",
      "Current State : [-0.28854415  0.9574666  -4.521857  ]\n",
      "Currrent Action : tensor([[0.6240]])\n",
      "Next State : [[-0.28854415]\n",
      " [ 0.9574666 ]\n",
      " [-4.521857  ]]\n",
      "Reward : [-7.14067953]\n",
      "--------------------------------------------------\n",
      "Step : 60\n",
      "Current State : [-0.10053894  0.9949331  -3.8399372 ]\n",
      "Currrent Action : tensor([[-0.2412]])\n",
      "Next State : [[-0.10053894]\n",
      " [ 0.9949331 ]\n",
      " [-3.8399372 ]]\n",
      "Reward : [-5.51741791]\n",
      "--------------------------------------------------\n",
      "Step : 61\n",
      "Current State : [ 0.04885457  0.9988059  -2.9916623 ]\n",
      "Currrent Action : tensor([[0.6805]])\n",
      "Next State : [[ 0.04885457]\n",
      " [ 0.9988059 ]\n",
      " [-2.9916623 ]]\n",
      "Reward : [-4.26890529]\n",
      "--------------------------------------------------\n",
      "Step : 62\n",
      "Current State : [ 0.1574367  0.9875291 -2.1844082]\n",
      "Currrent Action : tensor([[0.3877]])\n",
      "Next State : [[ 0.1574367]\n",
      " [ 0.9875291]\n",
      " [-2.1844082]]\n",
      "Reward : [-3.21140211]\n",
      "--------------------------------------------------\n",
      "Step : 63\n",
      "Current State : [ 0.22783054  0.9737008  -1.4350921 ]\n",
      "Currrent Action : tensor([[0.0578]])\n",
      "Next State : [[ 0.22783054]\n",
      " [ 0.9737008 ]\n",
      " [-1.4350921 ]]\n",
      "Reward : [-2.47289392]\n",
      "--------------------------------------------------\n",
      "Step : 64\n",
      "Current State : [ 0.25295404  0.9674783  -0.51766676]\n",
      "Currrent Action : tensor([[1.2477]])\n",
      "Next State : [[ 0.25295404]\n",
      " [ 0.9674783 ]\n",
      " [-0.51766676]]\n",
      "Reward : [-2.00564523]\n",
      "--------------------------------------------------\n",
      "Step : 65\n",
      "Current State : [0.24497458 0.9695295  0.16477816]\n",
      "Currrent Action : tensor([[-0.2878]])\n",
      "Next State : [[0.24497458]\n",
      " [0.9695295 ]\n",
      " [0.16477816]]\n",
      "Reward : [-1.75627388]\n",
      "--------------------------------------------------\n",
      "Step : 66\n",
      "Current State : [0.19410668 0.9809804  1.0429348 ]\n",
      "Currrent Action : tensor([[1.0067]])\n",
      "Next State : [[0.19410668]\n",
      " [0.9809804 ]\n",
      " [1.0429348 ]]\n",
      "Reward : [-1.75485913]\n",
      "--------------------------------------------------\n",
      "Step : 67\n",
      "Current State : [0.09510209 0.99546754 2.0020142 ]\n",
      "Currrent Action : tensor([[1.4890]])\n",
      "Next State : [[0.09510209]\n",
      " [0.99546754]\n",
      " [2.0020142 ]]\n",
      "Reward : [-2.00284986]\n",
      "--------------------------------------------------\n",
      "Step : 68\n",
      "Current State : [-0.04069198  0.99917173  2.718985  ]\n",
      "Currrent Action : tensor([[-0.1975]])\n",
      "Next State : [[-0.04069198]\n",
      " [ 0.99917173]\n",
      " [ 2.718985  ]]\n",
      "Reward : [-2.57809378]\n",
      "--------------------------------------------------\n",
      "Step : 69\n",
      "Current State : [-0.205302   0.9786987  3.3213809]\n",
      "Currrent Action : tensor([[-0.9799]])\n",
      "Next State : [[-0.205302 ]\n",
      " [ 0.9786987]\n",
      " [ 3.3213809]]\n",
      "Reward : [-3.33717889]\n",
      "--------------------------------------------------\n",
      "Step : 70\n",
      "Current State : [-0.397465   0.9176173  4.0396066]\n",
      "Currrent Action : tensor([[-0.1053]])\n",
      "Next State : [[-0.397465 ]\n",
      " [ 0.9176173]\n",
      " [ 4.0396066]]\n",
      "Reward : [-4.26291826]\n",
      "--------------------------------------------------\n",
      "Step : 71\n",
      "Current State : [-0.60490423  0.7962982   4.817861  ]\n",
      "Currrent Action : tensor([[0.6003]])\n",
      "Next State : [[-0.60490423]\n",
      " [ 0.7962982 ]\n",
      " [ 4.817861  ]]\n",
      "Reward : [-5.55081619]\n",
      "--------------------------------------------------\n",
      "Step : 72\n",
      "Current State : [-0.79438394  0.607416    5.3669415 ]\n",
      "Currrent Action : tensor([[-0.3210]])\n",
      "Next State : [[-0.79438394]\n",
      " [ 0.607416  ]\n",
      " [ 5.3669415 ]]\n",
      "Reward : [-7.2516439]\n",
      "--------------------------------------------------\n",
      "Step : 73\n",
      "Current State : [-0.93575853  0.35264134  5.848227  ]\n",
      "Currrent Action : tensor([[0.1715]])\n",
      "Next State : [[-0.93575853]\n",
      " [ 0.35264134]\n",
      " [ 5.848227  ]]\n",
      "Reward : [-9.07450625]\n",
      "--------------------------------------------------\n",
      "Step : 74\n",
      "Current State : [-0.9980056   0.06312481  5.9445095 ]\n",
      "Currrent Action : tensor([[-1.1213]])\n",
      "Next State : [[-0.9980056 ]\n",
      " [ 0.06312481]\n",
      " [ 5.9445095 ]]\n",
      "Reward : [-11.1565085]\n",
      "--------------------------------------------------\n",
      "Step : 75\n",
      "Current State : [-0.96995664 -0.24327779  6.1782107 ]\n",
      "Currrent Action : tensor([[1.2424]])\n",
      "Next State : [[-0.96995664]\n",
      " [-0.24327779]\n",
      " [ 6.1782107 ]]\n",
      "Reward : [-13.01196883]\n",
      "--------------------------------------------------\n",
      "Step : 76\n",
      "Current State : [-0.8507113 -0.5256333  6.1543126]\n",
      "Currrent Action : tensor([[1.0571]])\n",
      "Next State : [[-0.8507113]\n",
      " [-0.5256333]\n",
      " [ 6.1543126]]\n",
      "Reward : [-12.20408721]\n",
      "--------------------------------------------------\n",
      "Step : 77\n",
      "Current State : [-0.6759462 -0.736951   5.501783 ]\n",
      "Currrent Action : tensor([[-1.7220]])\n",
      "Next State : [[-0.6759462]\n",
      " [-0.736951 ]\n",
      " [ 5.501783 ]]\n",
      "Reward : [-10.48895559]\n",
      "--------------------------------------------------\n",
      "Step : 78\n",
      "Current State : [-0.47369117 -0.880691    4.9754105 ]\n",
      "Currrent Action : tensor([[0.1756]])\n",
      "Next State : [[-0.47369117]\n",
      " [-0.880691  ]\n",
      " [ 4.9754105 ]]\n",
      "Reward : [-8.37716553]\n",
      "--------------------------------------------------\n",
      "Step : 79\n",
      "Current State : [-0.2751294 -0.9614072  4.29506  ]\n",
      "Currrent Action : tensor([[-0.1322]])\n",
      "Next State : [[-0.2751294]\n",
      " [-0.9614072]\n",
      " [ 4.29506  ]]\n",
      "Reward : [-6.73671384]\n",
      "--------------------------------------------------\n",
      "Step : 80\n",
      "Current State : [-0.11326259 -0.9935651   3.3043625 ]\n",
      "Currrent Action : tensor([[-1.7976]])\n",
      "Next State : [[-0.11326259]\n",
      " [-0.9935651 ]\n",
      " [ 3.3043625 ]]\n",
      "Reward : [-5.26871216]\n",
      "--------------------------------------------------\n",
      "Step : 81\n",
      "Current State : [ 0.00695331 -0.9999758   2.4091904 ]\n",
      "Currrent Action : tensor([[-1.0000]])\n",
      "Next State : [[ 0.00695331]\n",
      " [-0.9999758 ]\n",
      " [ 2.4091904 ]]\n",
      "Reward : [-3.92975609]\n",
      "--------------------------------------------------\n",
      "Step : 82\n",
      "Current State : [ 0.08912835 -0.99602014  1.6458683 ]\n",
      "Currrent Action : tensor([[-0.0889]])\n",
      "Next State : [[ 0.08912835]\n",
      " [-0.99602014]\n",
      " [ 1.6458683 ]]\n",
      "Reward : [-3.02603258]\n",
      "--------------------------------------------------\n",
      "Step : 83\n",
      "Current State : [ 0.13612135 -0.9906922   0.9459697 ]\n",
      "Currrent Action : tensor([[0.3141]])\n",
      "Next State : [[ 0.13612135]\n",
      " [-0.9906922 ]\n",
      " [ 0.9459697 ]]\n",
      "Reward : [-2.46597597]\n",
      "--------------------------------------------------\n",
      "Step : 84\n",
      "Current State : [ 0.14221536 -0.98983574  0.123078  ]\n",
      "Currrent Action : tensor([[-0.5325]])\n",
      "Next State : [[ 0.14221536]\n",
      " [-0.98983574]\n",
      " [ 0.123078  ]]\n",
      "Reward : [-2.14684552]\n",
      "--------------------------------------------------\n",
      "Step : 85\n",
      "Current State : [ 0.12042127 -0.99272287 -0.4396987 ]\n",
      "Currrent Action : tensor([[1.1973]])\n",
      "Next State : [[ 0.12042127]\n",
      " [-0.99272287]\n",
      " [-0.4396987 ]]\n",
      "Reward : [-2.04240994]\n",
      "--------------------------------------------------\n",
      "Step : 86\n",
      "Current State : [ 0.07055531 -0.99750787 -1.002005  ]\n",
      "Currrent Action : tensor([[1.2149]])\n",
      "Next State : [[ 0.07055531]\n",
      " [-0.99750787]\n",
      " [-1.002005  ]]\n",
      "Reward : [-2.12354759]\n",
      "--------------------------------------------------\n",
      "Step : 87\n",
      "Current State : [-0.01958643 -0.9998082  -1.8040333 ]\n",
      "Currrent Action : tensor([[-0.3593]])\n",
      "Next State : [[-0.01958643]\n",
      " [-0.9998082 ]\n",
      " [-1.8040333 ]]\n",
      "Reward : [-2.35107758]\n",
      "--------------------------------------------------\n",
      "Step : 88\n",
      "Current State : [-0.1415857 -0.989926  -2.4495077]\n",
      "Currrent Action : tensor([[0.6959]])\n",
      "Next State : [[-0.1415857]\n",
      " [-0.989926 ]\n",
      " [-2.4495077]]\n",
      "Reward : [-2.85525917]\n",
      "--------------------------------------------------\n",
      "Step : 89\n",
      "Current State : [-0.29338944 -0.955993   -3.1141467 ]\n",
      "Currrent Action : tensor([[0.5187]])\n",
      "Next State : [[-0.29338944]\n",
      " [-0.955993  ]\n",
      " [-3.1141467 ]]\n",
      "Reward : [-3.5341651]\n",
      "--------------------------------------------------\n",
      "Step : 90\n",
      "Current State : [-0.4689601  -0.88321936 -3.8068526 ]\n",
      "Currrent Action : tensor([[0.1619]])\n",
      "Next State : [[-0.4689601 ]\n",
      " [-0.88321936]\n",
      " [-3.8068526 ]]\n",
      "Reward : [-4.46135875]\n",
      "--------------------------------------------------\n",
      "Step : 91\n",
      "Current State : [-0.6500726  -0.75987214 -4.391339  ]\n",
      "Currrent Action : tensor([[0.5195]])\n",
      "Next State : [[-0.6500726 ]\n",
      " [-0.75987214]\n",
      " [-4.391339  ]]\n",
      "Reward : [-5.6885903]\n",
      "--------------------------------------------------\n",
      "Step : 92\n",
      "Current State : [-0.8169406 -0.5767218 -4.9681234]\n",
      "Currrent Action : tensor([[-0.0459]])\n",
      "Next State : [[-0.8169406]\n",
      " [-0.5767218]\n",
      " [-4.9681234]]\n",
      "Reward : [-7.11984182]\n",
      "--------------------------------------------------\n",
      "Step : 93\n",
      "Current State : [-0.9386316  -0.34492126 -5.2511063 ]\n",
      "Currrent Action : tensor([[0.9971]])\n",
      "Next State : [[-0.9386316 ]\n",
      " [-0.34492126]\n",
      " [-5.2511063 ]]\n",
      "Reward : [-8.85435409]\n",
      "--------------------------------------------------\n",
      "Step : 94\n",
      "Current State : [-0.99585974 -0.09090322 -5.2225204 ]\n",
      "Currrent Action : tensor([[1.9152]])\n",
      "Next State : [[-0.99585974]\n",
      " [-0.09090322]\n",
      " [-5.2225204 ]]\n",
      "Reward : [-10.54204237]\n",
      "--------------------------------------------------\n",
      "Step : 95\n",
      "Current State : [-0.9866679   0.16274668 -5.090054  ]\n",
      "Currrent Action : tensor([[1.3376]])\n",
      "Next State : [[-0.9866679 ]\n",
      " [ 0.16274668]\n",
      " [-5.090054  ]]\n",
      "Reward : [-12.03520041]\n",
      "--------------------------------------------------\n",
      "Step : 96\n",
      "Current State : [-0.9158883   0.40143323 -4.9921474 ]\n",
      "Currrent Action : tensor([[-0.1610]])\n",
      "Next State : [[-0.9158883 ]\n",
      " [ 0.40143323]\n",
      " [-4.9921474 ]]\n",
      "Reward : [-11.46008256]\n",
      "--------------------------------------------------\n",
      "Step : 97\n",
      "Current State : [-0.7927118  0.6095966 -4.8494086]\n",
      "Currrent Action : tensor([[-1.0556]])\n",
      "Next State : [[-0.7927118]\n",
      " [ 0.6095966]\n",
      " [-4.8494086]]\n",
      "Reward : [-9.93804261]\n",
      "--------------------------------------------------\n",
      "Step : 98\n",
      "Current State : [-0.6457945  0.7635113 -4.263626 ]\n",
      "Currrent Action : tensor([[0.8572]])\n",
      "Next State : [[-0.6457945]\n",
      " [ 0.7635113]\n",
      " [-4.263626 ]]\n",
      "Reward : [-8.5328114]\n",
      "--------------------------------------------------\n",
      "Step : 99\n",
      "Current State : [-0.48995832  0.8717459  -3.8004293 ]\n",
      "Currrent Action : tensor([[-0.7296]])\n",
      "Next State : [[-0.48995832]\n",
      " [ 0.8717459 ]\n",
      " [-3.8004293 ]]\n",
      "Reward : [-6.98427447]\n",
      "learning iteration : 1\n",
      "--------------------------------------------------\n",
      "Step : 0\n",
      "Current State : [-0.536921   -0.8436325   0.27235794]\n",
      "Currrent Action : tensor([[0.6955]])\n",
      "Next State : [[-0.536921  ]\n",
      " [-0.8436325 ]\n",
      " [ 0.27235794]]\n",
      "Reward : [-4.69137141]\n",
      "--------------------------------------------------\n",
      "Step : 1\n",
      "Current State : [-0.556812   -0.8306385  -0.47519276]\n",
      "Currrent Action : tensor([[-0.7655]])\n",
      "Next State : [[-0.556812  ]\n",
      " [-0.8306385 ]\n",
      " [-0.47519276]]\n",
      "Reward : [-4.57725013]\n",
      "--------------------------------------------------\n",
      "Step : 2\n",
      "Current State : [-0.59073496 -0.8068657  -0.8285316 ]\n",
      "Currrent Action : tensor([[1.7976]])\n",
      "Next State : [[-0.59073496]\n",
      " [-0.8068657 ]\n",
      " [-0.8285316 ]]\n",
      "Reward : [-4.69719918]\n",
      "--------------------------------------------------\n",
      "Step : 3\n",
      "Current State : [-0.65320915 -0.75717753 -1.5969124 ]\n",
      "Currrent Action : tensor([[-1.0882]])\n",
      "Next State : [[-0.65320915]\n",
      " [-0.75717753]\n",
      " [-1.5969124 ]]\n",
      "Reward : [-4.9220076]\n",
      "--------------------------------------------------\n",
      "Step : 4\n",
      "Current State : [-0.7280058  -0.68557096 -2.0718725 ]\n",
      "Currrent Action : tensor([[0.6195]])\n",
      "Next State : [[-0.7280058 ]\n",
      " [-0.68557096]\n",
      " [-2.0718725 ]]\n",
      "Reward : [-5.46571136]\n",
      "--------------------------------------------------\n",
      "Step : 5\n",
      "Current State : [-0.8056997  -0.59232426 -2.4289422 ]\n",
      "Currrent Action : tensor([[1.0474]])\n",
      "Next State : [[-0.8056997 ]\n",
      " [-0.59232426]\n",
      " [-2.4289422 ]]\n",
      "Reward : [-6.1243368]\n",
      "--------------------------------------------------\n",
      "Step : 6\n",
      "Current State : [-0.8803696  -0.47428823 -2.7957041 ]\n",
      "Currrent Action : tensor([[0.5165]])\n",
      "Next State : [[-0.8803696 ]\n",
      " [-0.47428823]\n",
      " [-2.7957041 ]]\n",
      "Reward : [-6.87856193]\n",
      "--------------------------------------------------\n",
      "Step : 7\n",
      "Current State : [-0.9426436 -0.333801  -3.076447 ]\n",
      "Currrent Action : tensor([[0.4998]])\n",
      "Next State : [[-0.9426436]\n",
      " [-0.333801 ]\n",
      " [-3.076447 ]]\n",
      "Reward : [-7.79077021]\n",
      "--------------------------------------------------\n",
      "Step : 8\n",
      "Current State : [-0.9849377  -0.17290936 -3.3310046 ]\n",
      "Currrent Action : tensor([[-0.0280]])\n",
      "Next State : [[-0.9849377 ]\n",
      " [-0.17290936]\n",
      " [-3.3310046 ]]\n",
      "Reward : [-8.79350914]\n",
      "--------------------------------------------------\n",
      "Step : 9\n",
      "Current State : [-0.9999674  -0.00807802 -3.314093  ]\n",
      "Currrent Action : tensor([[0.9773]])\n",
      "Next State : [[-0.9999674 ]\n",
      " [-0.00807802]\n",
      " [-3.314093  ]]\n",
      "Reward : [-9.91840987]\n",
      "--------------------------------------------------\n",
      "Step : 10\n",
      "Current State : [-0.9863784  0.1644922 -3.466426 ]\n",
      "Currrent Action : tensor([[-0.9752]])\n",
      "Next State : [[-0.9863784]\n",
      " [ 0.1644922]\n",
      " [-3.466426 ]]\n",
      "Reward : [-10.91818565]\n",
      "--------------------------------------------------\n",
      "Step : 11\n",
      "Current State : [-0.9498786   0.31261906 -3.054117  ]\n",
      "Currrent Action : tensor([[1.9263]])\n",
      "Next State : [[-0.9498786 ]\n",
      " [ 0.31261906]\n",
      " [-3.054117  ]]\n",
      "Reward : [-10.06397746]\n",
      "--------------------------------------------------\n",
      "Step : 12\n",
      "Current State : [-0.8996757  0.4365588 -2.6764228]\n",
      "Currrent Action : tensor([[0.9549]])\n",
      "Next State : [[-0.8996757]\n",
      " [ 0.4365588]\n",
      " [-2.6764228]]\n",
      "Reward : [-8.90663812]\n",
      "--------------------------------------------------\n",
      "Step : 13\n",
      "Current State : [-0.84629494  0.53271466 -2.2006972 ]\n",
      "Currrent Action : tensor([[0.9887]])\n",
      "Next State : [[-0.84629494]\n",
      " [ 0.53271466]\n",
      " [-2.2006972 ]]\n",
      "Reward : [-7.95244645]\n",
      "--------------------------------------------------\n",
      "Step : 14\n",
      "Current State : [-0.79976684  0.60031074 -1.6416913 ]\n",
      "Currrent Action : tensor([[1.0631]])\n",
      "Next State : [[-0.79976684]\n",
      " [ 0.60031074]\n",
      " [-1.6416913 ]]\n",
      "Reward : [-7.14074117]\n",
      "--------------------------------------------------\n",
      "Step : 15\n",
      "Current State : [-0.76176435  0.6478542  -1.217492  ]\n",
      "Currrent Action : tensor([[-0.1736]])\n",
      "Next State : [[-0.76176435]\n",
      " [ 0.6478542 ]\n",
      " [-1.217492  ]]\n",
      "Reward : [-6.5080657]\n",
      "--------------------------------------------------\n",
      "Step : 16\n",
      "Current State : [-0.74066436  0.67187524 -0.6394697 ]\n",
      "Currrent Action : tensor([[0.6142]])\n",
      "Next State : [[-0.74066436]\n",
      " [ 0.67187524]\n",
      " [-0.6394697 ]]\n",
      "Reward : [-6.08673884]\n",
      "--------------------------------------------------\n",
      "Step : 17\n",
      "Current State : [-0.73463094  0.6784669  -0.17872147]\n",
      "Currrent Action : tensor([[-0.2877]])\n",
      "Next State : [[-0.73463094]\n",
      " [ 0.6784669 ]\n",
      " [-0.17872147]]\n",
      "Reward : [-5.82430234]\n",
      "--------------------------------------------------\n",
      "Step : 18\n",
      "Current State : [-0.7459408   0.6660123   0.33647442]\n",
      "Currrent Action : tensor([[0.0423]])\n",
      "Next State : [[-0.7459408 ]\n",
      " [ 0.6660123 ]\n",
      " [ 0.33647442]]\n",
      "Reward : [-5.74362327]\n",
      "--------------------------------------------------\n",
      "Step : 19\n",
      "Current State : [-0.76871043  0.63959694  0.6975244 ]\n",
      "Currrent Action : tensor([[-0.9231]])\n",
      "Next State : [[-0.76871043]\n",
      " [ 0.63959694]\n",
      " [ 0.6975244 ]]\n",
      "Reward : [-5.83350046]\n",
      "--------------------------------------------------\n",
      "Step : 20\n",
      "Current State : [-0.80515623  0.59306276  1.1823267 ]\n",
      "Currrent Action : tensor([[0.0340]])\n",
      "Next State : [[-0.80515623]\n",
      " [ 0.59306276]\n",
      " [ 1.1823267 ]]\n",
      "Reward : [-6.03949314]\n",
      "--------------------------------------------------\n",
      "Step : 21\n",
      "Current State : [-0.85054886  0.525896    1.6217852 ]\n",
      "Currrent Action : tensor([[-0.0356]])\n",
      "Next State : [[-0.85054886]\n",
      " [ 0.525896  ]\n",
      " [ 1.6217852 ]]\n",
      "Reward : [-6.42351213]\n",
      "--------------------------------------------------\n",
      "Step : 22\n",
      "Current State : [-0.9002248   0.43542543  2.0651484 ]\n",
      "Currrent Action : tensor([[0.3263]])\n",
      "Next State : [[-0.9002248 ]\n",
      " [ 0.43542543]\n",
      " [ 2.0651484 ]]\n",
      "Reward : [-6.95996047]\n",
      "--------------------------------------------------\n",
      "Step : 23\n",
      "Current State : [-0.9437591   0.33063394  2.2707117 ]\n",
      "Currrent Action : tensor([[-0.8067]])\n",
      "Next State : [[-0.9437591 ]\n",
      " [ 0.33063394]\n",
      " [ 2.2707117 ]]\n",
      "Reward : [-7.6690561]\n",
      "--------------------------------------------------\n",
      "Step : 24\n",
      "Current State : [-0.9780888   0.20818824  2.5450597 ]\n",
      "Currrent Action : tensor([[0.1758]])\n",
      "Next State : [[-0.9780888 ]\n",
      " [ 0.20818824]\n",
      " [ 2.5450597 ]]\n",
      "Reward : [-8.38152303]\n",
      "--------------------------------------------------\n",
      "Step : 25\n",
      "Current State : [-0.99787325  0.065184    2.8898396 ]\n",
      "Currrent Action : tensor([[1.2576]])\n",
      "Next State : [[-0.99787325]\n",
      " [ 0.065184  ]\n",
      " [ 2.8898396 ]]\n",
      "Reward : [-9.24517852]\n",
      "--------------------------------------------------\n",
      "Step : 26\n",
      "Current State : [-0.99728787 -0.07359936  2.7779245 ]\n",
      "Currrent Action : tensor([[-1.0720]])\n",
      "Next State : [[-0.99728787]\n",
      " [-0.07359936]\n",
      " [ 2.7779245 ]]\n",
      "Reward : [-10.30027225]\n",
      "--------------------------------------------------\n",
      "Step : 27\n",
      "Current State : [-0.97862935 -0.20563224  2.6688747 ]\n",
      "Currrent Action : tensor([[-0.3590]])\n",
      "Next State : [[-0.97862935]\n",
      " [-0.20563224]\n",
      " [ 2.6688747 ]]\n",
      "Reward : [-10.18398942]\n",
      "--------------------------------------------------\n",
      "Step : 28\n",
      "Current State : [-0.94501054 -0.3270398   2.5211942 ]\n",
      "Currrent Action : tensor([[0.0436]])\n",
      "Next State : [[-0.94501054]\n",
      " [-0.3270398 ]\n",
      " [ 2.5211942 ]]\n",
      "Reward : [-9.32348128]\n",
      "--------------------------------------------------\n",
      "Step : 29\n",
      "Current State : [-0.89933306 -0.4372643   2.3877017 ]\n",
      "Currrent Action : tensor([[0.7452]])\n",
      "Next State : [[-0.89933306]\n",
      " [-0.4372643 ]\n",
      " [ 2.3877017 ]]\n",
      "Reward : [-8.52343849]\n",
      "--------------------------------------------------\n",
      "Step : 30\n",
      "Current State : [-0.84409595 -0.5361921   2.2672985 ]\n",
      "Currrent Action : tensor([[1.3836]])\n",
      "Next State : [[-0.84409595]\n",
      " [-0.5361921 ]\n",
      " [ 2.2672985 ]]\n",
      "Reward : [-7.80295257]\n",
      "--------------------------------------------------\n",
      "Step : 31\n",
      "Current State : [-0.78341776 -0.6214955   2.094615  ]\n",
      "Currrent Action : tensor([[1.5297]])\n",
      "Next State : [[-0.78341776]\n",
      " [-0.6214955 ]\n",
      " [ 2.094615  ]]\n",
      "Reward : [-7.15049692]\n",
      "--------------------------------------------------\n",
      "Step : 32\n",
      "Current State : [-0.7360225  -0.67695713  1.4594069 ]\n",
      "Currrent Action : tensor([[-1.1272]])\n",
      "Next State : [[-0.7360225 ]\n",
      " [-0.67695713]\n",
      " [ 1.4594069 ]]\n",
      "Reward : [-6.54556861]\n",
      "--------------------------------------------------\n",
      "Step : 33\n",
      "Current State : [-0.70741516 -0.70679826  0.8268285 ]\n",
      "Currrent Action : tensor([[-0.8324]])\n",
      "Next State : [[-0.70741516]\n",
      " [-0.70679826]\n",
      " [ 0.8268285 ]]\n",
      "Reward : [-5.96395015]\n",
      "--------------------------------------------------\n",
      "Step : 34\n",
      "Current State : [-0.698032   -0.71606654  0.26377833]\n",
      "Currrent Action : tensor([[-0.2197]])\n",
      "Next State : [[-0.698032  ]\n",
      " [-0.71606654]\n",
      " [ 0.26377833]]\n",
      "Reward : [-5.62212116]\n",
      "--------------------------------------------------\n",
      "Step : 35\n",
      "Current State : [-0.7098889 -0.7043137 -0.333898 ]\n",
      "Currrent Action : tensor([[-0.4042]])\n",
      "Next State : [[-0.7098889]\n",
      " [-0.7043137]\n",
      " [-0.333898 ]]\n",
      "Reward : [-5.49884077]\n",
      "--------------------------------------------------\n",
      "Step : 36\n",
      "Current State : [-0.73328966 -0.6799164  -0.67614776]\n",
      "Currrent Action : tensor([[1.2399]])\n",
      "Next State : [[-0.73328966]\n",
      " [-0.6799164 ]\n",
      " [-0.67614776]]\n",
      "Reward : [-5.58293143]\n",
      "--------------------------------------------------\n",
      "Step : 37\n",
      "Current State : [-0.7723313  -0.63521993 -1.1871054 ]\n",
      "Currrent Action : tensor([[-0.0068]])\n",
      "Next State : [[-0.7723313 ]\n",
      " [-0.63521993]\n",
      " [-1.1871054 ]]\n",
      "Reward : [-5.77668597]\n",
      "--------------------------------------------------\n",
      "Step : 38\n",
      "Current State : [-0.81546056 -0.57881266 -1.4204278 ]\n",
      "Currrent Action : tensor([[1.6206]])\n",
      "Next State : [[-0.81546056]\n",
      " [-0.57881266]\n",
      " [-1.4204278 ]]\n",
      "Reward : [-6.1622261]\n",
      "--------------------------------------------------\n",
      "Step : 39\n",
      "Current State : [-0.8643062 -0.5029659 -1.8048989]\n",
      "Currrent Action : tensor([[0.3309]])\n",
      "Next State : [[-0.8643062]\n",
      " [-0.5029659]\n",
      " [-1.8048989]]\n",
      "Reward : [-6.57406629]\n",
      "--------------------------------------------------\n",
      "Step : 40\n",
      "Current State : [-0.9153478  -0.40266415 -2.2520287 ]\n",
      "Currrent Action : tensor([[-0.4660]])\n",
      "Next State : [[-0.9153478 ]\n",
      " [-0.40266415]\n",
      " [-2.2520287 ]]\n",
      "Reward : [-7.16193699]\n",
      "--------------------------------------------------\n",
      "Step : 41\n",
      "Current State : [-0.9586702 -0.2845196 -2.5184045]\n",
      "Currrent Action : tensor([[0.2375]])\n",
      "Next State : [[-0.9586702]\n",
      " [-0.2845196]\n",
      " [-2.5184045]]\n",
      "Reward : [-7.94466032]\n",
      "--------------------------------------------------\n",
      "Step : 42\n",
      "Current State : [-0.9892303 -0.1463675 -2.8322015]\n",
      "Currrent Action : tensor([[-0.6694]])\n",
      "Next State : [[-0.9892303]\n",
      " [-0.1463675]\n",
      " [-2.8322015]]\n",
      "Reward : [-8.7747916]\n",
      "--------------------------------------------------\n",
      "Step : 43\n",
      "Current State : [-9.9999988e-01  4.7983913e-04 -2.9475012e+00]\n",
      "Currrent Action : tensor([[-0.0368]])\n",
      "Next State : [[-9.9999988e-01]\n",
      " [ 4.7983913e-04]\n",
      " [-2.9475012e+00]]\n",
      "Reward : [-9.77035056]\n",
      "--------------------------------------------------\n",
      "Step : 44\n",
      "Current State : [-0.9906773   0.13622947 -2.7234912 ]\n",
      "Currrent Action : tensor([[1.4910]])\n",
      "Next State : [[-0.9906773 ]\n",
      " [ 0.13622947]\n",
      " [-2.7234912 ]]\n",
      "Reward : [-10.73758918]\n",
      "--------------------------------------------------\n",
      "Step : 45\n",
      "Current State : [-0.963429    0.26796392 -2.692493  ]\n",
      "Currrent Action : tensor([[-0.4745]])\n",
      "Next State : [[-0.963429  ]\n",
      " [ 0.26796392]\n",
      " [-2.692493  ]]\n",
      "Reward : [-9.77161948]\n",
      "--------------------------------------------------\n",
      "Step : 46\n",
      "Current State : [-0.9229298   0.38496828 -2.4778879 ]\n",
      "Currrent Action : tensor([[0.0909]])\n",
      "Next State : [[-0.9229298 ]\n",
      " [ 0.38496828]\n",
      " [-2.4778879 ]]\n",
      "Reward : [-8.96366029]\n",
      "--------------------------------------------------\n",
      "Step : 47\n",
      "Current State : [-0.87354034  0.48675177 -2.2638795 ]\n",
      "Currrent Action : tensor([[-0.4981]])\n",
      "Next State : [[-0.87354034]\n",
      " [ 0.48675177]\n",
      " [-2.2638795 ]]\n",
      "Reward : [-8.15705949]\n",
      "--------------------------------------------------\n",
      "Step : 48\n",
      "Current State : [-0.8235016  0.567314  -1.8974618]\n",
      "Currrent Action : tensor([[0.0090]])\n",
      "Next State : [[-0.8235016]\n",
      " [ 0.567314 ]\n",
      " [-1.8974618]]\n",
      "Reward : [-7.44639027]\n",
      "--------------------------------------------------\n",
      "Step : 49\n",
      "Current State : [-0.7815807   0.62380415 -1.4072009 ]\n",
      "Currrent Action : tensor([[0.4318]])\n",
      "Next State : [[-0.7815807 ]\n",
      " [ 0.62380415]\n",
      " [-1.4072009 ]]\n",
      "Reward : [-6.80345424]\n",
      "--------------------------------------------------\n",
      "Step : 50\n",
      "Current State : [-0.75602835  0.65453887 -0.7994402 ]\n",
      "Currrent Action : tensor([[0.9327]])\n",
      "Next State : [[-0.75602835]\n",
      " [ 0.65453887]\n",
      " [-0.7994402 ]]\n",
      "Reward : [-6.28987645]\n",
      "--------------------------------------------------\n",
      "Step : 51\n",
      "Current State : [-0.74674934  0.6651056  -0.281253  ]\n",
      "Currrent Action : tensor([[0.1819]])\n",
      "Next State : [[-0.74674934]\n",
      " [ 0.6651056 ]\n",
      " [-0.281253  ]]\n",
      "Reward : [-5.95922514]\n",
      "--------------------------------------------------\n",
      "Step : 52\n",
      "Current State : [-0.7538644   0.65703005  0.2152584 ]\n",
      "Currrent Action : tensor([[-0.0155]])\n",
      "Next State : [[-0.7538644 ]\n",
      " [ 0.65703005]\n",
      " [ 0.2152584 ]]\n",
      "Reward : [-5.83510111]\n",
      "--------------------------------------------------\n",
      "Step : 53\n",
      "Current State : [-0.77878076  0.6272962   0.77591604]\n",
      "Currrent Action : tensor([[0.4526]])\n",
      "Next State : [[-0.77878076]\n",
      " [ 0.6272962 ]\n",
      " [ 0.77591604]]\n",
      "Reward : [-5.88410728]\n",
      "--------------------------------------------------\n",
      "Step : 54\n",
      "Current State : [-0.81061316  0.585582    1.0495722 ]\n",
      "Currrent Action : tensor([[-1.3121]])\n",
      "Next State : [[-0.81061316]\n",
      " [ 0.585582  ]\n",
      " [ 1.0495722 ]]\n",
      "Reward : [-6.13083811]\n",
      "--------------------------------------------------\n",
      "Step : 55\n",
      "Current State : [-0.8577888   0.51400226  1.7150751 ]\n",
      "Currrent Action : tensor([[1.5088]])\n",
      "Next State : [[-0.8577888 ]\n",
      " [ 0.51400226]\n",
      " [ 1.7150751 ]]\n",
      "Reward : [-6.44266633]\n",
      "--------------------------------------------------\n",
      "Step : 56\n",
      "Current State : [-0.9095342   0.41562906  2.2241957 ]\n",
      "Currrent Action : tensor([[0.8241]])\n",
      "Next State : [[-0.9095342 ]\n",
      " [ 0.41562906]\n",
      " [ 2.2241957 ]]\n",
      "Reward : [-7.06392294]\n",
      "--------------------------------------------------\n",
      "Step : 57\n",
      "Current State : [-0.95321614  0.30228955  2.4308133 ]\n",
      "Currrent Action : tensor([[-0.7007]])\n",
      "Next State : [[-0.95321614]\n",
      " [ 0.30228955]\n",
      " [ 2.4308133 ]]\n",
      "Reward : [-7.85533843]\n",
      "--------------------------------------------------\n",
      "Step : 58\n",
      "Current State : [-0.9847452   0.17400253  2.6440182 ]\n",
      "Currrent Action : tensor([[-0.0901]])\n",
      "Next State : [[-0.9847452 ]\n",
      " [ 0.17400253]\n",
      " [ 2.6440182 ]]\n",
      "Reward : [-8.62527791]\n",
      "--------------------------------------------------\n",
      "Step : 59\n",
      "Current State : [-0.9997446   0.02260078  3.045801  ]\n",
      "Currrent Action : tensor([[1.8085]])\n",
      "Next State : [[-0.9997446 ]\n",
      " [ 0.02260078]\n",
      " [ 3.045801  ]]\n",
      "Reward : [-9.50366235]\n",
      "--------------------------------------------------\n",
      "Step : 60\n",
      "Current State : [-0.99091405 -0.13449676  3.1501658 ]\n",
      "Currrent Action : tensor([[0.5828]])\n",
      "Next State : [[-0.99091405]\n",
      " [-0.13449676]\n",
      " [ 3.1501658 ]]\n",
      "Reward : [-10.65612825]\n",
      "--------------------------------------------------\n",
      "Step : 61\n",
      "Current State : [-0.9625296  -0.27117664  2.7941937 ]\n",
      "Currrent Action : tensor([[-1.7007]])\n",
      "Next State : [[-0.9625296 ]\n",
      " [-0.27117664]\n",
      " [ 2.7941937 ]]\n",
      "Reward : [-10.0354139]\n",
      "--------------------------------------------------\n",
      "Step : 62\n",
      "Current State : [-0.91343284 -0.4069895   2.8908112 ]\n",
      "Currrent Action : tensor([[2.2550]])\n",
      "Next State : [[-0.91343284]\n",
      " [-0.4069895 ]\n",
      " [ 2.8908112 ]]\n",
      "Reward : [-9.00431117]\n",
      "--------------------------------------------------\n",
      "Step : 63\n",
      "Current State : [-0.85333246 -0.5213672   2.5859299 ]\n",
      "Currrent Action : tensor([[0.0024]])\n",
      "Next State : [[-0.85333246]\n",
      " [-0.5213672 ]\n",
      " [ 2.5859299 ]]\n",
      "Reward : [-8.2473412]\n",
      "--------------------------------------------------\n",
      "Step : 64\n",
      "Current State : [-0.7952571 -0.6062724  2.0582504]\n",
      "Currrent Action : tensor([[-0.9110]])\n",
      "Next State : [[-0.7952571]\n",
      " [-0.6062724]\n",
      " [ 2.0582504]]\n",
      "Reward : [-7.39391001]\n",
      "--------------------------------------------------\n",
      "Step : 65\n",
      "Current State : [-0.7437466 -0.6684617  1.6154734]\n",
      "Currrent Action : tensor([[0.0795]])\n",
      "Next State : [[-0.7437466]\n",
      " [-0.6684617]\n",
      " [ 1.6154734]]\n",
      "Reward : [-6.62488033]\n",
      "--------------------------------------------------\n",
      "Step : 66\n",
      "Current State : [-0.70587224 -0.7083392   1.1000808 ]\n",
      "Currrent Action : tensor([[-0.0936]])\n",
      "Next State : [[-0.70587224]\n",
      " [-0.7083392 ]\n",
      " [ 1.1000808 ]]\n",
      "Reward : [-6.0664534]\n",
      "--------------------------------------------------\n",
      "Step : 67\n",
      "Current State : [-0.6778123  -0.73523504  0.777416  ]\n",
      "Currrent Action : tensor([[1.3906]])\n",
      "Next State : [[-0.6778123 ]\n",
      " [-0.73523504]\n",
      " [ 0.777416  ]]\n",
      "Reward : [-5.66638675]\n",
      "--------------------------------------------------\n",
      "Step : 68\n",
      "Current State : [-0.6799285  -0.73327845 -0.05764231]\n",
      "Currrent Action : tensor([[-1.8909]])\n",
      "Next State : [[-0.6799285 ]\n",
      " [-0.73327845]\n",
      " [-0.05764231]]\n",
      "Reward : [-5.42592042]\n",
      "--------------------------------------------------\n",
      "Step : 69\n",
      "Current State : [-0.69580245 -0.7182332  -0.43742937]\n",
      "Currrent Action : tensor([[1.1345]])\n",
      "Next State : [[-0.69580245]\n",
      " [-0.7182332 ]\n",
      " [-0.43742937]]\n",
      "Reward : [-5.37688258]\n",
      "--------------------------------------------------\n",
      "Step : 70\n",
      "Current State : [-0.7324973 -0.68077   -1.0489304]\n",
      "Currrent Action : tensor([[-0.4855]])\n",
      "Next State : [[-0.7324973]\n",
      " [-0.68077  ]\n",
      " [-1.0489304]]\n",
      "Reward : [-5.49652811]\n",
      "--------------------------------------------------\n",
      "Step : 71\n",
      "Current State : [-0.77447766 -0.63260126 -1.2781183 ]\n",
      "Currrent Action : tensor([[1.8759]])\n",
      "Next State : [[-0.77447766]\n",
      " [-0.63260126]\n",
      " [-1.2781183 ]]\n",
      "Reward : [-5.83893781]\n",
      "--------------------------------------------------\n",
      "Step : 72\n",
      "Current State : [-0.8260425 -0.5636079 -1.723208 ]\n",
      "Currrent Action : tensor([[0.1957]])\n",
      "Next State : [[-0.8260425]\n",
      " [-0.5636079]\n",
      " [-1.723208 ]]\n",
      "Reward : [-6.19869962]\n",
      "--------------------------------------------------\n",
      "Step : 73\n",
      "Current State : [-0.87929595 -0.47627577 -2.0466516 ]\n",
      "Currrent Action : tensor([[0.6617]])\n",
      "Next State : [[-0.87929595]\n",
      " [-0.47627577]\n",
      " [-2.0466516 ]]\n",
      "Reward : [-6.76344673]\n",
      "--------------------------------------------------\n",
      "Step : 74\n",
      "Current State : [-0.92580944 -0.37799054 -2.1757908 ]\n",
      "Currrent Action : tensor([[1.5205]])\n",
      "Next State : [[-0.92580944]\n",
      " [-0.37799054]\n",
      " [-2.1757908 ]]\n",
      "Reward : [-7.41815816]\n",
      "--------------------------------------------------\n",
      "Step : 75\n",
      "Current State : [-0.96373683 -0.26685452 -2.3499427 ]\n",
      "Currrent Action : tensor([[0.7289]])\n",
      "Next State : [[-0.96373683]\n",
      " [-0.26685452]\n",
      " [-2.3499427 ]]\n",
      "Reward : [-8.05827658]\n",
      "--------------------------------------------------\n",
      "Step : 76\n",
      "Current State : [-0.9899893  -0.14114243 -2.5702481 ]\n",
      "Currrent Action : tensor([[-0.1344]])\n",
      "Next State : [[-0.9899893 ]\n",
      " [-0.14114243]\n",
      " [-2.5702481 ]]\n",
      "Reward : [-8.79755205]\n",
      "--------------------------------------------------\n",
      "Step : 77\n",
      "Current State : [-9.9999994e-01  2.6121264e-04 -2.8375304e+00]\n",
      "Currrent Action : tensor([[-1.0762]])\n",
      "Next State : [[-9.9999994e-01]\n",
      " [ 2.6121264e-04]\n",
      " [-2.8375304e+00]]\n",
      "Reward : [-9.66163972]\n",
      "--------------------------------------------------\n",
      "Step : 78\n",
      "Current State : [-0.9902467   0.13932489 -2.790368  ]\n",
      "Currrent Action : tensor([[0.3131]])\n",
      "Next State : [[-0.9902467 ]\n",
      " [ 0.13932489]\n",
      " [-2.790368  ]]\n",
      "Reward : [-10.67321913]\n",
      "--------------------------------------------------\n",
      "Step : 79\n",
      "Current State : [-0.96359533  0.26736507 -2.617558  ]\n",
      "Currrent Action : tensor([[0.4554]])\n",
      "Next State : [[-0.96359533]\n",
      " [ 0.26736507]\n",
      " [-2.617558  ]]\n",
      "Reward : [-9.78970432]\n",
      "--------------------------------------------------\n",
      "Step : 80\n",
      "Current State : [-0.9200622   0.39177227 -2.6379912 ]\n",
      "Currrent Action : tensor([[-1.4730]])\n",
      "Next State : [[-0.9200622 ]\n",
      " [ 0.39177227]\n",
      " [-2.6379912 ]]\n",
      "Reward : [-8.92959942]\n",
      "--------------------------------------------------\n",
      "Step : 81\n",
      "Current State : [-0.8684362  0.4958009 -2.3239956]\n",
      "Currrent Action : tensor([[0.1344]])\n",
      "Next State : [[-0.8684362]\n",
      " [ 0.4958009]\n",
      " [-2.3239956]]\n",
      "Reward : [-8.19823377]\n",
      "--------------------------------------------------\n",
      "Step : 82\n",
      "Current State : [-0.815996   0.5780575 -1.951787 ]\n",
      "Currrent Action : tensor([[0.0024]])\n",
      "Next State : [[-0.815996 ]\n",
      " [ 0.5780575]\n",
      " [-1.951787 ]]\n",
      "Reward : [-7.41936319]\n",
      "--------------------------------------------------\n",
      "Step : 83\n",
      "Current State : [-0.7792937   0.62665886 -1.2182438 ]\n",
      "Currrent Action : tensor([[2.3867]])\n",
      "Next State : [[-0.7792937 ]\n",
      " [ 0.62665886]\n",
      " [-1.2182438 ]]\n",
      "Reward : [-6.76181691]\n",
      "--------------------------------------------------\n",
      "Step : 84\n",
      "Current State : [-0.76505417  0.6439659  -0.44824964]\n",
      "Currrent Action : tensor([[2.1123]])\n",
      "Next State : [[-0.76505417]\n",
      " [ 0.6439659 ]\n",
      " [-0.44824964]]\n",
      "Reward : [-6.22535519]\n",
      "--------------------------------------------------\n",
      "Step : 85\n",
      "Current State : [-0.7712401   0.63654435  0.19323118]\n",
      "Currrent Action : tensor([[1.0567]])\n",
      "Next State : [[-0.7712401 ]\n",
      " [ 0.63654435]\n",
      " [ 0.19323118]]\n",
      "Reward : [-5.98419143]\n",
      "--------------------------------------------------\n",
      "Step : 86\n",
      "Current State : [-0.78751385  0.61629695  0.51955086]\n",
      "Currrent Action : tensor([[-1.0073]])\n",
      "Next State : [[-0.78751385]\n",
      " [ 0.61629695]\n",
      " [ 0.51955086]]\n",
      "Reward : [-6.0150093]\n",
      "--------------------------------------------------\n",
      "Step : 87\n",
      "Current State : [-0.8220047  0.5694807  1.1631558]\n",
      "Currrent Action : tensor([[1.2092]])\n",
      "Next State : [[-0.8220047]\n",
      " [ 0.5694807]\n",
      " [ 1.1631558]]\n",
      "Reward : [-6.16676348]\n",
      "--------------------------------------------------\n",
      "Step : 88\n",
      "Current State : [-0.86076057  0.5090101   1.4367923 ]\n",
      "Currrent Action : tensor([[-1.0232]])\n",
      "Next State : [[-0.86076057]\n",
      " [ 0.5090101 ]\n",
      " [ 1.4367923 ]]\n",
      "Reward : [-6.56620921]\n",
      "--------------------------------------------------\n",
      "Step : 89\n",
      "Current State : [-0.90106833  0.4336772   1.7092927 ]\n",
      "Currrent Action : tensor([[-0.7284]])\n",
      "Next State : [[-0.90106833]\n",
      " [ 0.4336772 ]\n",
      " [ 1.7092927 ]]\n",
      "Reward : [-7.00632801]\n",
      "--------------------------------------------------\n",
      "Step : 90\n",
      "Current State : [-0.93758714  0.34775034  1.8679821 ]\n",
      "Currrent Action : tensor([[-1.1105]])\n",
      "Next State : [[-0.93758714]\n",
      " [ 0.34775034]\n",
      " [ 1.8679821 ]]\n",
      "Reward : [-7.54577375]\n",
      "--------------------------------------------------\n",
      "Step : 91\n",
      "Current State : [-0.9673213   0.25355384  1.9763647 ]\n",
      "Currrent Action : tensor([[-1.0162]])\n",
      "Next State : [[-0.9673213 ]\n",
      " [ 0.25355384]\n",
      " [ 1.9763647 ]]\n",
      "Reward : [-8.11411616]\n",
      "--------------------------------------------------\n",
      "Step : 92\n",
      "Current State : [-0.9911651  0.1326339  2.46653  ]\n",
      "Currrent Action : tensor([[2.0351]])\n",
      "Next State : [[-0.9911651]\n",
      " [ 0.1326339]\n",
      " [ 2.46653  ]]\n",
      "Reward : [-8.71921319]\n",
      "--------------------------------------------------\n",
      "Step : 93\n",
      "Current State : [-9.9999791e-01  2.0302543e-03  2.6199124e+00]\n",
      "Currrent Action : tensor([[0.3594]])\n",
      "Next State : [[-9.9999791e-01]\n",
      " [ 2.0302543e-03]\n",
      " [ 2.6199124e+00]]\n",
      "Reward : [-9.65998028]\n",
      "--------------------------------------------------\n",
      "Step : 94\n",
      "Current State : [-0.99140716 -0.13081247  2.6643739 ]\n",
      "Currrent Action : tensor([[0.2863]])\n",
      "Next State : [[-0.99140716]\n",
      " [-0.13081247]\n",
      " [ 2.6643739 ]]\n",
      "Reward : [-10.54332809]\n",
      "--------------------------------------------------\n",
      "Step : 95\n",
      "Current State : [-0.9638864  -0.26631388  2.7675667 ]\n",
      "Currrent Action : tensor([[1.3420]])\n",
      "Next State : [[-0.9638864 ]\n",
      " [-0.26631388]\n",
      " [ 2.7675667 ]]\n",
      "Reward : [-9.77422336]\n",
      "--------------------------------------------------\n",
      "Step : 96\n",
      "Current State : [-0.926351   -0.37666142  2.332457  ]\n",
      "Currrent Action : tensor([[-1.5692]])\n",
      "Next State : [[-0.926351  ]\n",
      " [-0.37666142]\n",
      " [ 2.332457  ]]\n",
      "Reward : [-9.01693746]\n",
      "--------------------------------------------------\n",
      "Step : 97\n",
      "Current State : [-0.87949145 -0.47591475  2.196284  ]\n",
      "Currrent Action : tensor([[0.9755]])\n",
      "Next State : [[-0.87949145]\n",
      " [-0.47591475]\n",
      " [ 2.196284  ]]\n",
      "Reward : [-8.13723304]\n",
      "--------------------------------------------------\n",
      "Step : 98\n",
      "Current State : [-0.83079696 -0.5565756   1.8850889 ]\n",
      "Currrent Action : tensor([[0.3049]])\n",
      "Next State : [[-0.83079696]\n",
      " [-0.5565756 ]\n",
      " [ 1.8850889 ]]\n",
      "Reward : [-7.48159956]\n",
      "--------------------------------------------------\n",
      "Step : 99\n",
      "Current State : [-0.7888668 -0.6145642  1.431502 ]\n",
      "Currrent Action : tensor([[-0.2410]])\n",
      "Next State : [[-0.7888668]\n",
      " [-0.6145642]\n",
      " [ 1.431502 ]]\n",
      "Reward : [-6.86472126]\n",
      "learning iteration : 2\n",
      "--------------------------------------------------\n",
      "Step : 0\n",
      "Current State : [ 0.989902   -0.14175317 -0.40259656]\n",
      "Currrent Action : tensor([[0.0972]])\n",
      "Next State : [[ 0.989902  ]\n",
      " [-0.14175317]\n",
      " [-0.40259656]]\n",
      "Reward : [-0.02553491]\n",
      "--------------------------------------------------\n",
      "Step : 1\n",
      "Current State : [ 0.98619103 -0.16561179 -0.48292196]\n",
      "Currrent Action : tensor([[0.1733]])\n",
      "Next State : [[ 0.98619103]\n",
      " [-0.16561179]\n",
      " [-0.48292196]]\n",
      "Reward : [-0.03646843]\n",
      "--------------------------------------------------\n",
      "Step : 2\n",
      "Current State : [ 0.9835316  -0.18073615 -0.30713078]\n",
      "Currrent Action : tensor([[2.0532]])\n",
      "Next State : [[ 0.9835316 ]\n",
      " [-0.18073615]\n",
      " [-0.30713078]]\n",
      "Reward : [-0.05500311]\n",
      "--------------------------------------------------\n",
      "Step : 3\n",
      "Current State : [ 0.97633564 -0.21626085 -0.7249635 ]\n",
      "Currrent Action : tensor([[-1.8819]])\n",
      "Next State : [[ 0.97633564]\n",
      " [-0.21626085]\n",
      " [-0.7249635 ]]\n",
      "Reward : [-0.04600193]\n",
      "--------------------------------------------------\n",
      "Step : 4\n",
      "Current State : [ 0.9640416  -0.26575133 -1.0200027 ]\n",
      "Currrent Action : tensor([[-0.8856]])\n",
      "Next State : [[ 0.9640416 ]\n",
      " [-0.26575133]\n",
      " [-1.0200027 ]]\n",
      "Reward : [-0.10085815]\n",
      "--------------------------------------------------\n",
      "Step : 5\n",
      "Current State : [ 0.9433535 -0.3317895 -1.3843338]\n",
      "Currrent Action : tensor([[-1.1001]])\n",
      "Next State : [[ 0.9433535]\n",
      " [-0.3317895]\n",
      " [-1.3843338]]\n",
      "Reward : [-0.17760277]\n",
      "--------------------------------------------------\n",
      "Step : 6\n",
      "Current State : [ 0.91586375 -0.40148917 -1.4988474 ]\n",
      "Currrent Action : tensor([[0.8955]])\n",
      "Next State : [[ 0.91586375]\n",
      " [-0.40148917]\n",
      " [-1.4988474 ]]\n",
      "Reward : [-0.30681916]\n",
      "--------------------------------------------------\n",
      "Step : 7\n",
      "Current State : [ 0.8694952 -0.4939414 -2.0694935]\n",
      "Currrent Action : tensor([[-1.7969]])\n",
      "Next State : [[ 0.8694952]\n",
      " [-0.4939414]\n",
      " [-2.0694935]]\n",
      "Reward : [-0.39856959]\n",
      "--------------------------------------------------\n",
      "Step : 8\n",
      "Current State : [ 0.8067501 -0.5908928 -2.3109622]\n",
      "Currrent Action : tensor([[0.8599]])\n",
      "Next State : [[ 0.8067501]\n",
      " [-0.5908928]\n",
      " [-2.3109622]]\n",
      "Reward : [-0.69591286]\n",
      "--------------------------------------------------\n",
      "Step : 9\n",
      "Current State : [ 0.72300094 -0.69084704 -2.6098979 ]\n",
      "Currrent Action : tensor([[0.9616]])\n",
      "Next State : [[ 0.72300094]\n",
      " [-0.69084704]\n",
      " [-2.6098979 ]]\n",
      "Reward : [-0.93461186]\n",
      "--------------------------------------------------\n",
      "Step : 10\n",
      "Current State : [ 0.6133442 -0.7898157 -2.956972 ]\n",
      "Currrent Action : tensor([[1.1404]])\n",
      "Next State : [[ 0.6133442]\n",
      " [-0.7898157]\n",
      " [-2.956972 ]]\n",
      "Reward : [-1.26410743]\n",
      "--------------------------------------------------\n",
      "Step : 11\n",
      "Current State : [ 0.46494874 -0.8853376  -3.534222  ]\n",
      "Currrent Action : tensor([[0.1007]])\n",
      "Next State : [[ 0.46494874]\n",
      " [-0.8853376 ]\n",
      " [-3.534222  ]]\n",
      "Reward : [-1.70340421]\n",
      "--------------------------------------------------\n",
      "Step : 12\n",
      "Current State : [ 0.26311973 -0.96476316 -4.346445  ]\n",
      "Currrent Action : tensor([[-0.9881]])\n",
      "Next State : [[ 0.26311973]\n",
      " [-0.96476316]\n",
      " [-4.346445  ]]\n",
      "Reward : [-2.43209539]\n",
      "--------------------------------------------------\n",
      "Step : 13\n",
      "Current State : [ 0.02328353 -0.9997289  -4.8593764 ]\n",
      "Currrent Action : tensor([[1.4043]])\n",
      "Next State : [[ 0.02328353]\n",
      " [-0.9997289 ]\n",
      " [-4.8593764 ]]\n",
      "Reward : [-3.59295979]\n",
      "--------------------------------------------------\n",
      "Step : 14\n",
      "Current State : [-0.25027436 -0.96817493 -5.5249853 ]\n",
      "Currrent Action : tensor([[0.5613]])\n",
      "Next State : [[-0.25027436]\n",
      " [-0.96817493]\n",
      " [-5.5249853 ]]\n",
      "Reward : [-4.75645824]\n",
      "--------------------------------------------------\n",
      "Step : 15\n",
      "Current State : [-0.5412844 -0.8408396 -6.3800044]\n",
      "Currrent Action : tensor([[-0.8593]])\n",
      "Next State : [[-0.5412844]\n",
      " [-0.8408396]\n",
      " [-6.3800044]]\n",
      "Reward : [-6.379385]\n",
      "--------------------------------------------------\n",
      "Step : 16\n",
      "Current State : [-0.8032636  -0.59562373 -7.2158113 ]\n",
      "Currrent Action : tensor([[-1.3678]])\n",
      "Next State : [[-0.8032636 ]\n",
      " [-0.59562373]\n",
      " [-7.2158113 ]]\n",
      "Reward : [-8.66373748]\n",
      "--------------------------------------------------\n",
      "Step : 17\n",
      "Current State : [-0.96521723 -0.2614492  -7.470372  ]\n",
      "Currrent Action : tensor([[1.2810]])\n",
      "Next State : [[-0.96521723]\n",
      " [-0.2614492 ]\n",
      " [-7.470372  ]]\n",
      "Reward : [-11.47620067]\n",
      "--------------------------------------------------\n",
      "Step : 18\n",
      "Current State : [-0.99414825  0.10802414 -7.455173  ]\n",
      "Currrent Action : tensor([[1.4086]])\n",
      "Next State : [[-0.99414825]\n",
      " [ 0.10802414]\n",
      " [-7.455173  ]]\n",
      "Reward : [-13.86015828]\n",
      "--------------------------------------------------\n",
      "Step : 19\n",
      "Current State : [-0.89449185  0.44708425 -7.105353  ]\n",
      "Currrent Action : tensor([[1.7920]])\n",
      "Next State : [[-0.89449185]\n",
      " [ 0.44708425]\n",
      " [-7.105353  ]]\n",
      "Reward : [-14.76242831]\n",
      "--------------------------------------------------\n",
      "Step : 20\n",
      "Current State : [-0.6942766  0.7197083 -6.797582 ]\n",
      "Currrent Action : tensor([[-0.1836]])\n",
      "Next State : [[-0.6942766]\n",
      " [ 0.7197083]\n",
      " [-6.797582 ]]\n",
      "Reward : [-12.22080185]\n",
      "--------------------------------------------------\n",
      "Step : 21\n",
      "Current State : [-0.45246202  0.89178365 -5.957801  ]\n",
      "Currrent Action : tensor([[3.0107]])\n",
      "Next State : [[-0.45246202]\n",
      " [ 0.89178365]\n",
      " [-5.957801  ]]\n",
      "Reward : [-10.09194108]\n",
      "--------------------------------------------------\n",
      "Step : 22\n",
      "Current State : [-0.21073312  0.97754365 -5.143986  ]\n",
      "Currrent Action : tensor([[0.9665]])\n",
      "Next State : [[-0.21073312]\n",
      " [ 0.97754365]\n",
      " [-5.143986  ]]\n",
      "Reward : [-7.71338123]\n",
      "--------------------------------------------------\n",
      "Step : 23\n",
      "Current State : [-0.00678338  0.999977   -4.1108284 ]\n",
      "Currrent Action : tensor([[2.1322]])\n",
      "Next State : [[-0.00678338]\n",
      " [ 0.999977  ]\n",
      " [-4.1108284 ]]\n",
      "Reward : [-5.82958073]\n",
      "--------------------------------------------------\n",
      "Step : 24\n",
      "Current State : [ 0.15524226  0.9878764  -3.253122  ]\n",
      "Currrent Action : tensor([[0.7182]])\n",
      "Next State : [[ 0.15524226]\n",
      " [ 0.9878764 ]\n",
      " [-3.253122  ]]\n",
      "Reward : [-4.17916484]\n",
      "--------------------------------------------------\n",
      "Step : 25\n",
      "Current State : [ 0.27213514  0.96225905 -2.394771  ]\n",
      "Currrent Action : tensor([[0.7830]])\n",
      "Next State : [[ 0.27213514]\n",
      " [ 0.96225905]\n",
      " [-2.394771  ]]\n",
      "Reward : [-3.06090236]\n",
      "--------------------------------------------------\n",
      "Step : 26\n",
      "Current State : [ 0.35312542  0.93557596 -1.7059686 ]\n",
      "Currrent Action : tensor([[-0.2193]])\n",
      "Next State : [[ 0.35312542]\n",
      " [ 0.93557596]\n",
      " [-1.7059686 ]]\n",
      "Reward : [-2.25104534]\n",
      "--------------------------------------------------\n",
      "Step : 27\n",
      "Current State : [ 0.4070885  0.9133887 -1.1670908]\n",
      "Currrent Action : tensor([[-1.0854]])\n",
      "Next State : [[ 0.4070885]\n",
      " [ 0.9133887]\n",
      " [-1.1670908]]\n",
      "Reward : [-1.75603663]\n",
      "--------------------------------------------------\n",
      "Step : 28\n",
      "Current State : [ 0.42217016  0.9065166  -0.33147508]\n",
      "Currrent Action : tensor([[1.0038]])\n",
      "Next State : [[ 0.42217016]\n",
      " [ 0.9065166 ]\n",
      " [-0.33147508]]\n",
      "Reward : [-1.46324399]\n",
      "--------------------------------------------------\n",
      "Step : 29\n",
      "Current State : [0.4030976 0.915157  0.4187768]\n",
      "Currrent Action : tensor([[0.4691]])\n",
      "Next State : [[0.4030976]\n",
      " [0.915157 ]\n",
      " [0.4187768]]\n",
      "Reward : [-1.29933813]\n",
      "--------------------------------------------------\n",
      "Step : 30\n",
      "Current State : [0.3522569 0.9359034 1.0983526]\n",
      "Currrent Action : tensor([[-0.0453]])\n",
      "Next State : [[0.3522569]\n",
      " [0.9359034]\n",
      " [1.0983526]]\n",
      "Reward : [-1.35363782]\n",
      "--------------------------------------------------\n",
      "Step : 31\n",
      "Current State : [0.26477176 0.9643111  1.8402851 ]\n",
      "Currrent Action : tensor([[0.2667]])\n",
      "Next State : [[0.26477176]\n",
      " [0.9643111 ]\n",
      " [1.8402851 ]]\n",
      "Reward : [-1.58678155]\n",
      "--------------------------------------------------\n",
      "Step : 32\n",
      "Current State : [0.14260876 0.9897791  2.4974122 ]\n",
      "Currrent Action : tensor([[-0.4407]])\n",
      "Next State : [[0.14260876]\n",
      " [0.9897791 ]\n",
      " [2.4974122 ]]\n",
      "Reward : [-2.03622278]\n",
      "--------------------------------------------------\n",
      "Step : 33\n",
      "Current State : [-0.0108894  0.9999407  3.0797246]\n",
      "Currrent Action : tensor([[-1.0668]])\n",
      "Next State : [[-0.0108894]\n",
      " [ 0.9999407]\n",
      " [ 3.0797246]]\n",
      "Reward : [-2.6631713]\n",
      "--------------------------------------------------\n",
      "Step : 34\n",
      "Current State : [-0.18627913  0.98249686  3.5296803 ]\n",
      "Currrent Action : tensor([[-2.0272]])\n",
      "Next State : [[-0.18627913]\n",
      " [ 0.98249686]\n",
      " [ 3.5296803 ]]\n",
      "Reward : [-3.4542008]\n",
      "--------------------------------------------------\n",
      "Step : 35\n",
      "Current State : [-0.38542145  0.92274064  4.165818  ]\n",
      "Currrent Action : tensor([[-0.6716]])\n",
      "Next State : [[-0.38542145]\n",
      " [ 0.92274064]\n",
      " [ 4.165818  ]]\n",
      "Reward : [-4.33747678]\n",
      "--------------------------------------------------\n",
      "Step : 36\n",
      "Current State : [-0.5998382  0.8001213  4.952687 ]\n",
      "Currrent Action : tensor([[0.6321]])\n",
      "Next State : [[-0.5998382]\n",
      " [ 0.8001213]\n",
      " [ 4.952687 ]]\n",
      "Reward : [-5.60277203]\n",
      "--------------------------------------------------\n",
      "Step : 37\n",
      "Current State : [-0.7950787  0.6065063  5.516769 ]\n",
      "Currrent Action : tensor([[-0.2401]])\n",
      "Next State : [[-0.7950787]\n",
      " [ 0.6065063]\n",
      " [ 5.516769 ]]\n",
      "Reward : [-7.35518594]\n",
      "--------------------------------------------------\n",
      "Step : 38\n",
      "Current State : [-0.93837225  0.34562632  5.975057  ]\n",
      "Currrent Action : tensor([[0.0227]])\n",
      "Next State : [[-0.93837225]\n",
      " [ 0.34562632]\n",
      " [ 5.975057  ]]\n",
      "Reward : [-9.24324412]\n",
      "--------------------------------------------------\n",
      "Step : 39\n",
      "Current State : [-0.99942493  0.03390872  6.3798184 ]\n",
      "Currrent Action : tensor([[0.9703]])\n",
      "Next State : [[-0.99942493]\n",
      " [ 0.03390872]\n",
      " [ 6.3798184 ]]\n",
      "Reward : [-11.34784447]\n",
      "--------------------------------------------------\n",
      "Step : 40\n",
      "Current State : [-0.9634107  -0.26802966  6.1052504 ]\n",
      "Currrent Action : tensor([[-2.0220]])\n",
      "Next State : [[-0.9634107 ]\n",
      " [-0.26802966]\n",
      " [ 6.1052504 ]]\n",
      "Reward : [-13.73186763]\n",
      "--------------------------------------------------\n",
      "Step : 41\n",
      "Current State : [-0.8437838  -0.53668326  5.903081  ]\n",
      "Currrent Action : tensor([[-0.0076]])\n",
      "Next State : [[-0.8437838 ]\n",
      " [-0.53668326]\n",
      " [ 5.903081  ]]\n",
      "Reward : [-11.96571647]\n",
      "--------------------------------------------------\n",
      "Step : 42\n",
      "Current State : [-0.6668157 -0.7452226  5.4873466]\n",
      "Currrent Action : tensor([[-0.0881]])\n",
      "Next State : [[-0.6668157]\n",
      " [-0.7452226]\n",
      " [ 5.4873466]]\n",
      "Reward : [-10.11573967]\n",
      "--------------------------------------------------\n",
      "Step : 43\n",
      "Current State : [-0.455905  -0.8900285  5.1307783]\n",
      "Currrent Action : tensor([[1.3490]])\n",
      "Next State : [[-0.455905 ]\n",
      " [-0.8900285]\n",
      " [ 5.1307783]]\n",
      "Reward : [-8.30624772]\n",
      "--------------------------------------------------\n",
      "Step : 44\n",
      "Current State : [-0.25765023 -0.96623826  4.2559843 ]\n",
      "Currrent Action : tensor([[-1.3818]])\n",
      "Next State : [[-0.25765023]\n",
      " [-0.96623826]\n",
      " [ 4.2559843 ]]\n",
      "Reward : [-6.81309054]\n",
      "--------------------------------------------------\n",
      "Step : 45\n",
      "Current State : [-0.0924494 -0.9957174  3.3601584]\n",
      "Currrent Action : tensor([[-1.1410]])\n",
      "Next State : [[-0.0924494]\n",
      " [-0.9957174]\n",
      " [ 3.3601584]]\n",
      "Reward : [-5.16661628]\n",
      "--------------------------------------------------\n",
      "Step : 46\n",
      "Current State : [ 0.05306198 -0.99859124  2.9133704 ]\n",
      "Currrent Action : tensor([[2.5503]])\n",
      "Next State : [[ 0.05306198]\n",
      " [-0.99859124]\n",
      " [ 2.9133704 ]]\n",
      "Reward : [-3.89989263]\n",
      "--------------------------------------------------\n",
      "Step : 47\n",
      "Current State : [ 0.16677211 -0.9859955   2.2893622 ]\n",
      "Currrent Action : tensor([[0.8329]])\n",
      "Next State : [[ 0.16677211]\n",
      " [-0.9859955 ]\n",
      " [ 2.2893622 ]]\n",
      "Reward : [-3.15290831]\n",
      "--------------------------------------------------\n",
      "Step : 48\n",
      "Current State : [ 0.25436538 -0.9671082   1.7927288 ]\n",
      "Currrent Action : tensor([[1.6191]])\n",
      "Next State : [[ 0.25436538]\n",
      " [-0.9671082 ]\n",
      " [ 1.7927288 ]]\n",
      "Reward : [-2.49582552]\n",
      "--------------------------------------------------\n",
      "Step : 49\n",
      "Current State : [ 0.2912769 -0.9566388  0.7673976]\n",
      "Currrent Action : tensor([[-2.5435]])\n",
      "Next State : [[ 0.2912769]\n",
      " [-0.9566388]\n",
      " [ 0.7673976]]\n",
      "Reward : [-2.05094539]\n",
      "--------------------------------------------------\n",
      "Step : 50\n",
      "Current State : [ 0.3054262  -0.95221573  0.29649314]\n",
      "Currrent Action : tensor([[1.6438]])\n",
      "Next State : [[ 0.3054262 ]\n",
      " [-0.95221573]\n",
      " [ 0.29649314]]\n",
      "Reward : [-1.68781637]\n",
      "--------------------------------------------------\n",
      "Step : 51\n",
      "Current State : [ 0.28644043 -0.95809805 -0.39752954]\n",
      "Currrent Action : tensor([[0.1343]])\n",
      "Next State : [[ 0.28644043]\n",
      " [-0.95809805]\n",
      " [-0.39752954]]\n",
      "Reward : [-1.59744305]\n",
      "--------------------------------------------------\n",
      "Step : 52\n",
      "Current State : [ 0.22589639 -0.9741513  -1.2529283 ]\n",
      "Currrent Action : tensor([[-0.9122]])\n",
      "Next State : [[ 0.22589639]\n",
      " [-0.9741513 ]\n",
      " [-1.2529283 ]]\n",
      "Reward : [-1.65576934]\n",
      "--------------------------------------------------\n",
      "Step : 53\n",
      "Current State : [ 0.13988486 -0.9901678  -1.7503594 ]\n",
      "Currrent Action : tensor([[1.5545]])\n",
      "Next State : [[ 0.13988486]\n",
      " [-0.9901678 ]\n",
      " [-1.7503594 ]]\n",
      "Reward : [-1.96286917]\n",
      "--------------------------------------------------\n",
      "Step : 54\n",
      "Current State : [ 0.03069105 -0.99952894 -2.1929853 ]\n",
      "Currrent Action : tensor([[2.1676]])\n",
      "Next State : [[ 0.03069105]\n",
      " [-0.99952894]\n",
      " [-2.1929853 ]]\n",
      "Reward : [-2.35656641]\n",
      "--------------------------------------------------\n",
      "Step : 55\n",
      "Current State : [-0.10848888 -0.99409765 -2.787974  ]\n",
      "Currrent Action : tensor([[1.0311]])\n",
      "Next State : [[-0.10848888]\n",
      " [-0.99409765]\n",
      " [-2.787974  ]]\n",
      "Reward : [-2.85389089]\n",
      "--------------------------------------------------\n",
      "Step : 56\n",
      "Current State : [-0.26709786 -0.9636694  -3.2335474 ]\n",
      "Currrent Action : tensor([[2.4288]])\n",
      "Next State : [[-0.26709786]\n",
      " [-0.9636694 ]\n",
      " [-3.2335474 ]]\n",
      "Reward : [-3.60199744]\n",
      "--------------------------------------------------\n",
      "Step : 57\n",
      "Current State : [-0.45155805 -0.8922417  -3.9626117 ]\n",
      "Currrent Action : tensor([[-0.0421]])\n",
      "Next State : [[-0.45155805]\n",
      " [-0.8922417 ]\n",
      " [-3.9626117 ]]\n",
      "Reward : [-4.4355157]\n",
      "--------------------------------------------------\n",
      "Step : 58\n",
      "Current State : [-0.6432153 -0.7656854 -4.6035876]\n",
      "Currrent Action : tensor([[0.1880]])\n",
      "Next State : [[-0.6432153]\n",
      " [-0.7656854]\n",
      " [-4.6035876]]\n",
      "Reward : [-5.72903798]\n",
      "--------------------------------------------------\n",
      "Step : 59\n",
      "Current State : [-0.8104695  -0.58578086 -4.92526   ]\n",
      "Currrent Action : tensor([[1.6839]])\n",
      "Next State : [[-0.8104695 ]\n",
      " [-0.58578086]\n",
      " [-4.92526   ]]\n",
      "Reward : [-7.27270634]\n",
      "--------------------------------------------------\n",
      "Step : 60\n",
      "Current State : [-0.937987   -0.34667036 -5.4364834 ]\n",
      "Currrent Action : tensor([[-0.4792]])\n",
      "Next State : [[-0.937987  ]\n",
      " [-0.34667036]\n",
      " [-5.4364834 ]]\n",
      "Reward : [-8.75504396]\n",
      "--------------------------------------------------\n",
      "Step : 61\n",
      "Current State : [-0.9964577  -0.08409526 -5.396486  ]\n",
      "Currrent Action : tensor([[2.2130]])\n",
      "Next State : [[-0.9964577 ]\n",
      " [-0.08409526]\n",
      " [-5.396486  ]]\n",
      "Reward : [-10.73010185]\n",
      "--------------------------------------------------\n",
      "Step : 62\n",
      "Current State : [-0.98364997  0.18009105 -5.3054743 ]\n",
      "Currrent Action : tensor([[1.0272]])\n",
      "Next State : [[-0.98364997]\n",
      " [ 0.18009105]\n",
      " [-5.3054743 ]]\n",
      "Reward : [-12.26094352]\n",
      "--------------------------------------------------\n",
      "Step : 63\n",
      "Current State : [-0.90490365  0.42561644 -5.17128   ]\n",
      "Currrent Action : tensor([[-0.0058]])\n",
      "Next State : [[-0.90490365]\n",
      " [ 0.42561644]\n",
      " [-5.17128   ]]\n",
      "Reward : [-11.57944672]\n",
      "--------------------------------------------------\n",
      "Step : 64\n",
      "Current State : [-0.77072126  0.6371725  -5.02362   ]\n",
      "Currrent Action : tensor([[-1.1437]])\n",
      "Next State : [[-0.77072126]\n",
      " [ 0.6371725 ]\n",
      " [-5.02362   ]]\n",
      "Reward : [-9.97605361]\n",
      "--------------------------------------------------\n",
      "Step : 65\n",
      "Current State : [-0.60911214  0.7930841  -4.5006337 ]\n",
      "Currrent Action : tensor([[0.3007]])\n",
      "Next State : [[-0.60911214]\n",
      " [ 0.7930841 ]\n",
      " [-4.5006337 ]]\n",
      "Reward : [-8.53003334]\n",
      "--------------------------------------------------\n",
      "Step : 66\n",
      "Current State : [-0.44055453  0.8977259  -3.9744866 ]\n",
      "Currrent Action : tensor([[-0.4578]])\n",
      "Next State : [[-0.44055453]\n",
      " [ 0.8977259 ]\n",
      " [-3.9744866 ]]\n",
      "Reward : [-6.97968489]\n",
      "--------------------------------------------------\n",
      "Step : 67\n",
      "Current State : [-0.29053697  0.95686376 -3.2285666 ]\n",
      "Currrent Action : tensor([[0.4842]])\n",
      "Next State : [[-0.29053697]\n",
      " [ 0.95686376]\n",
      " [-3.2285666 ]]\n",
      "Reward : [-5.6886689]\n",
      "--------------------------------------------------\n",
      "Step : 68\n",
      "Current State : [-0.18056424  0.9835632  -2.2645571 ]\n",
      "Currrent Action : tensor([[1.6424]])\n",
      "Next State : [[-0.18056424]\n",
      " [ 0.9835632 ]\n",
      " [-2.2645571 ]]\n",
      "Reward : [-4.52546643]\n",
      "--------------------------------------------------\n",
      "Step : 69\n",
      "Current State : [-0.10670187  0.99429107 -1.493094  ]\n",
      "Currrent Action : tensor([[0.2253]])\n",
      "Next State : [[-0.10670187]\n",
      " [ 0.99429107]\n",
      " [-1.493094  ]]\n",
      "Reward : [-3.5836257]\n",
      "--------------------------------------------------\n",
      "Step : 70\n",
      "Current State : [-0.08443595  0.9964289  -0.4473757 ]\n",
      "Currrent Action : tensor([[2.2638]])\n",
      "Next State : [[-0.08443595]\n",
      " [ 0.9964289 ]\n",
      " [-0.4473757 ]]\n",
      "Reward : [-3.04161602]\n",
      "--------------------------------------------------\n",
      "Step : 71\n",
      "Current State : [-0.0924569   0.9957167   0.16105057]\n",
      "Currrent Action : tensor([[-0.9260]])\n",
      "Next State : [[-0.0924569 ]\n",
      " [ 0.9957167 ]\n",
      " [ 0.16105057]]\n",
      "Reward : [-2.76099903]\n",
      "--------------------------------------------------\n",
      "Step : 72\n",
      "Current State : [-0.1379565  0.9904383  0.9161752]\n",
      "Currrent Action : tensor([[0.0556]])\n",
      "Next State : [[-0.1379565]\n",
      " [ 0.9904383]\n",
      " [ 0.9161752]]\n",
      "Reward : [-2.769448]\n",
      "--------------------------------------------------\n",
      "Step : 73\n",
      "Current State : [-0.23337938  0.97238576  1.943074  ]\n",
      "Currrent Action : tensor([[1.8938]])\n",
      "Next State : [[-0.23337938]\n",
      " [ 0.97238576]\n",
      " [ 1.943074  ]]\n",
      "Reward : [-3.00886906]\n",
      "--------------------------------------------------\n",
      "Step : 74\n",
      "Current State : [-0.36154166  0.93235594  2.6873853 ]\n",
      "Currrent Action : tensor([[0.1001]])\n",
      "Next State : [[-0.36154166]\n",
      " [ 0.93235594]\n",
      " [ 2.6873853 ]]\n",
      "Reward : [-3.64045648]\n",
      "--------------------------------------------------\n",
      "Step : 75\n",
      "Current State : [-0.5125955   0.85863024  3.365681  ]\n",
      "Currrent Action : tensor([[-0.1398]])\n",
      "Next State : [[-0.5125955 ]\n",
      " [ 0.85863024]\n",
      " [ 3.365681  ]]\n",
      "Reward : [-4.48860675]\n",
      "--------------------------------------------------\n",
      "Step : 76\n",
      "Current State : [-0.6831063   0.73031896  4.276054  ]\n",
      "Currrent Action : tensor([[1.7760]])\n",
      "Next State : [[-0.6831063 ]\n",
      " [ 0.73031896]\n",
      " [ 4.276054  ]]\n",
      "Reward : [-5.58382128]\n",
      "--------------------------------------------------\n",
      "Step : 77\n",
      "Current State : [-0.8339309   0.55186886  4.683703  ]\n",
      "Currrent Action : tensor([[-0.9339]])\n",
      "Next State : [[-0.8339309 ]\n",
      " [ 0.55186886]\n",
      " [ 4.683703  ]]\n",
      "Reward : [-7.22475392]\n",
      "--------------------------------------------------\n",
      "Step : 78\n",
      "Current State : [-0.9474014   0.32004774  5.176474  ]\n",
      "Currrent Action : tensor([[0.5258]])\n",
      "Next State : [[-0.9474014 ]\n",
      " [ 0.32004774]\n",
      " [ 5.176474  ]]\n",
      "Reward : [-8.73217664]\n",
      "--------------------------------------------------\n",
      "Step : 79\n",
      "Current State : [-0.99868196  0.05132553  5.488636  ]\n",
      "Currrent Action : tensor([[0.4808]])\n",
      "Next State : [[-0.99868196]\n",
      " [ 0.05132553]\n",
      " [ 5.488636  ]]\n",
      "Reward : [-10.60862131]\n",
      "--------------------------------------------------\n",
      "Step : 80\n",
      "Current State : [-0.97309136 -0.23041952  5.6771374 ]\n",
      "Currrent Action : tensor([[1.0000]])\n",
      "Next State : [[-0.97309136]\n",
      " [-0.23041952]\n",
      " [ 5.6771374 ]]\n",
      "Reward : [-12.56312384]\n",
      "--------------------------------------------------\n",
      "Step : 81\n",
      "Current State : [-0.8764379 -0.4815149  5.397472 ]\n",
      "Currrent Action : tensor([[-0.7123]])\n",
      "Next State : [[-0.8764379]\n",
      " [-0.4815149]\n",
      " [ 5.397472 ]]\n",
      "Reward : [-11.68626526]\n",
      "--------------------------------------------------\n",
      "Step : 82\n",
      "Current State : [-0.7287079  -0.68482465  5.0396132 ]\n",
      "Currrent Action : tensor([[0.0218]])\n",
      "Next State : [[-0.7287079 ]\n",
      " [-0.68482465]\n",
      " [ 5.0396132 ]]\n",
      "Reward : [-9.87870186]\n",
      "--------------------------------------------------\n",
      "Step : 83\n",
      "Current State : [-0.5489146  -0.83587843  4.707362  ]\n",
      "Currrent Action : tensor([[1.2091]])\n",
      "Next State : [[-0.5489146 ]\n",
      " [-0.83587843]\n",
      " [ 4.707362  ]]\n",
      "Reward : [-8.24009735]\n",
      "--------------------------------------------------\n",
      "Step : 84\n",
      "Current State : [-0.36874607 -0.92953014  4.06811   ]\n",
      "Currrent Action : tensor([[-0.0823]])\n",
      "Next State : [[-0.36874607]\n",
      " [-0.92953014]\n",
      " [ 4.06811   ]]\n",
      "Reward : [-6.84644045]\n",
      "--------------------------------------------------\n",
      "Step : 85\n",
      "Current State : [-0.20277847 -0.9792246   3.4693031 ]\n",
      "Currrent Action : tensor([[0.6556]])\n",
      "Next State : [[-0.20277847]\n",
      " [-0.9792246 ]\n",
      " [ 3.4693031 ]]\n",
      "Reward : [-5.45186267]\n",
      "--------------------------------------------------\n",
      "Step : 86\n",
      "Current State : [-0.06797695 -0.99768686  2.723302  ]\n",
      "Currrent Action : tensor([[-0.0772]])\n",
      "Next State : [[-0.06797695]\n",
      " [-0.99768686]\n",
      " [ 2.723302  ]]\n",
      "Reward : [-4.35420488]\n",
      "--------------------------------------------------\n",
      "Step : 87\n",
      "Current State : [ 0.02093389 -0.99978083  1.7792966 ]\n",
      "Currrent Action : tensor([[-1.3049]])\n",
      "Next State : [[ 0.02093389]\n",
      " [-0.99978083]\n",
      " [ 1.7792966 ]]\n",
      "Reward : [-3.42909001]\n",
      "--------------------------------------------------\n",
      "Step : 88\n",
      "Current State : [ 0.07007392 -0.9975418   0.9839196 ]\n",
      "Currrent Action : tensor([[-0.3036]])\n",
      "Next State : [[ 0.07007392]\n",
      " [-0.9975418 ]\n",
      " [ 0.9839196 ]]\n",
      "Reward : [-2.71875067]\n",
      "--------------------------------------------------\n",
      "Step : 89\n",
      "Current State : [ 0.0874652  -0.9961676   0.34891412]\n",
      "Currrent Action : tensor([[0.7543]])\n",
      "Next State : [[ 0.0874652 ]\n",
      " [-0.9961676 ]\n",
      " [ 0.34891412]]\n",
      "Reward : [-2.34937403]\n",
      "--------------------------------------------------\n",
      "Step : 90\n",
      "Current State : [ 0.07425373 -0.9972394  -0.26509938]\n",
      "Currrent Action : tensor([[0.8874]])\n",
      "Next State : [[ 0.07425373]\n",
      " [-0.9972394 ]\n",
      " [-0.26509938]]\n",
      "Reward : [-2.21290087]\n",
      "--------------------------------------------------\n",
      "Step : 91\n",
      "Current State : [ 0.03423334 -0.99941385 -0.8016421 ]\n",
      "Currrent Action : tensor([[1.4092]])\n",
      "Next State : [[ 0.03423334]\n",
      " [-0.99941385]\n",
      " [-0.8016421 ]]\n",
      "Reward : [-2.24844874]\n",
      "--------------------------------------------------\n",
      "Step : 92\n",
      "Current State : [-0.03670068 -0.9993263  -1.418979  ]\n",
      "Currrent Action : tensor([[0.8815]])\n",
      "Next State : [[-0.03670068]\n",
      " [-0.9993263 ]\n",
      " [-1.418979  ]]\n",
      "Reward : [-2.42604529]\n",
      "--------------------------------------------------\n",
      "Step : 93\n",
      "Current State : [-0.12976564 -0.99154466 -1.8684738 ]\n",
      "Currrent Action : tensor([[2.6882]])\n",
      "Next State : [[-0.12976564]\n",
      " [-0.99154466]\n",
      " [-1.8684738 ]]\n",
      "Reward : [-2.7894233]\n",
      "--------------------------------------------------\n",
      "Step : 94\n",
      "Current State : [-0.26584566 -0.9640156  -2.7789686 ]\n",
      "Currrent Action : tensor([[-1.1122]])\n",
      "Next State : [[-0.26584566]\n",
      " [-0.9640156 ]\n",
      " [-2.7789686 ]]\n",
      "Reward : [-3.24351578]\n",
      "--------------------------------------------------\n",
      "Step : 95\n",
      "Current State : [-0.43043992 -0.90261924 -3.5179813 ]\n",
      "Currrent Action : tensor([[-0.1067]])\n",
      "Next State : [[-0.43043992]\n",
      " [-0.90261924]\n",
      " [-3.5179813 ]]\n",
      "Reward : [-4.15742673]\n",
      "--------------------------------------------------\n",
      "Step : 96\n",
      "Current State : [-0.5971325 -0.8021426 -3.898829 ]\n",
      "Currrent Action : tensor([[1.9741]])\n",
      "Next State : [[-0.5971325]\n",
      " [-0.8021426]\n",
      " [-3.898829 ]]\n",
      "Reward : [-5.304871]\n",
      "--------------------------------------------------\n",
      "Step : 97\n",
      "Current State : [-0.75965375 -0.65032774 -4.457179  ]\n",
      "Currrent Action : tensor([[0.2884]])\n",
      "Next State : [[-0.75965375]\n",
      " [-0.65032774]\n",
      " [-4.457179  ]]\n",
      "Reward : [-6.40744338]\n",
      "--------------------------------------------------\n",
      "Step : 98\n",
      "Current State : [-0.894674   -0.44671968 -4.8984065 ]\n",
      "Currrent Action : tensor([[0.3101]])\n",
      "Next State : [[-0.894674  ]\n",
      " [-0.44671968]\n",
      " [-4.8984065 ]]\n",
      "Reward : [-7.90903704]\n",
      "--------------------------------------------------\n",
      "Step : 99\n",
      "Current State : [-0.9793966  -0.20194626 -5.1950154 ]\n",
      "Currrent Action : tensor([[0.2562]])\n",
      "Next State : [[-0.9793966 ]\n",
      " [-0.20194626]\n",
      " [-5.1950154 ]]\n",
      "Reward : [-9.57385157]\n",
      "learning iteration : 3\n",
      "--------------------------------------------------\n",
      "Step : 0\n",
      "Current State : [-0.93001103  0.36753154  1.1029719 ]\n",
      "Currrent Action : tensor([[0.4541]])\n",
      "Next State : [[-0.93001103]\n",
      " [ 0.36753154]\n",
      " [ 1.1029719 ]]\n",
      "Reward : [-7.39680779]\n",
      "--------------------------------------------------\n",
      "Step : 1\n",
      "Current State : [-0.9513894   0.30799067  1.2654619 ]\n",
      "Currrent Action : tensor([[-0.7544]])\n",
      "Next State : [[-0.9513894 ]\n",
      " [ 0.30799067]\n",
      " [ 1.2654619 ]]\n",
      "Reward : [-7.76877197]\n",
      "--------------------------------------------------\n",
      "Step : 2\n",
      "Current State : [-0.9704188   0.24142778  1.3848684 ]\n",
      "Currrent Action : tensor([[-0.7439]])\n",
      "Next State : [[-0.9704188 ]\n",
      " [ 0.24142778]\n",
      " [ 1.3848684 ]]\n",
      "Reward : [-8.16117493]\n",
      "--------------------------------------------------\n",
      "Step : 3\n",
      "Current State : [-0.98700327  0.16070019  1.6487372 ]\n",
      "Currrent Action : tensor([[0.5520]])\n",
      "Next State : [[-0.98700327]\n",
      " [ 0.16070019]\n",
      " [ 1.6487372 ]]\n",
      "Reward : [-8.58907924]\n",
      "--------------------------------------------------\n",
      "Step : 4\n",
      "Current State : [-0.9975342   0.07018179  1.8232098 ]\n",
      "Currrent Action : tensor([[0.3596]])\n",
      "Next State : [[-0.9975342 ]\n",
      " [ 0.07018179]\n",
      " [ 1.8232098 ]]\n",
      "Reward : [-9.15351093]\n",
      "--------------------------------------------------\n",
      "Step : 5\n",
      "Current State : [-0.9993818  -0.03515773  2.1080902 ]\n",
      "Currrent Action : tensor([[1.5483]])\n",
      "Next State : [[-0.9993818 ]\n",
      " [-0.03515773]\n",
      " [ 2.1080902 ]]\n",
      "Reward : [-9.7680166]\n",
      "--------------------------------------------------\n",
      "Step : 6\n",
      "Current State : [-0.9898309 -0.1422489  2.1513615]\n",
      "Currrent Action : tensor([[0.4643]])\n",
      "Next State : [[-0.9898309]\n",
      " [-0.1422489]\n",
      " [ 2.1513615]]\n",
      "Reward : [-10.09451287]\n",
      "--------------------------------------------------\n",
      "Step : 7\n",
      "Current State : [-0.97367394 -0.2279452   1.7446748 ]\n",
      "Currrent Action : tensor([[-2.0382]])\n",
      "Next State : [[-0.97367394]\n",
      " [-0.2279452 ]\n",
      " [ 1.7446748 ]]\n",
      "Reward : [-9.45999454]\n",
      "--------------------------------------------------\n",
      "Step : 8\n",
      "Current State : [-0.95102584 -0.30911145  1.6858355 ]\n",
      "Currrent Action : tensor([[0.7475]])\n",
      "Next State : [[-0.95102584]\n",
      " [-0.30911145]\n",
      " [ 1.6858355 ]]\n",
      "Reward : [-8.78251288]\n",
      "--------------------------------------------------\n",
      "Step : 9\n",
      "Current State : [-0.92291933 -0.38499334  1.6188407 ]\n",
      "Currrent Action : tensor([[1.0989]])\n",
      "Next State : [[-0.92291933]\n",
      " [-0.38499334]\n",
      " [ 1.6188407 ]]\n",
      "Reward : [-8.27922979]\n",
      "--------------------------------------------------\n",
      "Step : 10\n",
      "Current State : [-0.89814174 -0.43970606  1.2014151 ]\n",
      "Currrent Action : tensor([[-0.8579]])\n",
      "Next State : [[-0.89814174]\n",
      " [-0.43970606]\n",
      " [ 1.2014151 ]]\n",
      "Reward : [-7.80546977]\n",
      "--------------------------------------------------\n",
      "Step : 11\n",
      "Current State : [-0.8779443  -0.4787628   0.87947214]\n",
      "Currrent Action : tensor([[0.0522]])\n",
      "Next State : [[-0.8779443 ]\n",
      " [-0.4787628 ]\n",
      " [ 0.87947214]]\n",
      "Reward : [-7.36066466]\n",
      "--------------------------------------------------\n",
      "Step : 12\n",
      "Current State : [-0.86686754 -0.49853855  0.4533423 ]\n",
      "Currrent Action : tensor([[-0.4471]])\n",
      "Next State : [[-0.86686754]\n",
      " [-0.49853855]\n",
      " [ 0.4533423 ]]\n",
      "Reward : [-7.0595483]\n",
      "--------------------------------------------------\n",
      "Step : 13\n",
      "Current State : [-0.8662527  -0.4996061   0.02463852]\n",
      "Currrent Action : tensor([[-0.3653]])\n",
      "Next State : [[-0.8662527 ]\n",
      " [-0.4996061 ]\n",
      " [ 0.02463852]]\n",
      "Reward : [-6.88341171]\n",
      "--------------------------------------------------\n",
      "Step : 14\n",
      "Current State : [-0.8755682  -0.48309448 -0.37916952]\n",
      "Currrent Action : tensor([[-0.1940]])\n",
      "Next State : [[-0.8755682 ]\n",
      " [-0.48309448]\n",
      " [-0.37916952]]\n",
      "Reward : [-6.85637169]\n",
      "--------------------------------------------------\n",
      "Step : 15\n",
      "Current State : [-0.89404994 -0.44796732 -0.79390115]\n",
      "Currrent Action : tensor([[-0.3494]])\n",
      "Next State : [[-0.89404994]\n",
      " [-0.44796732]\n",
      " [-0.79390115]]\n",
      "Reward : [-6.97041539]\n",
      "--------------------------------------------------\n",
      "Step : 16\n",
      "Current State : [-0.9176374 -0.3974187 -1.1157663]\n",
      "Currrent Action : tensor([[0.0941]])\n",
      "Next State : [[-0.9176374]\n",
      " [-0.3974187]\n",
      " [-1.1157663]]\n",
      "Reward : [-7.22991286]\n",
      "--------------------------------------------------\n",
      "Step : 17\n",
      "Current State : [-0.93907815 -0.3437037  -1.1568826 ]\n",
      "Currrent Action : tensor([[1.7130]])\n",
      "Next State : [[-0.93907815]\n",
      " [-0.3437037 ]\n",
      " [-1.1568826 ]]\n",
      "Reward : [-7.59611826]\n",
      "--------------------------------------------------\n",
      "Step : 18\n",
      "Current State : [-0.9582544  -0.28591692 -1.2178972 ]\n",
      "Currrent Action : tensor([[1.3118]])\n",
      "Next State : [[-0.9582544 ]\n",
      " [-0.28591692]\n",
      " [-1.2178972 ]]\n",
      "Reward : [-7.92375826]\n",
      "--------------------------------------------------\n",
      "Step : 19\n",
      "Current State : [-0.9756641  -0.21927068 -1.3779253 ]\n",
      "Currrent Action : tensor([[0.3627]])\n",
      "Next State : [[-0.9756641 ]\n",
      " [-0.21927068]\n",
      " [-1.3779253 ]]\n",
      "Reward : [-8.2802497]\n",
      "--------------------------------------------------\n",
      "Step : 20\n",
      "Current State : [-0.98923975 -0.14630356 -1.4847262 ]\n",
      "Currrent Action : tensor([[0.3843]])\n",
      "Next State : [[-0.98923975]\n",
      " [-0.14630356]\n",
      " [-1.4847262 ]]\n",
      "Reward : [-8.71948627]\n",
      "--------------------------------------------------\n",
      "Step : 21\n",
      "Current State : [-0.99793875 -0.06417359 -1.6522573 ]\n",
      "Currrent Action : tensor([[-0.3854]])\n",
      "Next State : [[-0.99793875]\n",
      " [-0.06417359]\n",
      " [-1.6522573 ]]\n",
      "Reward : [-9.18918952]\n",
      "--------------------------------------------------\n",
      "Step : 22\n",
      "Current State : [-0.9995951   0.02845445 -1.8535202 ]\n",
      "Currrent Action : tensor([[-1.0209]])\n",
      "Next State : [[-0.9995951 ]\n",
      " [ 0.02845445]\n",
      " [-1.8535202 ]]\n",
      "Reward : [-9.74427414]\n",
      "--------------------------------------------------\n",
      "Step : 23\n",
      "Current State : [-0.9933835   0.11484411 -1.7327956 ]\n",
      "Currrent Action : tensor([[0.6626]])\n",
      "Next State : [[-0.9933835 ]\n",
      " [ 0.11484411]\n",
      " [-1.7327956 ]]\n",
      "Reward : [-10.03559825]\n",
      "--------------------------------------------------\n",
      "Step : 24\n",
      "Current State : [-0.980655    0.19574414 -1.6383629 ]\n",
      "Currrent Action : tensor([[0.0553]])\n",
      "Next State : [[-0.980655  ]\n",
      " [ 0.19574414]\n",
      " [-1.6383629 ]]\n",
      "Reward : [-9.45993058]\n",
      "--------------------------------------------------\n",
      "Step : 25\n",
      "Current State : [-0.96644527  0.25687265 -1.2553732 ]\n",
      "Currrent Action : tensor([[1.5745]])\n",
      "Next State : [[-0.96644527]\n",
      " [ 0.25687265]\n",
      " [-1.2553732 ]]\n",
      "Reward : [-8.94143289]\n",
      "--------------------------------------------------\n",
      "Step : 26\n",
      "Current State : [-0.9553547   0.2954614  -0.80307096]\n",
      "Currrent Action : tensor([[1.7310]])\n",
      "Next State : [[-0.9553547 ]\n",
      " [ 0.2954614 ]\n",
      " [-0.80307096]]\n",
      "Reward : [-8.46540858]\n",
      "--------------------------------------------------\n",
      "Step : 27\n",
      "Current State : [-0.9486016   0.31647283 -0.44140905]\n",
      "Currrent Action : tensor([[0.9338]])\n",
      "Next State : [[-0.9486016 ]\n",
      " [ 0.31647283]\n",
      " [-0.44140905]]\n",
      "Reward : [-8.14036301]\n",
      "--------------------------------------------------\n",
      "Step : 28\n",
      "Current State : [-0.9448375  0.3275395 -0.2337878]\n",
      "Currrent Action : tensor([[-0.1982]])\n",
      "Next State : [[-0.9448375]\n",
      " [ 0.3275395]\n",
      " [-0.2337878]]\n",
      "Reward : [-7.96957619]\n",
      "--------------------------------------------------\n",
      "Step : 29\n",
      "Current State : [-0.9476537   0.3193      0.17415059]\n",
      "Currrent Action : tensor([[1.0819]])\n",
      "Next State : [[-0.9476537 ]\n",
      " [ 0.3193    ]\n",
      " [ 0.17415059]]\n",
      "Reward : [-7.89090708]\n",
      "--------------------------------------------------\n",
      "Step : 30\n",
      "Current State : [-0.95093286  0.30939722  0.20863278]\n",
      "Currrent Action : tensor([[-1.3666]])\n",
      "Next State : [[-0.95093286]\n",
      " [ 0.30939722]\n",
      " [ 0.20863278]]\n",
      "Reward : [-7.93814687]\n",
      "--------------------------------------------------\n",
      "Step : 31\n",
      "Current State : [-0.9585801   0.28482312  0.5147437 ]\n",
      "Currrent Action : tensor([[0.4938]])\n",
      "Next State : [[-0.9585801 ]\n",
      " [ 0.28482312]\n",
      " [ 0.5147437 ]]\n",
      "Reward : [-7.99671531]\n",
      "--------------------------------------------------\n",
      "Step : 32\n",
      "Current State : [-0.96843827  0.24925363  0.7382486 ]\n",
      "Currrent Action : tensor([[0.0659]])\n",
      "Next State : [[-0.96843827]\n",
      " [ 0.24925363]\n",
      " [ 0.7382486 ]]\n",
      "Reward : [-8.16480138]\n",
      "--------------------------------------------------\n",
      "Step : 33\n",
      "Current State : [-0.97909147  0.2034204   0.94118756]\n",
      "Currrent Action : tensor([[0.1067]])\n",
      "Next State : [[-0.97909147]\n",
      " [ 0.2034204 ]\n",
      " [ 0.94118756]]\n",
      "Reward : [-8.40478132]\n",
      "--------------------------------------------------\n",
      "Step : 34\n",
      "Current State : [-0.9887424   0.14962761  1.0931695 ]\n",
      "Currrent Action : tensor([[-0.0039]])\n",
      "Next State : [[-0.9887424 ]\n",
      " [ 0.14962761]\n",
      " [ 1.0931695 ]]\n",
      "Reward : [-8.71304024]\n",
      "--------------------------------------------------\n",
      "Step : 35\n",
      "Current State : [-0.9961556   0.08760094  1.2495652 ]\n",
      "Currrent Action : tensor([[0.2945]])\n",
      "Next State : [[-0.9961556 ]\n",
      " [ 0.08760094]\n",
      " [ 1.2495652 ]]\n",
      "Reward : [-9.06806878]\n",
      "--------------------------------------------------\n",
      "Step : 36\n",
      "Current State : [-0.999809    0.01954388  1.3633649 ]\n",
      "Currrent Action : tensor([[0.3207]])\n",
      "Next State : [[-0.999809  ]\n",
      " [ 0.01954388]\n",
      " [ 1.3633649 ]]\n",
      "Reward : [-9.48242284]\n",
      "--------------------------------------------------\n",
      "Step : 37\n",
      "Current State : [-0.9983678  -0.05711175  1.5337592 ]\n",
      "Currrent Action : tensor([[1.0382]])\n",
      "Next State : [[-0.9983678 ]\n",
      " [-0.05711175]\n",
      " [ 1.5337592 ]]\n",
      "Reward : [-9.93413513]\n",
      "--------------------------------------------------\n",
      "Step : 38\n",
      "Current State : [-0.99109894 -0.13312738  1.5276188 ]\n",
      "Currrent Action : tensor([[0.2446]])\n",
      "Next State : [[-0.99109894]\n",
      " [-0.13312738]\n",
      " [ 1.5276188 ]]\n",
      "Reward : [-9.74913223]\n",
      "--------------------------------------------------\n",
      "Step : 39\n",
      "Current State : [-0.97848    -0.20634174  1.4862199 ]\n",
      "Currrent Action : tensor([[0.3896]])\n",
      "Next State : [[-0.97848   ]\n",
      " [-0.20634174]\n",
      " [ 1.4862199 ]]\n",
      "Reward : [-9.2819921]\n",
      "--------------------------------------------------\n",
      "Step : 40\n",
      "Current State : [-0.9611533 -0.2760151  1.4362179]\n",
      "Currrent Action : tensor([[0.6984]])\n",
      "Next State : [[-0.9611533]\n",
      " [-0.2760151]\n",
      " [ 1.4362179]]\n",
      "Reward : [-8.82830798]\n",
      "--------------------------------------------------\n",
      "Step : 41\n",
      "Current State : [-0.9395653 -0.3423697  1.3958441]\n",
      "Currrent Action : tensor([[1.1109]])\n",
      "Next State : [[-0.9395653]\n",
      " [-0.3423697]\n",
      " [ 1.3958441]]\n",
      "Reward : [-8.39824689]\n",
      "--------------------------------------------------\n",
      "Step : 42\n",
      "Current State : [-0.9173014  -0.39819354  1.2021762 ]\n",
      "Currrent Action : tensor([[0.4207]])\n",
      "Next State : [[-0.9173014 ]\n",
      " [-0.39819354]\n",
      " [ 1.2021762 ]]\n",
      "Reward : [-7.99114343]\n",
      "--------------------------------------------------\n",
      "Step : 43\n",
      "Current State : [-0.8997393  -0.43642777  0.8415571 ]\n",
      "Currrent Action : tensor([[-0.4132]])\n",
      "Next State : [[-0.8997393 ]\n",
      " [-0.43642777]\n",
      " [ 0.8415571 ]]\n",
      "Reward : [-7.60876866]\n",
      "--------------------------------------------------\n",
      "Step : 44\n",
      "Current State : [-0.88618994 -0.46332216  0.6023163 ]\n",
      "Currrent Action : tensor([[0.5872]])\n",
      "Next State : [[-0.88618994]\n",
      " [-0.46332216]\n",
      " [ 0.6023163 ]]\n",
      "Reward : [-7.30709511]\n",
      "--------------------------------------------------\n",
      "Step : 45\n",
      "Current State : [-0.88420796 -0.4670934   0.08520658]\n",
      "Currrent Action : tensor([[-1.1308]])\n",
      "Next State : [[-0.88420796]\n",
      " [-0.4670934 ]\n",
      " [ 0.08520658]]\n",
      "Reward : [-7.11237143]\n",
      "--------------------------------------------------\n",
      "Step : 46\n",
      "Current State : [-0.8935636  -0.44893655 -0.4085167 ]\n",
      "Currrent Action : tensor([[-0.9560]])\n",
      "Next State : [[-0.8935636 ]\n",
      " [-0.44893655]\n",
      " [-0.4085167 ]]\n",
      "Reward : [-7.0538087]\n",
      "--------------------------------------------------\n",
      "Step : 47\n",
      "Current State : [-0.9108545 -0.4127275 -0.8025672]\n",
      "Currrent Action : tensor([[-0.3823]])\n",
      "Next State : [[-0.9108545]\n",
      " [-0.4127275]\n",
      " [-0.8025672]]\n",
      "Reward : [-7.17790605]\n",
      "--------------------------------------------------\n",
      "Step : 48\n",
      "Current State : [-0.9333299  -0.35901994 -1.1645777 ]\n",
      "Currrent Action : tensor([[-0.3498]])\n",
      "Next State : [[-0.9333299 ]\n",
      " [-0.35901994]\n",
      " [-1.1645777 ]]\n",
      "Reward : [-7.44198372]\n",
      "--------------------------------------------------\n",
      "Step : 49\n",
      "Current State : [-0.9581702  -0.28619903 -1.5392003 ]\n",
      "Currrent Action : tensor([[-0.7024]])\n",
      "Next State : [[-0.9581702 ]\n",
      " [-0.28619903]\n",
      " [-1.5392003 ]]\n",
      "Reward : [-7.83327441]\n",
      "--------------------------------------------------\n",
      "Step : 50\n",
      "Current State : [-0.9801062 -0.1984739 -1.80914  ]\n",
      "Currrent Action : tensor([[-0.3686]])\n",
      "Next State : [[-0.9801062]\n",
      " [-0.1984739]\n",
      " [-1.80914  ]]\n",
      "Reward : [-8.36716127]\n",
      "--------------------------------------------------\n",
      "Step : 51\n",
      "Current State : [-0.99492997 -0.10056989 -1.9812078 ]\n",
      "Currrent Action : tensor([[-0.1548]])\n",
      "Next State : [[-0.99492997]\n",
      " [-0.10056989]\n",
      " [-1.9812078 ]]\n",
      "Reward : [-8.9814632]\n",
      "--------------------------------------------------\n",
      "Step : 52\n",
      "Current State : [-9.9999988e-01 -4.6145628e-04 -2.0055749e+00]\n",
      "Currrent Action : tensor([[0.3404]])\n",
      "Next State : [[-9.9999988e-01]\n",
      " [-4.6145628e-04]\n",
      " [-2.0055749e+00]]\n",
      "Reward : [-9.63941799]\n",
      "--------------------------------------------------\n",
      "Step : 53\n",
      "Current State : [-0.99617445  0.08738682 -1.7591976 ]\n",
      "Currrent Action : tensor([[1.6448]])\n",
      "Next State : [[-0.99617445]\n",
      " [ 0.08738682]\n",
      " [-1.7591976 ]]\n",
      "Reward : [-10.27164369]\n",
      "--------------------------------------------------\n",
      "Step : 54\n",
      "Current State : [-0.98476917  0.17386694 -1.745133  ]\n",
      "Currrent Action : tensor([[-0.3432]])\n",
      "Next State : [[-0.98476917]\n",
      " [ 0.17386694]\n",
      " [-1.745133  ]]\n",
      "Reward : [-9.63708695]\n",
      "--------------------------------------------------\n",
      "Step : 55\n",
      "Current State : [-0.9683885  0.2494467 -1.5470759]\n",
      "Currrent Action : tensor([[0.4510]])\n",
      "Next State : [[-0.9683885]\n",
      " [ 0.2494467]\n",
      " [-1.5470759]]\n",
      "Reward : [-9.10687761]\n",
      "--------------------------------------------------\n",
      "Step : 56\n",
      "Current State : [-0.9510894   0.30891576 -1.238879  ]\n",
      "Currrent Action : tensor([[0.8074]])\n",
      "Next State : [[-0.9510894 ]\n",
      " [ 0.30891576]\n",
      " [-1.238879  ]]\n",
      "Reward : [-8.58911286]\n",
      "--------------------------------------------------\n",
      "Step : 57\n",
      "Current State : [-0.9358592   0.35237417 -0.92107904]\n",
      "Currrent Action : tensor([[0.5741]])\n",
      "Next State : [[-0.9358592 ]\n",
      " [ 0.35237417]\n",
      " [-0.92107904]]\n",
      "Reward : [-8.14879325]\n",
      "--------------------------------------------------\n",
      "Step : 58\n",
      "Current State : [-0.92780167  0.37307388 -0.44426253]\n",
      "Currrent Action : tensor([[1.4169]])\n",
      "Next State : [[-0.92780167]\n",
      " [ 0.37307388]\n",
      " [-0.44426253]]\n",
      "Reward : [-7.82351003]\n",
      "--------------------------------------------------\n",
      "Step : 59\n",
      "Current State : [-0.92439675  0.3814324  -0.18050976]\n",
      "Currrent Action : tensor([[-0.1070]])\n",
      "Next State : [[-0.92439675]\n",
      " [ 0.3814324 ]\n",
      " [-0.18050976]]\n",
      "Reward : [-7.63333455]\n",
      "--------------------------------------------------\n",
      "Step : 60\n",
      "Current State : [-0.9287797   0.37063223  0.23311491]\n",
      "Currrent Action : tensor([[0.8503]])\n",
      "Next State : [[-0.9287797 ]\n",
      " [ 0.37063223]\n",
      " [ 0.23311491]]\n",
      "Reward : [-7.56784152]\n",
      "--------------------------------------------------\n",
      "Step : 61\n",
      "Current State : [-0.93853694  0.34517884  0.5452059 ]\n",
      "Currrent Action : tensor([[0.2274]])\n",
      "Next State : [[-0.93853694]\n",
      " [ 0.34517884]\n",
      " [ 0.5452059 ]]\n",
      "Reward : [-7.63359428]\n",
      "--------------------------------------------------\n",
      "Step : 62\n",
      "Current State : [-0.95571494  0.294294    1.0742526 ]\n",
      "Currrent Action : tensor([[1.8011]])\n",
      "Next State : [[-0.95571494]\n",
      " [ 0.294294  ]\n",
      " [ 1.0742526 ]]\n",
      "Reward : [-7.81240084]\n",
      "--------------------------------------------------\n",
      "Step : 63\n",
      "Current State : [-0.9733099   0.22949484  1.3431615 ]\n",
      "Currrent Action : tensor([[0.3213]])\n",
      "Next State : [[-0.9733099 ]\n",
      " [ 0.22949484]\n",
      " [ 1.3431615 ]]\n",
      "Reward : [-8.19744868]\n",
      "--------------------------------------------------\n",
      "Step : 64\n",
      "Current State : [-0.98851705  0.15110926  1.5973663 ]\n",
      "Currrent Action : tensor([[0.5472]])\n",
      "Next State : [[-0.98851705]\n",
      " [ 0.15110926]\n",
      " [ 1.5973663 ]]\n",
      "Reward : [-8.64900572]\n",
      "--------------------------------------------------\n",
      "Step : 65\n",
      "Current State : [-0.9975936   0.06933298  1.6460335 ]\n",
      "Currrent Action : tensor([[-0.4311]])\n",
      "Next State : [[-0.9975936 ]\n",
      " [ 0.06933298]\n",
      " [ 1.6460335 ]]\n",
      "Reward : [-9.19485974]\n",
      "--------------------------------------------------\n",
      "Step : 66\n",
      "Current State : [-0.9998411  -0.01782688  1.7443293 ]\n",
      "Currrent Action : tensor([[0.3086]])\n",
      "Next State : [[-0.9998411 ]\n",
      " [-0.01782688]\n",
      " [ 1.7443293 ]]\n",
      "Reward : [-9.70947535]\n",
      "--------------------------------------------------\n",
      "Step : 67\n",
      "Current State : [-0.9940434  -0.10898501  1.8274819 ]\n",
      "Currrent Action : tensor([[0.6435]])\n",
      "Next State : [[-0.9940434 ]\n",
      " [-0.10898501]\n",
      " [ 1.8274819 ]]\n",
      "Reward : [-10.06258928]\n",
      "--------------------------------------------------\n",
      "Step : 68\n",
      "Current State : [-0.98009497 -0.19852933  1.8131047 ]\n",
      "Currrent Action : tensor([[0.4491]])\n",
      "Next State : [[-0.98009497]\n",
      " [-0.19852933]\n",
      " [ 1.8131047 ]]\n",
      "Reward : [-9.52956426]\n",
      "--------------------------------------------------\n",
      "Step : 69\n",
      "Current State : [-0.9631252  -0.26905343  1.451059  ]\n",
      "Currrent Action : tensor([[-1.4210]])\n",
      "Next State : [[-0.9631252 ]\n",
      " [-0.26905343]\n",
      " [ 1.451059  ]]\n",
      "Reward : [-8.98456186]\n",
      "--------------------------------------------------\n",
      "Step : 70\n",
      "Current State : [-0.9437535  -0.33064985  1.2916396 ]\n",
      "Currrent Action : tensor([[0.2825]])\n",
      "Next State : [[-0.9437535 ]\n",
      " [-0.33064985]\n",
      " [ 1.2916396 ]]\n",
      "Reward : [-8.44284553]\n",
      "--------------------------------------------------\n",
      "Step : 71\n",
      "Current State : [-0.9255808  -0.37855017  1.0247465 ]\n",
      "Currrent Action : tensor([[-0.1260]])\n",
      "Next State : [[-0.9255808 ]\n",
      " [-0.37855017]\n",
      " [ 1.0247465 ]]\n",
      "Reward : [-8.03263356]\n",
      "--------------------------------------------------\n",
      "Step : 72\n",
      "Current State : [-0.9070268 -0.4210728  0.9279674]\n",
      "Currrent Action : tensor([[1.2476]])\n",
      "Next State : [[-0.9070268]\n",
      " [-0.4210728]\n",
      " [ 0.9279674]]\n",
      "Reward : [-7.68757614]\n",
      "--------------------------------------------------\n",
      "Step : 73\n",
      "Current State : [-0.8916681  -0.45268974  0.7030354 ]\n",
      "Currrent Action : tensor([[0.6058]])\n",
      "Next State : [[-0.8916681 ]\n",
      " [-0.45268974]\n",
      " [ 0.7030354 ]]\n",
      "Reward : [-7.41413823]\n",
      "--------------------------------------------------\n",
      "Step : 74\n",
      "Current State : [-0.8855751  -0.4644962   0.26572177]\n",
      "Currrent Action : tensor([[-0.6520]])\n",
      "Next State : [[-0.8855751 ]\n",
      " [-0.4644962 ]\n",
      " [ 0.26572177]]\n",
      "Reward : [-7.18843625]\n",
      "--------------------------------------------------\n",
      "Step : 75\n",
      "Current State : [-0.8883738  -0.45912096 -0.12120393]\n",
      "Currrent Action : tensor([[-0.2570]])\n",
      "Next State : [[-0.8883738 ]\n",
      " [-0.45912096]\n",
      " [-0.12120393]]\n",
      "Reward : [-7.07489279]\n",
      "--------------------------------------------------\n",
      "Step : 76\n",
      "Current State : [-0.8981218  -0.43974683 -0.43377373]\n",
      "Currrent Action : tensor([[0.2118]])\n",
      "Next State : [[-0.8981218 ]\n",
      " [-0.43974683]\n",
      " [-0.43377373]]\n",
      "Reward : [-7.10153894]\n",
      "--------------------------------------------------\n",
      "Step : 77\n",
      "Current State : [-0.9162686  -0.4005645  -0.86367816]\n",
      "Currrent Action : tensor([[-0.6673]])\n",
      "Next State : [[-0.9162686 ]\n",
      " [-0.4005645 ]\n",
      " [-0.86367816]]\n",
      "Reward : [-7.23533949]\n",
      "--------------------------------------------------\n",
      "Step : 78\n",
      "Current State : [-0.93909377 -0.3436611  -1.2264035 ]\n",
      "Currrent Action : tensor([[-0.4153]])\n",
      "Next State : [[-0.93909377]\n",
      " [-0.3436611 ]\n",
      " [-1.2264035 ]]\n",
      "Reward : [-7.52471737]\n",
      "--------------------------------------------------\n",
      "Step : 79\n",
      "Current State : [-0.9624848  -0.27133578 -1.5206412 ]\n",
      "Currrent Action : tensor([[-0.2433]])\n",
      "Next State : [[-0.9624848 ]\n",
      " [-0.27133578]\n",
      " [-1.5206412 ]]\n",
      "Reward : [-7.93891869]\n",
      "--------------------------------------------------\n",
      "Step : 80\n",
      "Current State : [-0.98335785 -0.18167916 -1.8417369 ]\n",
      "Currrent Action : tensor([[-0.7840]])\n",
      "Next State : [[-0.98335785]\n",
      " [-0.18167916]\n",
      " [-1.8417369 ]]\n",
      "Reward : [-8.45046092]\n",
      "--------------------------------------------------\n",
      "Step : 81\n",
      "Current State : [-0.99609125 -0.08833037 -1.8849623 ]\n",
      "Currrent Action : tensor([[0.6202]])\n",
      "Next State : [[-0.99609125]\n",
      " [-0.08833037]\n",
      " [-1.8849623 ]]\n",
      "Reward : [-9.09466687]\n",
      "--------------------------------------------------\n",
      "Step : 82\n",
      "Current State : [-0.99980927  0.01953162 -2.15957   ]\n",
      "Currrent Action : tensor([[-1.3891]])\n",
      "Next State : [[-0.99980927]\n",
      " [ 0.01953162]\n",
      " [-2.15957   ]]\n",
      "Reward : [-9.67894446]\n",
      "--------------------------------------------------\n",
      "Step : 83\n",
      "Current State : [-0.99272704  0.12038715 -2.0229402 ]\n",
      "Currrent Action : tensor([[0.8132]])\n",
      "Next State : [[-0.99272704]\n",
      " [ 0.12038715]\n",
      " [-2.0229402 ]]\n",
      "Reward : [-10.21429292]\n",
      "--------------------------------------------------\n",
      "Step : 84\n",
      "Current State : [-0.97624207  0.21668266 -1.954705  ]\n",
      "Currrent Action : tensor([[-0.1470]])\n",
      "Next State : [[-0.97624207]\n",
      " [ 0.21668266]\n",
      " [-1.954705  ]]\n",
      "Reward : [-9.53516438]\n",
      "--------------------------------------------------\n",
      "Step : 85\n",
      "Current State : [-0.95568836  0.29438037 -1.6078393 ]\n",
      "Currrent Action : tensor([[1.2290]])\n",
      "Next State : [[-0.95568836]\n",
      " [ 0.29438037]\n",
      " [-1.6078393 ]]\n",
      "Reward : [-8.92856461]\n",
      "--------------------------------------------------\n",
      "Step : 86\n",
      "Current State : [-0.9307368  0.3656899 -1.5113369]\n",
      "Currrent Action : tensor([[-0.8286]])\n",
      "Next State : [[-0.9307368]\n",
      " [ 0.3656899]\n",
      " [-1.5113369]]\n",
      "Reward : [-8.34063104]\n",
      "--------------------------------------------------\n",
      "Step : 87\n",
      "Current State : [-0.90748847  0.42007703 -1.1831254 ]\n",
      "Currrent Action : tensor([[0.3596]])\n",
      "Next State : [[-0.90748847]\n",
      " [ 0.42007703]\n",
      " [-1.1831254 ]]\n",
      "Reward : [-7.88604275]\n",
      "--------------------------------------------------\n",
      "Step : 88\n",
      "Current State : [-0.88609904  0.46349594 -0.96812516]\n",
      "Currrent Action : tensor([[-0.6671]])\n",
      "Next State : [[-0.88609904]\n",
      " [ 0.46349594]\n",
      " [-0.96812516]]\n",
      "Reward : [-7.4740258]\n",
      "--------------------------------------------------\n",
      "Step : 89\n",
      "Current State : [-0.87357676  0.48668635 -0.52712166]\n",
      "Currrent Action : tensor([[0.6225]])\n",
      "Next State : [[-0.87357676]\n",
      " [ 0.48668635]\n",
      " [-0.52712166]]\n",
      "Reward : [-7.16788531]\n",
      "--------------------------------------------------\n",
      "Step : 90\n",
      "Current State : [-0.8634964   0.50435495 -0.4068457 ]\n",
      "Currrent Action : tensor([[-1.6316]])\n",
      "Next State : [[-0.8634964 ]\n",
      " [ 0.50435495]\n",
      " [-0.4068457 ]]\n",
      "Reward : [-6.96471733]\n",
      "--------------------------------------------------\n",
      "Step : 91\n",
      "Current State : [-0.8622122  0.5065473 -0.0508146]\n",
      "Currrent Action : tensor([[-0.1482]])\n",
      "Next State : [[-0.8622122]\n",
      " [ 0.5065473]\n",
      " [-0.0508146]]\n",
      "Reward : [-6.84412296]\n",
      "--------------------------------------------------\n",
      "Step : 92\n",
      "Current State : [-0.8742523   0.48547179  0.48545653]\n",
      "Currrent Action : tensor([[1.0424]])\n",
      "Next State : [[-0.8742523 ]\n",
      " [ 0.48547179]\n",
      " [ 0.48545653]]\n",
      "Reward : [-6.81562228]\n",
      "--------------------------------------------------\n",
      "Step : 93\n",
      "Current State : [-0.89848375  0.43900678  1.0481957 ]\n",
      "Currrent Action : tensor([[1.3242]])\n",
      "Next State : [[-0.89848375]\n",
      " [ 0.43900678]\n",
      " [ 1.0481957 ]]\n",
      "Reward : [-6.96691144]\n",
      "--------------------------------------------------\n",
      "Step : 94\n",
      "Current State : [-0.926973   0.3751281  1.3991598]\n",
      "Currrent Action : tensor([[0.1447]])\n",
      "Next State : [[-0.926973 ]\n",
      " [ 0.3751281]\n",
      " [ 1.3991598]]\n",
      "Reward : [-7.33039727]\n",
      "--------------------------------------------------\n",
      "Step : 95\n",
      "Current State : [-0.95429367  0.29887047  1.6205225 ]\n",
      "Currrent Action : tensor([[-0.3999]])\n",
      "Next State : [[-0.95429367]\n",
      " [ 0.29887047]\n",
      " [ 1.6205225 ]]\n",
      "Reward : [-7.79729196]\n",
      "--------------------------------------------------\n",
      "Step : 96\n",
      "Current State : [-0.9807998   0.19501726  2.1446753 ]\n",
      "Currrent Action : tensor([[2.6935]])\n",
      "Next State : [[-0.9807998 ]\n",
      " [ 0.19501726]\n",
      " [ 2.1446753 ]]\n",
      "Reward : [-8.32132916]\n",
      "--------------------------------------------------\n",
      "Step : 97\n",
      "Current State : [-0.9965369   0.08315194  2.26054   ]\n",
      "Currrent Action : tensor([[-0.2027]])\n",
      "Next State : [[-0.9965369 ]\n",
      " [ 0.08315194]\n",
      " [ 2.26054   ]]\n",
      "Reward : [-9.13490007]\n",
      "--------------------------------------------------\n",
      "Step : 98\n",
      "Current State : [-0.99909806 -0.04246249  2.5144663 ]\n",
      "Currrent Action : tensor([[1.2771]])\n",
      "Next State : [[-0.99909806]\n",
      " [-0.04246249]\n",
      " [ 2.5144663 ]]\n",
      "Reward : [-9.86610671]\n",
      "--------------------------------------------------\n",
      "Step : 99\n",
      "Current State : [-0.9879449 -0.1548058  2.2591124]\n",
      "Currrent Action : tensor([[-1.4900]])\n",
      "Next State : [[-0.9879449]\n",
      " [-0.1548058]\n",
      " [ 2.2591124]]\n",
      "Reward : [-10.23900293]\n",
      "learning iteration : 4\n",
      "--------------------------------------------------\n",
      "Step : 0\n",
      "Current State : [ 0.08544844  0.9963426  -0.15087318]\n",
      "Currrent Action : tensor([[0.5351]])\n",
      "Next State : [[ 0.08544844]\n",
      " [ 0.9963426 ]\n",
      " [-0.15087318]]\n",
      "Reward : [-2.32451476]\n",
      "--------------------------------------------------\n",
      "Step : 1\n",
      "Current State : [0.05118588 0.9986891  0.68688995]\n",
      "Currrent Action : tensor([[0.6034]])\n",
      "Next State : [[0.05118588]\n",
      " [0.9986891 ]\n",
      " [0.68688995]]\n",
      "Reward : [-2.20858879]\n",
      "--------------------------------------------------\n",
      "Step : 2\n",
      "Current State : [-0.02254848  0.9997457   1.4751731 ]\n",
      "Currrent Action : tensor([[0.2618]])\n",
      "Next State : [[-0.02254848]\n",
      " [ 0.9997457 ]\n",
      " [ 1.4751731 ]]\n",
      "Reward : [-2.3563982]\n",
      "--------------------------------------------------\n",
      "Step : 3\n",
      "Current State : [-0.13281581  0.9911407   2.2131805 ]\n",
      "Currrent Action : tensor([[-0.0787]])\n",
      "Next State : [[-0.13281581]\n",
      " [ 0.9911407 ]\n",
      " [ 2.2131805 ]]\n",
      "Reward : [-2.75637352]\n",
      "--------------------------------------------------\n",
      "Step : 4\n",
      "Current State : [-0.27505404  0.96142876  2.9087298 ]\n",
      "Currrent Action : tensor([[-0.3187]])\n",
      "Next State : [[-0.27505404]\n",
      " [ 0.96142876]\n",
      " [ 2.9087298 ]]\n",
      "Reward : [-3.39355396]\n",
      "--------------------------------------------------\n",
      "Step : 5\n",
      "Current State : [-0.44625378  0.89490646  3.678578  ]\n",
      "Currrent Action : tensor([[0.3252]])\n",
      "Next State : [[-0.44625378]\n",
      " [ 0.89490646]\n",
      " [ 3.678578  ]]\n",
      "Reward : [-4.26661314]\n",
      "--------------------------------------------------\n",
      "Step : 6\n",
      "Current State : [-0.6329964  0.7741547  4.4568567]\n",
      "Currrent Action : tensor([[0.7140]])\n",
      "Next State : [[-0.6329964]\n",
      " [ 0.7741547]\n",
      " [ 4.4568567]]\n",
      "Reward : [-5.48830146]\n",
      "--------------------------------------------------\n",
      "Step : 7\n",
      "Current State : [-0.8073623  0.590056   5.0850096]\n",
      "Currrent Action : tensor([[0.3169]])\n",
      "Next State : [[-0.8073623]\n",
      " [ 0.590056 ]\n",
      " [ 5.0850096]]\n",
      "Reward : [-7.07695914]\n",
      "--------------------------------------------------\n",
      "Step : 8\n",
      "Current State : [-0.9405992  0.3395189  5.6944566]\n",
      "Currrent Action : tensor([[1.1127]])\n",
      "Next State : [[-0.9405992]\n",
      " [ 0.3395189]\n",
      " [ 5.6944566]]\n",
      "Reward : [-8.88940218]\n",
      "--------------------------------------------------\n",
      "Step : 9\n",
      "Current State : [-0.99869525  0.05106646  5.906334  ]\n",
      "Currrent Action : tensor([[-0.2851]])\n",
      "Next State : [[-0.99869525]\n",
      " [ 0.05106646]\n",
      " [ 5.906334  ]]\n",
      "Reward : [-11.05583673]\n",
      "--------------------------------------------------\n",
      "Step : 10\n",
      "Current State : [-0.96825206 -0.24997577  6.074878  ]\n",
      "Currrent Action : tensor([[0.8683]])\n",
      "Next State : [[-0.96825206]\n",
      " [-0.24997577]\n",
      " [ 6.074878  ]]\n",
      "Reward : [-13.04044677]\n",
      "--------------------------------------------------\n",
      "Step : 11\n",
      "Current State : [-0.8571897 -0.5150007  5.767065 ]\n",
      "Currrent Action : tensor([[-0.8022]])\n",
      "Next State : [[-0.8571897]\n",
      " [-0.5150007]\n",
      " [ 5.767065 ]]\n",
      "Reward : [-12.0370176]\n",
      "--------------------------------------------------\n",
      "Step : 12\n",
      "Current State : [-0.69926524 -0.7148623   5.108373  ]\n",
      "Currrent Action : tensor([[-1.8163]])\n",
      "Next State : [[-0.69926524]\n",
      " [-0.7148623 ]\n",
      " [ 5.108373  ]]\n",
      "Reward : [-10.09224091]\n",
      "--------------------------------------------------\n",
      "Step : 13\n",
      "Current State : [-0.520908  -0.8536128  4.529096 ]\n",
      "Currrent Action : tensor([[-0.2875]])\n",
      "Next State : [[-0.520908 ]\n",
      " [-0.8536128]\n",
      " [ 4.529096 ]]\n",
      "Reward : [-8.10943173]\n",
      "--------------------------------------------------\n",
      "Step : 14\n",
      "Current State : [-0.34293938 -0.9393575   3.9574022 ]\n",
      "Currrent Action : tensor([[0.4568]])\n",
      "Next State : [[-0.34293938]\n",
      " [-0.9393575 ]\n",
      " [ 3.9574022 ]]\n",
      "Reward : [-6.54041489]\n",
      "--------------------------------------------------\n",
      "Step : 15\n",
      "Current State : [-0.17154731 -0.9851759   3.552884  ]\n",
      "Currrent Action : tensor([[2.3764]])\n",
      "Next State : [[-0.17154731]\n",
      " [-0.9851759 ]\n",
      " [ 3.552884  ]]\n",
      "Reward : [-5.25973167]\n",
      "--------------------------------------------------\n",
      "Step : 16\n",
      "Current State : [-0.0349802 -0.999388   2.748254 ]\n",
      "Currrent Action : tensor([[-0.4383]])\n",
      "Next State : [[-0.0349802]\n",
      " [-0.999388 ]\n",
      " [ 2.748254 ]]\n",
      "Reward : [-4.30122418]\n",
      "--------------------------------------------------\n",
      "Step : 17\n",
      "Current State : [ 0.07986318 -0.99680585  2.2987132 ]\n",
      "Currrent Action : tensor([[2.0121]])\n",
      "Next State : [[ 0.07986318]\n",
      " [-0.99680585]\n",
      " [ 2.2987132 ]]\n",
      "Reward : [-3.33783126]\n",
      "--------------------------------------------------\n",
      "Step : 18\n",
      "Current State : [ 0.15616424 -0.9877311   1.5371546 ]\n",
      "Currrent Action : tensor([[-0.0930]])\n",
      "Next State : [[ 0.15616424]\n",
      " [-0.9877311 ]\n",
      " [ 1.5371546 ]]\n",
      "Reward : [-2.75104465]\n",
      "--------------------------------------------------\n",
      "Step : 19\n",
      "Current State : [ 0.20432082 -0.978904    0.97927576]\n",
      "Currrent Action : tensor([[1.2195]])\n",
      "Next State : [[ 0.20432082]\n",
      " [-0.978904  ]\n",
      " [ 0.97927576]]\n",
      "Reward : [-2.23714002]\n",
      "--------------------------------------------------\n",
      "Step : 20\n",
      "Current State : [ 0.22675444 -0.97395194  0.45948377]\n",
      "Currrent Action : tensor([[1.4292]])\n",
      "Next State : [[ 0.22675444]\n",
      " [-0.97395194]\n",
      " [ 0.45948377]]\n",
      "Reward : [-1.96123815]\n",
      "--------------------------------------------------\n",
      "Step : 21\n",
      "Current State : [ 0.20659372 -0.9784268  -0.41303453]\n",
      "Currrent Action : tensor([[-0.9470]])\n",
      "Next State : [[ 0.20659372]\n",
      " [-0.9784268 ]\n",
      " [-0.41303453]]\n",
      "Reward : [-1.82311378]\n",
      "--------------------------------------------------\n",
      "Step : 22\n",
      "Current State : [ 0.13533302 -0.99080014 -1.4468546 ]\n",
      "Currrent Action : tensor([[-2.1225]])\n",
      "Next State : [[ 0.13533302]\n",
      " [-0.99080014]\n",
      " [-1.4468546 ]]\n",
      "Reward : [-1.87802203]\n",
      "--------------------------------------------------\n",
      "Step : 23\n",
      "Current State : [ 0.03926259 -0.99922895 -1.9295375 ]\n",
      "Currrent Action : tensor([[1.7361]])\n",
      "Next State : [[ 0.03926259]\n",
      " [-0.99922895]\n",
      " [-1.9295375 ]]\n",
      "Reward : [-2.27171214]\n",
      "--------------------------------------------------\n",
      "Step : 24\n",
      "Current State : [-0.0983633  -0.99515057 -2.755906  ]\n",
      "Currrent Action : tensor([[-0.5130]])\n",
      "Next State : [[-0.0983633 ]\n",
      " [-0.99515057]\n",
      " [-2.755906  ]]\n",
      "Reward : [-2.71813931]\n",
      "--------------------------------------------------\n",
      "Step : 25\n",
      "Current State : [-0.2701761  -0.96281093 -3.5010667 ]\n",
      "Currrent Action : tensor([[0.0080]])\n",
      "Next State : [[-0.2701761 ]\n",
      " [-0.96281093]\n",
      " [-3.5010667 ]]\n",
      "Reward : [-3.54612758]\n",
      "--------------------------------------------------\n",
      "Step : 26\n",
      "Current State : [-0.45783767 -0.8890358  -4.0397105 ]\n",
      "Currrent Action : tensor([[1.2231]])\n",
      "Next State : [[-0.45783767]\n",
      " [-0.8890358 ]\n",
      " [-4.0397105 ]]\n",
      "Reward : [-4.62895175]\n",
      "--------------------------------------------------\n",
      "Step : 27\n",
      "Current State : [-0.6512486  -0.75886446 -4.6733427 ]\n",
      "Currrent Action : tensor([[0.2210]])\n",
      "Next State : [[-0.6512486 ]\n",
      " [-0.75886446]\n",
      " [-4.6733427 ]]\n",
      "Reward : [-5.81955499]\n",
      "--------------------------------------------------\n",
      "Step : 28\n",
      "Current State : [-0.8240476 -0.5665205 -5.185809 ]\n",
      "Currrent Action : tensor([[0.3779]])\n",
      "Next State : [[-0.8240476]\n",
      " [-0.5665205]\n",
      " [-5.185809 ]]\n",
      "Reward : [-7.38266961]\n",
      "--------------------------------------------------\n",
      "Step : 29\n",
      "Current State : [-0.9453923 -0.3259347 -5.4055405]\n",
      "Currrent Action : tensor([[1.3677]])\n",
      "Next State : [[-0.9453923]\n",
      " [-0.3259347]\n",
      " [-5.4055405]]\n",
      "Reward : [-9.1392552]\n",
      "--------------------------------------------------\n",
      "Step : 30\n",
      "Current State : [-0.99802405 -0.06283361 -5.3825045 ]\n",
      "Currrent Action : tensor([[1.7832]])\n",
      "Next State : [[-0.99802405]\n",
      " [-0.06283361]\n",
      " [-5.3825045 ]]\n",
      "Reward : [-10.81897603]\n",
      "--------------------------------------------------\n",
      "Step : 31\n",
      "Current State : [-0.98068666  0.19558531 -5.1945853 ]\n",
      "Currrent Action : tensor([[1.5670]])\n",
      "Next State : [[-0.98068666]\n",
      " [ 0.19558531]\n",
      " [-5.1945853 ]]\n",
      "Reward : [-12.37809309]\n",
      "--------------------------------------------------\n",
      "Step : 32\n",
      "Current State : [-0.9045085  0.4264557 -4.874328 ]\n",
      "Currrent Action : tensor([[1.1571]])\n",
      "Next State : [[-0.9045085]\n",
      " [ 0.4264557]\n",
      " [-4.874328 ]]\n",
      "Reward : [-11.37119499]\n",
      "--------------------------------------------------\n",
      "Step : 33\n",
      "Current State : [-0.78538126  0.61901236 -4.538278  ]\n",
      "Currrent Action : tensor([[0.1081]])\n",
      "Next State : [[-0.78538126]\n",
      " [ 0.61901236]\n",
      " [-4.538278  ]]\n",
      "Reward : [-9.67143908]\n",
      "--------------------------------------------------\n",
      "Step : 34\n",
      "Current State : [-0.64600545  0.7633328  -4.0194397 ]\n",
      "Currrent Action : tensor([[0.3639]])\n",
      "Next State : [[-0.64600545]\n",
      " [ 0.7633328 ]\n",
      " [-4.0194397 ]]\n",
      "Reward : [-8.18094017]\n",
      "--------------------------------------------------\n",
      "Step : 35\n",
      "Current State : [-0.5034994   0.86399555 -3.49391   ]\n",
      "Currrent Action : tensor([[-0.3131]])\n",
      "Next State : [[-0.5034994 ]\n",
      " [ 0.86399555]\n",
      " [-3.49391   ]]\n",
      "Reward : [-6.78283529]\n",
      "--------------------------------------------------\n",
      "Step : 36\n",
      "Current State : [-0.38439423  0.923169   -2.6618545 ]\n",
      "Currrent Action : tensor([[1.2271]])\n",
      "Next State : [[-0.38439423]\n",
      " [ 0.923169  ]\n",
      " [-2.6618545 ]]\n",
      "Reward : [-5.62569934]\n",
      "--------------------------------------------------\n",
      "Step : 37\n",
      "Current State : [-0.28120387  0.9596481  -2.190064  ]\n",
      "Currrent Action : tensor([[-1.4706]])\n",
      "Next State : [[-0.28120387]\n",
      " [ 0.9596481 ]\n",
      " [-2.190064  ]]\n",
      "Reward : [-4.57330175]\n",
      "--------------------------------------------------\n",
      "Step : 38\n",
      "Current State : [-0.21528429  0.9765514  -1.3613085 ]\n",
      "Currrent Action : tensor([[0.7268]])\n",
      "Next State : [[-0.21528429]\n",
      " [ 0.9765514 ]\n",
      " [-1.3613085 ]]\n",
      "Reward : [-3.92432569]\n",
      "--------------------------------------------------\n",
      "Step : 39\n",
      "Current State : [-0.19919676  0.9799595  -0.32889488]\n",
      "Currrent Action : tensor([[2.1118]])\n",
      "Next State : [[-0.19919676]\n",
      " [ 0.9799595 ]\n",
      " [-0.32889488]]\n",
      "Reward : [-3.38547074]\n",
      "--------------------------------------------------\n",
      "Step : 40\n",
      "Current State : [-0.23366168  0.97231793  0.7060747 ]\n",
      "Currrent Action : tensor([[2.6791]])\n",
      "Next State : [[-0.23366168]\n",
      " [ 0.97231793]\n",
      " [ 0.7060747 ]]\n",
      "Reward : [-3.15244313]\n",
      "--------------------------------------------------\n",
      "Step : 41\n",
      "Current State : [-0.2971418  0.9548334  1.3171183]\n",
      "Currrent Action : tensor([[-0.7880]])\n",
      "Next State : [[-0.2971418]\n",
      " [ 0.9548334]\n",
      " [ 1.3171183]]\n",
      "Reward : [-3.3144168]\n",
      "--------------------------------------------------\n",
      "Step : 42\n",
      "Current State : [-0.3927939   0.91962653  2.0393972 ]\n",
      "Currrent Action : tensor([[0.0410]])\n",
      "Next State : [[-0.3927939 ]\n",
      " [ 0.91962653]\n",
      " [ 2.0393972 ]]\n",
      "Reward : [-3.67971612]\n",
      "--------------------------------------------------\n",
      "Step : 43\n",
      "Current State : [-0.51455    0.8574604  2.7363007]\n",
      "Currrent Action : tensor([[0.0479]])\n",
      "Next State : [[-0.51455  ]\n",
      " [ 0.8574604]\n",
      " [ 2.7363007]]\n",
      "Reward : [-4.31442457]\n",
      "--------------------------------------------------\n",
      "Step : 44\n",
      "Current State : [-0.65274054  0.75758153  3.414272  ]\n",
      "Currrent Action : tensor([[0.2325]])\n",
      "Next State : [[-0.65274054]\n",
      " [ 0.75758153]\n",
      " [ 3.414272  ]]\n",
      "Reward : [-5.20628751]\n",
      "--------------------------------------------------\n",
      "Step : 45\n",
      "Current State : [-0.79078794  0.61209023  4.017978  ]\n",
      "Currrent Action : tensor([[0.2368]])\n",
      "Next State : [[-0.79078794]\n",
      " [ 0.61209023]\n",
      " [ 4.017978  ]]\n",
      "Reward : [-6.37327197]\n",
      "--------------------------------------------------\n",
      "Step : 46\n",
      "Current State : [-0.9005129   0.43482938  4.177046  ]\n",
      "Currrent Action : tensor([[-2.3400]])\n",
      "Next State : [[-0.9005129 ]\n",
      " [ 0.43482938]\n",
      " [ 4.177046  ]]\n",
      "Reward : [-7.78316532]\n",
      "--------------------------------------------------\n",
      "Step : 47\n",
      "Current State : [-0.9762537   0.21663038  4.629745  ]\n",
      "Currrent Action : tensor([[0.8438]])\n",
      "Next State : [[-0.9762537 ]\n",
      " [ 0.21663038]\n",
      " [ 4.629745  ]]\n",
      "Reward : [-8.99096816]\n",
      "--------------------------------------------------\n",
      "Step : 48\n",
      "Current State : [-0.99974823 -0.0224375   4.816019  ]\n",
      "Currrent Action : tensor([[0.1587]])\n",
      "Next State : [[-0.99974823]\n",
      " [-0.0224375 ]\n",
      " [ 4.816019  ]]\n",
      "Reward : [-10.68875893]\n",
      "--------------------------------------------------\n",
      "Step : 49\n",
      "Current State : [-0.9638109  -0.26658687  4.948212  ]\n",
      "Currrent Action : tensor([[0.9935]])\n",
      "Next State : [[-0.9638109 ]\n",
      " [-0.26658687]\n",
      " [ 4.948212  ]]\n",
      "Reward : [-12.0495079]\n",
      "--------------------------------------------------\n",
      "Step : 50\n",
      "Current State : [-0.8730588  -0.48761496  4.7901173 ]\n",
      "Currrent Action : tensor([[0.2790]])\n",
      "Next State : [[-0.8730588 ]\n",
      " [-0.48761496]\n",
      " [ 4.7901173 ]]\n",
      "Reward : [-10.69546405]\n",
      "--------------------------------------------------\n",
      "Step : 51\n",
      "Current State : [-0.73994863 -0.6726634   4.568929  ]\n",
      "Currrent Action : tensor([[0.9635]])\n",
      "Next State : [[-0.73994863]\n",
      " [-0.6726634 ]\n",
      " [ 4.568929  ]]\n",
      "Reward : [-9.22412116]\n",
      "--------------------------------------------------\n",
      "Step : 52\n",
      "Current State : [-0.59229094 -0.8057242   3.9818966 ]\n",
      "Currrent Action : tensor([[-0.5502]])\n",
      "Next State : [[-0.59229094]\n",
      " [-0.8057242 ]\n",
      " [ 3.9818966 ]]\n",
      "Reward : [-7.86602217]\n",
      "--------------------------------------------------\n",
      "Step : 53\n",
      "Current State : [-0.4469237 -0.8945721  3.4115133]\n",
      "Currrent Action : tensor([[0.2261]])\n",
      "Next State : [[-0.4469237]\n",
      " [-0.8945721]\n",
      " [ 3.4115133]]\n",
      "Reward : [-6.44628346]\n",
      "--------------------------------------------------\n",
      "Step : 54\n",
      "Current State : [-0.31903064 -0.94774437  2.77234   ]\n",
      "Currrent Action : tensor([[0.2117]])\n",
      "Next State : [[-0.31903064]\n",
      " [-0.94774437]\n",
      " [ 2.77234   ]]\n",
      "Reward : [-5.30153069]\n",
      "--------------------------------------------------\n",
      "Step : 55\n",
      "Current State : [-0.2226926  -0.97488874  2.0026193 ]\n",
      "Currrent Action : tensor([[-0.3927]])\n",
      "Next State : [[-0.2226926 ]\n",
      " [-0.97488874]\n",
      " [ 2.0026193 ]]\n",
      "Reward : [-4.36167218]\n",
      "--------------------------------------------------\n",
      "Step : 56\n",
      "Current State : [-0.1520876 -0.988367   1.4379091]\n",
      "Currrent Action : tensor([[1.1097]])\n",
      "Next State : [[-0.1520876]\n",
      " [-0.988367 ]\n",
      " [ 1.4379091]]\n",
      "Reward : [-3.62564004]\n",
      "--------------------------------------------------\n",
      "Step : 57\n",
      "Current State : [-0.11655902 -0.9931838   0.71711046]\n",
      "Currrent Action : tensor([[0.1365]])\n",
      "Next State : [[-0.11655902]\n",
      " [-0.9931838 ]\n",
      " [ 0.71711046]]\n",
      "Reward : [-3.17714789]\n",
      "--------------------------------------------------\n",
      "Step : 58\n",
      "Current State : [-0.11817909 -0.9929923  -0.03262692]\n",
      "Currrent Action : tensor([[-0.0323]])\n",
      "Next State : [[-0.11817909]\n",
      " [-0.9929923 ]\n",
      " [-0.03262692]]\n",
      "Reward : [-2.8994901]\n",
      "--------------------------------------------------\n",
      "Step : 59\n",
      "Current State : [-0.16682175 -0.98598707 -0.9829887 ]\n",
      "Currrent Action : tensor([[-1.3708]])\n",
      "Next State : [[-0.16682175]\n",
      " [-0.98598707]\n",
      " [-0.9829887 ]]\n",
      "Reward : [-2.85555866]\n",
      "--------------------------------------------------\n",
      "Step : 60\n",
      "Current State : [-0.26550657 -0.96410906 -2.022479  ]\n",
      "Currrent Action : tensor([[-3.0295]])\n",
      "Next State : [[-0.26550657]\n",
      " [-0.96410906]\n",
      " [-2.022479  ]]\n",
      "Reward : [-3.1226671]\n",
      "--------------------------------------------------\n",
      "Step : 61\n",
      "Current State : [-0.39024928 -0.92070925 -2.6434608 ]\n",
      "Currrent Action : tensor([[0.6807]])\n",
      "Next State : [[-0.39024928]\n",
      " [-0.92070925]\n",
      " [-2.6434608 ]]\n",
      "Reward : [-3.79336001]\n",
      "--------------------------------------------------\n",
      "Step : 62\n",
      "Current State : [-0.53075415 -0.84752584 -3.1717558 ]\n",
      "Currrent Action : tensor([[1.0816]])\n",
      "Next State : [[-0.53075415]\n",
      " [-0.84752584]\n",
      " [-3.1717558 ]]\n",
      "Reward : [-4.58755389]\n",
      "--------------------------------------------------\n",
      "Step : 63\n",
      "Current State : [-0.67670465 -0.7362546  -3.6757479 ]\n",
      "Currrent Action : tensor([[0.8777]])\n",
      "Next State : [[-0.67670465]\n",
      " [-0.7362546 ]\n",
      " [-3.6757479 ]]\n",
      "Reward : [-5.54489423]\n",
      "--------------------------------------------------\n",
      "Step : 64\n",
      "Current State : [-0.8073661 -0.5900509 -3.9279387]\n",
      "Currrent Action : tensor([[2.2760]])\n",
      "Next State : [[-0.8073661]\n",
      " [-0.5900509]\n",
      " [-3.9279387]]\n",
      "Reward : [-6.71004997]\n",
      "--------------------------------------------------\n",
      "Step : 65\n",
      "Current State : [-0.91456074 -0.40444854 -4.294922  ]\n",
      "Currrent Action : tensor([[0.5037]])\n",
      "Next State : [[-0.91456074]\n",
      " [-0.40444854]\n",
      " [-4.294922  ]]\n",
      "Reward : [-7.84558757]\n",
      "--------------------------------------------------\n",
      "Step : 66\n",
      "Current State : [-0.98533624 -0.17062394 -4.898258  ]\n",
      "Currrent Action : tensor([[-3.1230]])\n",
      "Next State : [[-0.98533624]\n",
      " [-0.17062394]\n",
      " [-4.898258  ]]\n",
      "Reward : [-9.27544243]\n",
      "--------------------------------------------------\n",
      "Step : 67\n",
      "Current State : [-0.9978981   0.06480301 -4.7262263 ]\n",
      "Currrent Action : tensor([[2.7976]])\n",
      "Next State : [[-0.9978981 ]\n",
      " [ 0.06480301]\n",
      " [-4.7262263 ]]\n",
      "Reward : [-11.2249644]\n",
      "--------------------------------------------------\n",
      "Step : 68\n",
      "Current State : [-0.9591099   0.28303385 -4.4421473 ]\n",
      "Currrent Action : tensor([[1.5698]])\n",
      "Next State : [[-0.9591099 ]\n",
      " [ 0.28303385]\n",
      " [-4.4421473 ]]\n",
      "Reward : [-11.70254073]\n",
      "--------------------------------------------------\n",
      "Step : 69\n",
      "Current State : [-0.87673694  0.48097017 -4.2961025 ]\n",
      "Currrent Action : tensor([[-0.4415]])\n",
      "Next State : [[-0.87673694]\n",
      " [ 0.48097017]\n",
      " [-4.2961025 ]]\n",
      "Reward : [-10.12241371]\n",
      "--------------------------------------------------\n",
      "Step : 70\n",
      "Current State : [-0.7671681  0.6414461 -3.892417 ]\n",
      "Currrent Action : tensor([[0.2864]])\n",
      "Next State : [[-0.7671681]\n",
      " [ 0.6414461]\n",
      " [-3.892417 ]]\n",
      "Reward : [-8.81444321]\n",
      "--------------------------------------------------\n",
      "Step : 71\n",
      "Current State : [-0.6486723   0.76106787 -3.3715193 ]\n",
      "Currrent Action : tensor([[0.2654]])\n",
      "Next State : [[-0.6486723 ]\n",
      " [ 0.76106787]\n",
      " [-3.3715193 ]]\n",
      "Reward : [-7.4942176]\n",
      "--------------------------------------------------\n",
      "Step : 72\n",
      "Current State : [-0.53890437  0.842367   -2.7340534 ]\n",
      "Currrent Action : tensor([[0.4444]])\n",
      "Next State : [[-0.53890437]\n",
      " [ 0.842367  ]\n",
      " [-2.7340534 ]]\n",
      "Reward : [-6.31997822]\n",
      "--------------------------------------------------\n",
      "Step : 73\n",
      "Current State : [-0.45274988  0.89163756 -1.9857777 ]\n",
      "Currrent Action : tensor([[0.7767]])\n",
      "Next State : [[-0.45274988]\n",
      " [ 0.89163756]\n",
      " [-1.9857777 ]]\n",
      "Reward : [-5.32741792]\n",
      "--------------------------------------------------\n",
      "Step : 74\n",
      "Current State : [-0.40130535  0.91594434 -1.1381088 ]\n",
      "Currrent Action : tensor([[1.1929]])\n",
      "Next State : [[-0.40130535]\n",
      " [ 0.91594434]\n",
      " [-1.1381088 ]]\n",
      "Reward : [-4.55997961]\n",
      "--------------------------------------------------\n",
      "Step : 75\n",
      "Current State : [-0.38785347  0.92172104 -0.29279852]\n",
      "Currrent Action : tensor([[1.0557]])\n",
      "Next State : [[-0.38785347]\n",
      " [ 0.92172104]\n",
      " [-0.29279852]]\n",
      "Reward : [-4.0658596]\n",
      "--------------------------------------------------\n",
      "Step : 76\n",
      "Current State : [-0.4130806   0.9106945   0.55065084]\n",
      "Currrent Action : tensor([[1.0144]])\n",
      "Next State : [[-0.4130806 ]\n",
      " [ 0.9106945 ]\n",
      " [ 0.55065084]]\n",
      "Reward : [-3.88694884]\n",
      "--------------------------------------------------\n",
      "Step : 77\n",
      "Current State : [-0.47343698  0.8808277   1.3470904 ]\n",
      "Currrent Action : tensor([[0.7561]])\n",
      "Next State : [[-0.47343698]\n",
      " [ 0.8808277 ]\n",
      " [ 1.3470904 ]]\n",
      "Reward : [-4.01742669]\n",
      "--------------------------------------------------\n",
      "Step : 78\n",
      "Current State : [-0.5597744   0.82864505  2.0184963 ]\n",
      "Currrent Action : tensor([[0.0719]])\n",
      "Next State : [[-0.5597744 ]\n",
      " [ 0.82864505]\n",
      " [ 2.0184963 ]]\n",
      "Reward : [-4.44150453]\n",
      "--------------------------------------------------\n",
      "Step : 79\n",
      "Current State : [-0.6617487   0.74972576  2.580706  ]\n",
      "Currrent Action : tensor([[-0.3952]])\n",
      "Next State : [[-0.6617487 ]\n",
      " [ 0.74972576]\n",
      " [ 2.580706  ]]\n",
      "Reward : [-5.09442349]\n",
      "--------------------------------------------------\n",
      "Step : 80\n",
      "Current State : [-0.76585764  0.64301014  2.984506  ]\n",
      "Currrent Action : tensor([[-1.0566]])\n",
      "Next State : [[-0.76585764]\n",
      " [ 0.64301014]\n",
      " [ 2.984506  ]]\n",
      "Reward : [-5.92930498]\n",
      "--------------------------------------------------\n",
      "Step : 81\n",
      "Current State : [-0.86559874  0.5007383   3.4794142 ]\n",
      "Currrent Action : tensor([[0.0843]])\n",
      "Next State : [[-0.86559874]\n",
      " [ 0.5007383 ]\n",
      " [ 3.4794142 ]]\n",
      "Reward : [-6.8598164]\n",
      "--------------------------------------------------\n",
      "Step : 82\n",
      "Current State : [-0.9460569   0.32400054  3.8899286 ]\n",
      "Currrent Action : tensor([[0.2331]])\n",
      "Next State : [[-0.9460569 ]\n",
      " [ 0.32400054]\n",
      " [ 3.8899286 ]]\n",
      "Reward : [-8.06011429]\n",
      "--------------------------------------------------\n",
      "Step : 83\n",
      "Current State : [-0.9923653   0.12333318  4.1261415 ]\n",
      "Currrent Action : tensor([[-0.0452]])\n",
      "Next State : [[-0.9923653 ]\n",
      " [ 0.12333318]\n",
      " [ 4.1261415 ]]\n",
      "Reward : [-9.41846223]\n",
      "--------------------------------------------------\n",
      "Step : 84\n",
      "Current State : [-0.9967869  -0.08009946  4.0766673 ]\n",
      "Currrent Action : tensor([[-0.9465]])\n",
      "Next State : [[-0.9967869 ]\n",
      " [-0.08009946]\n",
      " [ 4.0766673 ]]\n",
      "Reward : [-10.81139029]\n",
      "--------------------------------------------------\n",
      "Step : 85\n",
      "Current State : [-0.9587014  -0.28441453  4.1642065 ]\n",
      "Currrent Action : tensor([[0.9841]])\n",
      "Next State : [[-0.9587014 ]\n",
      " [-0.28441453]\n",
      " [ 4.1642065 ]]\n",
      "Reward : [-11.0351048]\n",
      "--------------------------------------------------\n",
      "Step : 86\n",
      "Current State : [-0.8858077  -0.46405253  3.88338   ]\n",
      "Currrent Action : tensor([[-0.4501]])\n",
      "Next State : [[-0.8858077 ]\n",
      " [-0.46405253]\n",
      " [ 3.88338   ]]\n",
      "Reward : [-9.87499704]\n",
      "--------------------------------------------------\n",
      "Step : 87\n",
      "Current State : [-0.7894673 -0.6137927  3.565822 ]\n",
      "Currrent Action : tensor([[0.2032]])\n",
      "Next State : [[-0.7894673]\n",
      " [-0.6137927]\n",
      " [ 3.565822 ]]\n",
      "Reward : [-8.57853507]\n",
      "--------------------------------------------------\n",
      "Step : 88\n",
      "Current State : [-0.6892258 -0.7245466  2.990412 ]\n",
      "Currrent Action : tensor([[-0.7671]])\n",
      "Next State : [[-0.6892258]\n",
      " [-0.7245466]\n",
      " [ 2.990412 ]]\n",
      "Reward : [-7.42615245]\n",
      "--------------------------------------------------\n",
      "Step : 89\n",
      "Current State : [-0.59844303 -0.8011654   2.3772733 ]\n",
      "Currrent Action : tensor([[-0.4649]])\n",
      "Next State : [[-0.59844303]\n",
      " [-0.8011654 ]\n",
      " [ 2.3772733 ]]\n",
      "Reward : [-6.32904176]\n",
      "--------------------------------------------------\n",
      "Step : 90\n",
      "Current State : [-0.53772485 -0.8431204   1.4763992 ]\n",
      "Currrent Action : tensor([[-3.1604]])\n",
      "Next State : [[-0.53772485]\n",
      " [-0.8431204 ]\n",
      " [ 1.4763992 ]]\n",
      "Reward : [-5.46364687]\n",
      "--------------------------------------------------\n",
      "Step : 91\n",
      "Current State : [-0.5057657 -0.8626709  0.7493401]\n",
      "Currrent Action : tensor([[-0.6315]])\n",
      "Next State : [[-0.5057657]\n",
      " [-0.8626709]\n",
      " [ 0.7493401]]\n",
      "Reward : [-4.79169609]\n",
      "--------------------------------------------------\n",
      "Step : 92\n",
      "Current State : [-0.5058171  -0.86264074 -0.00119169]\n",
      "Currrent Action : tensor([[-0.6902]])\n",
      "Next State : [[-0.5058171 ]\n",
      " [-0.86264074]\n",
      " [-0.00119169]]\n",
      "Reward : [-4.47110426]\n",
      "--------------------------------------------------\n",
      "Step : 93\n",
      "Current State : [-0.52218866 -0.85283    -0.38172752]\n",
      "Currrent Action : tensor([[1.7763]])\n",
      "Next State : [[-0.52218866]\n",
      " [-0.85283   ]\n",
      " [-0.38172752]]\n",
      "Reward : [-4.4178826]\n",
      "--------------------------------------------------\n",
      "Step : 94\n",
      "Current State : [-0.56686264 -0.8238123  -1.0655446 ]\n",
      "Currrent Action : tensor([[-0.2946]])\n",
      "Next State : [[-0.56686264]\n",
      " [-0.8238123 ]\n",
      " [-1.0655446 ]]\n",
      "Reward : [-4.50995563]\n",
      "--------------------------------------------------\n",
      "Step : 95\n",
      "Current State : [-0.62992674 -0.77665454 -1.5753279 ]\n",
      "Currrent Action : tensor([[0.7205]])\n",
      "Next State : [[-0.62992674]\n",
      " [-0.77665454]\n",
      " [-1.5753279 ]]\n",
      "Reward : [-4.83811138]\n",
      "--------------------------------------------------\n",
      "Step : 96\n",
      "Current State : [-0.70667785 -0.70753545 -2.0666566 ]\n",
      "Currrent Action : tensor([[0.6077]])\n",
      "Next State : [[-0.70667785]\n",
      " [-0.70753545]\n",
      " [-2.0666566 ]]\n",
      "Reward : [-5.32118874]\n",
      "--------------------------------------------------\n",
      "Step : 97\n",
      "Current State : [-0.78658307 -0.6174845  -2.4092789 ]\n",
      "Currrent Action : tensor([[1.2535]])\n",
      "Next State : [[-0.78658307]\n",
      " [-0.6174845 ]\n",
      " [-2.4092789 ]]\n",
      "Reward : [-5.97747337]\n",
      "--------------------------------------------------\n",
      "Step : 98\n",
      "Current State : [-0.86057484 -0.509324   -2.6228318 ]\n",
      "Currrent Action : tensor([[1.6637]])\n",
      "Next State : [[-0.86057484]\n",
      " [-0.509324  ]\n",
      " [-2.6228318 ]]\n",
      "Reward : [-6.71406393]\n",
      "--------------------------------------------------\n",
      "Step : 99\n",
      "Current State : [-0.9259165  -0.37772828 -2.9411497 ]\n",
      "Currrent Action : tensor([[0.4245]])\n",
      "Next State : [[-0.9259165 ]\n",
      " [-0.37772828]\n",
      " [-2.9411497 ]]\n",
      "Reward : [-7.48556321]\n",
      "learning iteration : 5\n",
      "--------------------------------------------------\n",
      "Step : 0\n",
      "Current State : [ 0.8666078  -0.49898994 -0.24849622]\n",
      "Currrent Action : tensor([[2.3188]])\n",
      "Next State : [[ 0.8666078 ]\n",
      " [-0.49898994]\n",
      " [-0.24849622]]\n",
      "Reward : [-0.26743365]\n",
      "--------------------------------------------------\n",
      "Step : 1\n",
      "Current State : [ 0.8538345 -0.5205446 -0.5011153]\n",
      "Currrent Action : tensor([[0.8108]])\n",
      "Next State : [[ 0.8538345]\n",
      " [-0.5205446]\n",
      " [-0.5011153]]\n",
      "Reward : [-0.27976856]\n",
      "--------------------------------------------------\n",
      "Step : 2\n",
      "Current State : [ 0.82304263 -0.56797963 -1.1312073 ]\n",
      "Currrent Action : tensor([[-1.5979]])\n",
      "Next State : [[ 0.82304263]\n",
      " [-0.56797963]\n",
      " [-1.1312073 ]]\n",
      "Reward : [-0.32740871]\n",
      "--------------------------------------------------\n",
      "Step : 3\n",
      "Current State : [ 0.7782348  -0.62797344 -1.497948  ]\n",
      "Currrent Action : tensor([[0.3950]])\n",
      "Next State : [[ 0.7782348 ]\n",
      " [-0.62797344]\n",
      " [-1.497948  ]]\n",
      "Reward : [-0.49299418]\n",
      "--------------------------------------------------\n",
      "Step : 4\n",
      "Current State : [ 0.7175724 -0.6964839 -1.8307881]\n",
      "Currrent Action : tensor([[0.9209]])\n",
      "Next State : [[ 0.7175724]\n",
      " [-0.6964839]\n",
      " [-1.8307881]]\n",
      "Reward : [-0.68620117]\n",
      "--------------------------------------------------\n",
      "Step : 5\n",
      "Current State : [ 0.6341207  -0.77323407 -2.2687945 ]\n",
      "Currrent Action : tensor([[0.5624]])\n",
      "Next State : [[ 0.6341207 ]\n",
      " [-0.77323407]\n",
      " [-2.2687945 ]]\n",
      "Reward : [-0.92914316]\n",
      "--------------------------------------------------\n",
      "Step : 6\n",
      "Current State : [ 0.52102387 -0.8535421  -2.7764146 ]\n",
      "Currrent Action : tensor([[0.4820]])\n",
      "Next State : [[ 0.52102387]\n",
      " [-0.8535421 ]\n",
      " [-2.7764146 ]]\n",
      "Reward : [-1.29629959]\n",
      "--------------------------------------------------\n",
      "Step : 7\n",
      "Current State : [ 0.36282066 -0.93185896 -3.5351405 ]\n",
      "Currrent Action : tensor([[-0.7905]])\n",
      "Next State : [[ 0.36282066]\n",
      " [-0.93185896]\n",
      " [-3.5351405 ]]\n",
      "Reward : [-1.81748263]\n",
      "--------------------------------------------------\n",
      "Step : 8\n",
      "Current State : [ 0.14964642 -0.98873955 -4.4216475 ]\n",
      "Currrent Action : tensor([[-1.2508]])\n",
      "Next State : [[ 0.14964642]\n",
      " [-0.98873955]\n",
      " [-4.4216475 ]]\n",
      "Reward : [-2.6900944]\n",
      "--------------------------------------------------\n",
      "Step : 9\n",
      "Current State : [-0.11168833 -0.9937433  -5.24265   ]\n",
      "Currrent Action : tensor([[-0.5297]])\n",
      "Next State : [[-0.11168833]\n",
      " [-0.9937433 ]\n",
      " [-5.24265   ]]\n",
      "Reward : [-3.97344081]\n",
      "--------------------------------------------------\n",
      "Step : 10\n",
      "Current State : [-0.39691955 -0.9178534  -5.9247284 ]\n",
      "Currrent Action : tensor([[0.4215]])\n",
      "Next State : [[-0.39691955]\n",
      " [-0.9178534 ]\n",
      " [-5.9247284 ]]\n",
      "Reward : [-5.58025618]\n",
      "--------------------------------------------------\n",
      "Step : 11\n",
      "Current State : [-0.6666851 -0.7453395 -6.4318824]\n",
      "Currrent Action : tensor([[1.2082]])\n",
      "Next State : [[-0.6666851]\n",
      " [-0.7453395]\n",
      " [-6.4318824]]\n",
      "Reward : [-7.42796165]\n",
      "--------------------------------------------------\n",
      "Step : 12\n",
      "Current State : [-0.8778272  -0.47897744 -6.831089  ]\n",
      "Currrent Action : tensor([[1.0653]])\n",
      "Next State : [[-0.8778272 ]\n",
      " [-0.47897744]\n",
      " [-6.831089  ]]\n",
      "Reward : [-9.43057047]\n",
      "--------------------------------------------------\n",
      "Step : 13\n",
      "Current State : [-0.989038   -0.14766131 -7.025725  ]\n",
      "Currrent Action : tensor([[1.0973]])\n",
      "Next State : [[-0.989038  ]\n",
      " [-0.14766131]\n",
      " [-7.025725  ]]\n",
      "Reward : [-11.64829124]\n",
      "--------------------------------------------------\n",
      "Step : 14\n",
      "Current State : [-0.9810151  0.1939313 -6.867424 ]\n",
      "Currrent Action : tensor([[1.7936]])\n",
      "Next State : [[-0.9810151]\n",
      " [ 0.1939313]\n",
      " [-6.867424 ]]\n",
      "Reward : [-13.89967811]\n",
      "--------------------------------------------------\n",
      "Step : 15\n",
      "Current State : [-0.8667031  0.4988244 -6.5414777]\n",
      "Currrent Action : tensor([[1.2033]])\n",
      "Next State : [[-0.8667031]\n",
      " [ 0.4988244]\n",
      " [-6.5414777]]\n",
      "Reward : [-13.39901764]\n",
      "--------------------------------------------------\n",
      "Step : 16\n",
      "Current State : [-0.675547   0.7373169 -6.136963 ]\n",
      "Currrent Action : tensor([[0.2026]])\n",
      "Next State : [[-0.675547 ]\n",
      " [ 0.7373169]\n",
      " [-6.136963 ]]\n",
      "Reward : [-11.1401329]\n",
      "--------------------------------------------------\n",
      "Step : 17\n",
      "Current State : [-0.4506309  0.8927104 -5.484688 ]\n",
      "Currrent Action : tensor([[0.6619]])\n",
      "Next State : [[-0.4506309]\n",
      " [ 0.8927104]\n",
      " [-5.484688 ]]\n",
      "Reward : [-9.11433811]\n",
      "--------------------------------------------------\n",
      "Step : 18\n",
      "Current State : [-0.21764764  0.9760274  -4.961365  ]\n",
      "Currrent Action : tensor([[-0.9747]])\n",
      "Next State : [[-0.21764764]\n",
      " [ 0.9760274 ]\n",
      " [-4.961365  ]]\n",
      "Reward : [-7.16366787]\n",
      "--------------------------------------------------\n",
      "Step : 19\n",
      "Current State : [-0.01106839  0.9999387  -4.1667013 ]\n",
      "Currrent Action : tensor([[0.4176]])\n",
      "Next State : [[-0.01106839]\n",
      " [ 0.9999387 ]\n",
      " [-4.1667013 ]]\n",
      "Reward : [-5.66650491]\n",
      "--------------------------------------------------\n",
      "Step : 20\n",
      "Current State : [ 0.15548837  0.98783773 -3.343809  ]\n",
      "Currrent Action : tensor([[0.4863]])\n",
      "Next State : [[ 0.15548837]\n",
      " [ 0.98783773]\n",
      " [-3.343809  ]]\n",
      "Reward : [-4.23867307]\n",
      "--------------------------------------------------\n",
      "Step : 21\n",
      "Current State : [ 0.2759706  0.9611661 -2.4695513]\n",
      "Currrent Action : tensor([[0.8892]])\n",
      "Next State : [[ 0.2759706]\n",
      " [ 0.9611661]\n",
      " [-2.4695513]]\n",
      "Reward : [-3.12020038]\n",
      "--------------------------------------------------\n",
      "Step : 22\n",
      "Current State : [ 0.3521947   0.93592674 -1.6063124 ]\n",
      "Currrent Action : tensor([[0.9491]])\n",
      "Next State : [[ 0.3521947 ]\n",
      " [ 0.93592674]\n",
      " [-1.6063124 ]]\n",
      "Reward : [-2.27795872]\n",
      "--------------------------------------------------\n",
      "Step : 23\n",
      "Current State : [ 0.38698775  0.92208487 -0.74895126]\n",
      "Currrent Action : tensor([[1.0361]])\n",
      "Next State : [[ 0.38698775]\n",
      " [ 0.92208487]\n",
      " [-0.74895126]]\n",
      "Reward : [-1.72533101]\n",
      "--------------------------------------------------\n",
      "Step : 24\n",
      "Current State : [0.3859495  0.9225199  0.02251476]\n",
      "Currrent Action : tensor([[0.5327]])\n",
      "Next State : [[0.3859495 ]\n",
      " [0.9225199 ]\n",
      " [0.02251476]]\n",
      "Reward : [-1.43332332]\n",
      "--------------------------------------------------\n",
      "Step : 25\n",
      "Current State : [0.35494676 0.9348865  0.66759425]\n",
      "Currrent Action : tensor([[-0.3121]])\n",
      "Next State : [[0.35494676]\n",
      " [0.9348865 ]\n",
      " [0.66759425]]\n",
      "Reward : [-1.37973808]\n",
      "--------------------------------------------------\n",
      "Step : 26\n",
      "Current State : [0.29122433 0.9566548  1.3470147 ]\n",
      "Currrent Action : tensor([[-0.1450]])\n",
      "Next State : [[0.29122433]\n",
      " [0.9566548 ]\n",
      " [1.3470147 ]]\n",
      "Reward : [-1.50370634]\n",
      "--------------------------------------------------\n",
      "Step : 27\n",
      "Current State : [0.19105616 0.9815791  2.0653672 ]\n",
      "Currrent Action : tensor([[0.0057]])\n",
      "Next State : [[0.19105616]\n",
      " [0.9815791 ]\n",
      " [2.0653672 ]]\n",
      "Reward : [-1.80780933]\n",
      "--------------------------------------------------\n",
      "Step : 28\n",
      "Current State : [0.05075175 0.9987113  2.8292892 ]\n",
      "Currrent Action : tensor([[0.1849]])\n",
      "Next State : [[0.05075175]\n",
      " [0.9987113 ]\n",
      " [2.8292892 ]]\n",
      "Reward : [-2.3270314]\n",
      "--------------------------------------------------\n",
      "Step : 29\n",
      "Current State : [-0.1147185  0.993398   3.314904 ]\n",
      "Currrent Action : tensor([[-1.7561]])\n",
      "Next State : [[-0.1147185]\n",
      " [ 0.993398 ]\n",
      " [ 3.314904 ]]\n",
      "Reward : [-3.11404086]\n",
      "--------------------------------------------------\n",
      "Step : 30\n",
      "Current State : [-0.3166036  0.9485579  4.143502 ]\n",
      "Currrent Action : tensor([[0.5570]])\n",
      "Next State : [[-0.3166036]\n",
      " [ 0.9485579]\n",
      " [ 4.143502 ]]\n",
      "Reward : [-3.94098269]\n",
      "--------------------------------------------------\n",
      "Step : 31\n",
      "Current State : [-0.5357981   0.84434617  4.866118  ]\n",
      "Currrent Action : tensor([[0.0746]])\n",
      "Next State : [[-0.5357981 ]\n",
      " [ 0.84434617]\n",
      " [ 4.866118  ]]\n",
      "Reward : [-5.30010003]\n",
      "--------------------------------------------------\n",
      "Step : 32\n",
      "Current State : [-0.7450551  0.6670029  5.5032954]\n",
      "Currrent Action : tensor([[0.0261]])\n",
      "Next State : [[-0.7450551]\n",
      " [ 0.6670029]\n",
      " [ 5.5032954]]\n",
      "Reward : [-6.93147077]\n",
      "--------------------------------------------------\n",
      "Step : 33\n",
      "Current State : [-0.9115277   0.41123876  6.1273255 ]\n",
      "Currrent Action : tensor([[0.8252]])\n",
      "Next State : [[-0.9115277 ]\n",
      " [ 0.41123876]\n",
      " [ 6.1273255 ]]\n",
      "Reward : [-8.84422342]\n",
      "--------------------------------------------------\n",
      "Step : 34\n",
      "Current State : [-0.99388826  0.11039086  6.2639265 ]\n",
      "Currrent Action : tensor([[-1.1455]])\n",
      "Next State : [[-0.99388826]\n",
      " [ 0.11039086]\n",
      " [ 6.2639265 ]]\n",
      "Reward : [-11.14205205]\n",
      "--------------------------------------------------\n",
      "Step : 35\n",
      "Current State : [-0.97964096 -0.20075744  6.254947  ]\n",
      "Currrent Action : tensor([[-0.6118]])\n",
      "Next State : [[-0.97964096]\n",
      " [-0.20075744]\n",
      " [ 6.254947  ]]\n",
      "Reward : [-13.11086962]\n",
      "--------------------------------------------------\n",
      "Step : 36\n",
      "Current State : [-0.8759268  -0.48244402  6.026235  ]\n",
      "Currrent Action : tensor([[-0.5210]])\n",
      "Next State : [[-0.8759268 ]\n",
      " [-0.48244402]\n",
      " [ 6.026235  ]]\n",
      "Reward : [-12.55314223]\n",
      "--------------------------------------------------\n",
      "Step : 37\n",
      "Current State : [-0.7044537  -0.70974994  5.714007  ]\n",
      "Currrent Action : tensor([[0.3307]])\n",
      "Next State : [[-0.7044537 ]\n",
      " [-0.70974994]\n",
      " [ 5.714007  ]]\n",
      "Reward : [-10.59149489]\n",
      "--------------------------------------------------\n",
      "Step : 38\n",
      "Current State : [-0.499328   -0.86641306  5.1766043 ]\n",
      "Currrent Action : tensor([[-0.0339]])\n",
      "Next State : [[-0.499328  ]\n",
      " [-0.86641306]\n",
      " [ 5.1766043 ]]\n",
      "Reward : [-8.7990074]\n",
      "--------------------------------------------------\n",
      "Step : 39\n",
      "Current State : [-0.29745305 -0.9547365   4.4159822 ]\n",
      "Currrent Action : tensor([[-0.7387]])\n",
      "Next State : [[-0.29745305]\n",
      " [-0.9547365 ]\n",
      " [ 4.4159822 ]]\n",
      "Reward : [-7.06351078]\n",
      "--------------------------------------------------\n",
      "Step : 40\n",
      "Current State : [-0.12324658 -0.9923761   3.5692618 ]\n",
      "Currrent Action : tensor([[-0.8711]])\n",
      "Next State : [[-0.12324658]\n",
      " [-0.9923761 ]\n",
      " [ 3.5692618 ]]\n",
      "Reward : [-5.4583043]\n",
      "--------------------------------------------------\n",
      "Step : 41\n",
      "Current State : [ 0.01891683 -0.99982107  2.849574  ]\n",
      "Currrent Action : tensor([[0.1640]])\n",
      "Next State : [[ 0.01891683]\n",
      " [-0.99982107]\n",
      " [ 2.849574  ]]\n",
      "Reward : [-4.14483577]\n",
      "--------------------------------------------------\n",
      "Step : 42\n",
      "Current State : [ 0.11175805 -0.99373543  1.861481  ]\n",
      "Currrent Action : tensor([[-1.5882]])\n",
      "Next State : [[ 0.11175805]\n",
      " [-0.99373543]\n",
      " [ 1.861481  ]]\n",
      "Reward : [-3.22285606]\n",
      "--------------------------------------------------\n",
      "Step : 43\n",
      "Current State : [ 0.1704313  -0.98536956  1.1855072 ]\n",
      "Currrent Action : tensor([[0.4622]])\n",
      "Next State : [[ 0.1704313 ]\n",
      " [-0.98536956]\n",
      " [ 1.1855072 ]]\n",
      "Reward : [-2.47483478]\n",
      "--------------------------------------------------\n",
      "Step : 44\n",
      "Current State : [ 0.18975887 -0.9818307   0.39298365]\n",
      "Currrent Action : tensor([[-0.3566]])\n",
      "Next State : [[ 0.18975887]\n",
      " [-0.9818307 ]\n",
      " [ 0.39298365]]\n",
      "Reward : [-2.09935124]\n",
      "--------------------------------------------------\n",
      "Step : 45\n",
      "Current State : [ 0.16927071 -0.9855696  -0.41653773]\n",
      "Currrent Action : tensor([[-0.4877]])\n",
      "Next State : [[ 0.16927071]\n",
      " [-0.9855696 ]\n",
      " [-0.41653773]]\n",
      "Reward : [-1.91974964]\n",
      "--------------------------------------------------\n",
      "Step : 46\n",
      "Current State : [ 0.11127703 -0.99378943 -1.1716338 ]\n",
      "Currrent Action : tensor([[-0.1061]])\n",
      "Next State : [[ 0.11127703]\n",
      " [-0.99378943]\n",
      " [-1.1716338 ]]\n",
      "Reward : [-1.97934081]\n",
      "--------------------------------------------------\n",
      "Step : 47\n",
      "Current State : [ 6.5917295e-04 -9.9999976e-01 -2.2169759e+00]\n",
      "Currrent Action : tensor([[-2.1990]])\n",
      "Next State : [[ 6.5917295e-04]\n",
      " [-9.9999976e-01]\n",
      " [-2.2169759e+00]]\n",
      "Reward : [-2.2707951]\n",
      "--------------------------------------------------\n",
      "Step : 48\n",
      "Current State : [-0.14288408 -0.9897394  -2.8806791 ]\n",
      "Currrent Action : tensor([[0.5753]])\n",
      "Next State : [[-0.14288408]\n",
      " [-0.9897394 ]\n",
      " [-2.8806791 ]]\n",
      "Reward : [-2.95715987]\n",
      "--------------------------------------------------\n",
      "Step : 49\n",
      "Current State : [-0.32700145 -0.94502383 -3.795081  ]\n",
      "Currrent Action : tensor([[-1.1473]])\n",
      "Next State : [[-0.32700145]\n",
      " [-0.94502383]\n",
      " [-3.795081  ]]\n",
      "Reward : [-3.76953009]\n",
      "--------------------------------------------------\n",
      "Step : 50\n",
      "Current State : [-0.5327198 -0.8462917 -4.5736456]\n",
      "Currrent Action : tensor([[-0.4653]])\n",
      "Next State : [[-0.5327198]\n",
      " [-0.8462917]\n",
      " [-4.5736456]]\n",
      "Reward : [-5.06541141]\n",
      "--------------------------------------------------\n",
      "Step : 51\n",
      "Current State : [-0.7277877 -0.6858026 -5.0655875]\n",
      "Currrent Action : tensor([[0.9518]])\n",
      "Next State : [[-0.7277877]\n",
      " [-0.6858026]\n",
      " [-5.0655875]]\n",
      "Reward : [-6.64074366]\n",
      "--------------------------------------------------\n",
      "Step : 52\n",
      "Current State : [-0.8880054 -0.4598331 -5.5579724]\n",
      "Currrent Action : tensor([[0.1464]])\n",
      "Next State : [[-0.8880054]\n",
      " [-0.4598331]\n",
      " [-5.5579724]]\n",
      "Reward : [-8.25849489]\n",
      "--------------------------------------------------\n",
      "Step : 53\n",
      "Current State : [-0.98303515 -0.18341732 -5.866912  ]\n",
      "Currrent Action : tensor([[0.2396]])\n",
      "Next State : [[-0.98303515]\n",
      " [-0.18341732]\n",
      " [-5.866912  ]]\n",
      "Reward : [-10.18491612]\n",
      "--------------------------------------------------\n",
      "Step : 54\n",
      "Current State : [-0.9940615   0.10881921 -5.869935  ]\n",
      "Currrent Action : tensor([[0.8969]])\n",
      "Next State : [[-0.9940615 ]\n",
      " [ 0.10881921]\n",
      " [-5.869935  ]]\n",
      "Reward : [-12.18749409]\n",
      "--------------------------------------------------\n",
      "Step : 55\n",
      "Current State : [-0.927379    0.37412322 -5.488321  ]\n",
      "Currrent Action : tensor([[2.4987]])\n",
      "Next State : [[-0.927379  ]\n",
      " [ 0.37412322]\n",
      " [-5.488321  ]]\n",
      "Reward : [-12.6460189]\n",
      "--------------------------------------------------\n",
      "Step : 56\n",
      "Current State : [-0.8062426   0.59158504 -4.991445  ]\n",
      "Currrent Action : tensor([[1.4419]])\n",
      "Next State : [[-0.8062426 ]\n",
      " [ 0.59158504]\n",
      " [-4.991445  ]]\n",
      "Reward : [-10.62158989]\n",
      "--------------------------------------------------\n",
      "Step : 57\n",
      "Current State : [-0.65665746  0.75418895 -4.427894  ]\n",
      "Currrent Action : tensor([[0.7991]])\n",
      "Next State : [[-0.65665746]\n",
      " [ 0.75418895]\n",
      " [-4.427894  ]]\n",
      "Reward : [-8.78501061]\n",
      "--------------------------------------------------\n",
      "Step : 58\n",
      "Current State : [-0.5086764   0.86095774 -3.6546285 ]\n",
      "Currrent Action : tensor([[1.3842]])\n",
      "Next State : [[-0.5086764 ]\n",
      " [ 0.86095774]\n",
      " [-3.6546285 ]]\n",
      "Reward : [-7.19370792]\n",
      "--------------------------------------------------\n",
      "Step : 59\n",
      "Current State : [-0.3656017  0.9307714 -3.1873527]\n",
      "Currrent Action : tensor([[-1.1896]])\n",
      "Next State : [[-0.3656017]\n",
      " [ 0.9307714]\n",
      " [-3.1873527]]\n",
      "Reward : [-5.76572691]\n",
      "--------------------------------------------------\n",
      "Step : 60\n",
      "Current State : [-0.251335    0.96790016 -2.4043972 ]\n",
      "Currrent Action : tensor([[0.5658]])\n",
      "Next State : [[-0.251335  ]\n",
      " [ 0.96790016]\n",
      " [-2.4043972 ]]\n",
      "Reward : [-4.79956043]\n",
      "--------------------------------------------------\n",
      "Step : 61\n",
      "Current State : [-0.16850509  0.9857008  -1.6949283 ]\n",
      "Currrent Action : tensor([[-0.1097]])\n",
      "Next State : [[-0.16850509]\n",
      " [ 0.9857008 ]\n",
      " [-1.6949283 ]]\n",
      "Reward : [-3.90822268]\n",
      "--------------------------------------------------\n",
      "Step : 62\n",
      "Current State : [-0.12554702  0.99208766 -0.8686735 ]\n",
      "Currrent Action : tensor([[0.5799]])\n",
      "Next State : [[-0.12554702]\n",
      " [ 0.99208766]\n",
      " [-0.8686735 ]]\n",
      "Reward : [-3.31559445]\n",
      "--------------------------------------------------\n",
      "Step : 63\n",
      "Current State : [-0.11781243  0.99303585 -0.15585007]\n",
      "Currrent Action : tensor([[-0.2083]])\n",
      "Next State : [[-0.11781243]\n",
      " [ 0.99303585]\n",
      " [-0.15585007]]\n",
      "Reward : [-2.95421056]\n",
      "--------------------------------------------------\n",
      "Step : 64\n",
      "Current State : [-0.15553348  0.98783064  0.76161575]\n",
      "Currrent Action : tensor([[1.1513]])\n",
      "Next State : [[-0.15553348]\n",
      " [ 0.98783064]\n",
      " [ 0.76161575]]\n",
      "Reward : [-2.85608016]\n",
      "--------------------------------------------------\n",
      "Step : 65\n",
      "Current State : [-0.24380945  0.9698232   1.8024887 ]\n",
      "Currrent Action : tensor([[2.3976]])\n",
      "Next State : [[-0.24380945]\n",
      " [ 0.9698232 ]\n",
      " [ 1.8024887 ]]\n",
      "Reward : [-3.04440982]\n",
      "--------------------------------------------------\n",
      "Step : 66\n",
      "Current State : [-0.3731025  0.9277901  2.7211766]\n",
      "Currrent Action : tensor([[1.2755]])\n",
      "Next State : [[-0.3731025]\n",
      " [ 0.9277901]\n",
      " [ 2.7211766]]\n",
      "Reward : [-3.62833304]\n",
      "--------------------------------------------------\n",
      "Step : 67\n",
      "Current State : [-0.521117   0.8534853  3.3161676]\n",
      "Currrent Action : tensor([[-0.6723]])\n",
      "Next State : [[-0.521117 ]\n",
      " [ 0.8534853]\n",
      " [ 3.3161676]]\n",
      "Reward : [-4.55571576]\n",
      "--------------------------------------------------\n",
      "Step : 68\n",
      "Current State : [-0.6710373  0.7414236  3.748957 ]\n",
      "Currrent Action : tensor([[-1.3822]])\n",
      "Next State : [[-0.6710373]\n",
      " [ 0.7414236]\n",
      " [ 3.748957 ]]\n",
      "Reward : [-5.5915793]\n",
      "--------------------------------------------------\n",
      "Step : 69\n",
      "Current State : [-0.81721395  0.5763344   4.419065  ]\n",
      "Currrent Action : tensor([[0.7603]])\n",
      "Next State : [[-0.81721395]\n",
      " [ 0.5763344 ]\n",
      " [ 4.419065  ]]\n",
      "Reward : [-6.72554202]\n",
      "--------------------------------------------------\n",
      "Step : 70\n",
      "Current State : [-0.92961496  0.36853227  4.7361283 ]\n",
      "Currrent Action : tensor([[-0.7679]])\n",
      "Next State : [[-0.92961496]\n",
      " [ 0.36853227]\n",
      " [ 4.7361283 ]]\n",
      "Reward : [-8.34093462]\n",
      "--------------------------------------------------\n",
      "Step : 71\n",
      "Current State : [-0.99303955  0.11778123  5.1874886 ]\n",
      "Currrent Action : tensor([[1.1664]])\n",
      "Next State : [[-0.99303955]\n",
      " [ 0.11778123]\n",
      " [ 5.1874886 ]]\n",
      "Reward : [-9.88504853]\n",
      "--------------------------------------------------\n",
      "Step : 72\n",
      "Current State : [-0.99062425 -0.13661468  5.10197   ]\n",
      "Currrent Action : tensor([[-1.1590]])\n",
      "Next State : [[-0.99062425]\n",
      " [-0.13661468]\n",
      " [ 5.10197   ]]\n",
      "Reward : [-11.83412523]\n",
      "--------------------------------------------------\n",
      "Step : 73\n",
      "Current State : [-0.92625356 -0.37690103  4.9881015 ]\n",
      "Currrent Action : tensor([[-0.0761]])\n",
      "Next State : [[-0.92625356]\n",
      " [-0.37690103]\n",
      " [ 4.9881015 ]]\n",
      "Reward : [-11.63033286]\n",
      "--------------------------------------------------\n",
      "Step : 74\n",
      "Current State : [-0.82152283 -0.5701756   4.4054255 ]\n",
      "Currrent Action : tensor([[-2.2891]])\n",
      "Next State : [[-0.82152283]\n",
      " [-0.5701756 ]\n",
      " [ 4.4054255 ]]\n",
      "Reward : [-10.08293583]\n",
      "--------------------------------------------------\n",
      "Step : 75\n",
      "Current State : [-0.69521976 -0.71879727  3.9070232 ]\n",
      "Currrent Action : tensor([[-0.4718]])\n",
      "Next State : [[-0.69521976]\n",
      " [-0.71879727]\n",
      " [ 3.9070232 ]]\n",
      "Reward : [-8.36658149]\n",
      "--------------------------------------------------\n",
      "Step : 76\n",
      "Current State : [-0.56158304 -0.8274204   3.4485564 ]\n",
      "Currrent Action : tensor([[0.5375]])\n",
      "Next State : [[-0.56158304]\n",
      " [-0.8274204 ]\n",
      " [ 3.4485564 ]]\n",
      "Reward : [-7.00013471]\n",
      "--------------------------------------------------\n",
      "Step : 77\n",
      "Current State : [-0.43862516 -0.89867014  2.8445904 ]\n",
      "Currrent Action : tensor([[0.1107]])\n",
      "Next State : [[-0.43862516]\n",
      " [-0.89867014]\n",
      " [ 2.8445904 ]]\n",
      "Reward : [-5.88556325]\n",
      "--------------------------------------------------\n",
      "Step : 78\n",
      "Current State : [-0.33113056 -0.9435849   2.331335  ]\n",
      "Currrent Action : tensor([[1.0716]])\n",
      "Next State : [[-0.33113056]\n",
      " [-0.9435849 ]\n",
      " [ 2.331335  ]]\n",
      "Reward : [-4.91039441]\n",
      "--------------------------------------------------\n",
      "Step : 79\n",
      "Current State : [-0.24332823 -0.969944    1.834115  ]\n",
      "Currrent Action : tensor([[1.4031]])\n",
      "Next State : [[-0.24332823]\n",
      " [-0.969944  ]\n",
      " [ 1.834115  ]]\n",
      "Reward : [-4.18708165]\n",
      "--------------------------------------------------\n",
      "Step : 80\n",
      "Current State : [-0.19429563 -0.980943    1.005128  ]\n",
      "Currrent Action : tensor([[-0.6769]])\n",
      "Next State : [[-0.19429563]\n",
      " [-0.980943  ]\n",
      " [ 1.005128  ]]\n",
      "Reward : [-3.63686269]\n",
      "--------------------------------------------------\n",
      "Step : 81\n",
      "Current State : [-0.1706793  -0.98532665  0.480406  ]\n",
      "Currrent Action : tensor([[1.4066]])\n",
      "Next State : [[-0.1706793 ]\n",
      " [-0.98532665]\n",
      " [ 0.480406  ]]\n",
      "Reward : [-3.22294836]\n",
      "--------------------------------------------------\n",
      "Step : 82\n",
      "Current State : [-0.18022598 -0.98362523 -0.19394311]\n",
      "Currrent Action : tensor([[0.4310]])\n",
      "Next State : [[-0.18022598]\n",
      " [-0.98362523]\n",
      " [-0.19394311]]\n",
      "Reward : [-3.05892757]\n",
      "--------------------------------------------------\n",
      "Step : 83\n",
      "Current State : [-0.22006616 -0.97548497 -0.81332177]\n",
      "Currrent Action : tensor([[0.7889]])\n",
      "Next State : [[-0.22006616]\n",
      " [-0.97548497]\n",
      " [-0.81332177]]\n",
      "Reward : [-3.0739317]\n",
      "--------------------------------------------------\n",
      "Step : 84\n",
      "Current State : [-0.29035607 -0.95691866 -1.4543327 ]\n",
      "Currrent Action : tensor([[0.6040]])\n",
      "Next State : [[-0.29035607]\n",
      " [-0.95691866]\n",
      " [-1.4543327 ]]\n",
      "Reward : [-3.28021068]\n",
      "--------------------------------------------------\n",
      "Step : 85\n",
      "Current State : [-0.40611625 -0.9138214  -2.4720218 ]\n",
      "Currrent Action : tensor([[-2.5202]])\n",
      "Next State : [[-0.40611625]\n",
      " [-0.9138214 ]\n",
      " [-2.4720218 ]]\n",
      "Reward : [-3.69520782]\n",
      "--------------------------------------------------\n",
      "Step : 86\n",
      "Current State : [-0.5442772  -0.83890545 -3.1465454 ]\n",
      "Currrent Action : tensor([[0.0723]])\n",
      "Next State : [[-0.5442772 ]\n",
      " [-0.83890545]\n",
      " [-3.1465454 ]]\n",
      "Reward : [-4.56720083]\n",
      "--------------------------------------------------\n",
      "Step : 87\n",
      "Current State : [-0.68454266 -0.7289728  -3.5689774 ]\n",
      "Currrent Action : tensor([[1.3783]])\n",
      "Next State : [[-0.68454266]\n",
      " [-0.7289728 ]\n",
      " [-3.5689774 ]]\n",
      "Reward : [-5.59867957]\n",
      "--------------------------------------------------\n",
      "Step : 88\n",
      "Current State : [-0.8183048 -0.5747846 -4.0895853]\n",
      "Currrent Action : tensor([[0.1741]])\n",
      "Next State : [[-0.8183048]\n",
      " [-0.5747846]\n",
      " [-4.0895853]]\n",
      "Reward : [-6.67835743]\n",
      "--------------------------------------------------\n",
      "Step : 89\n",
      "Current State : [-0.9227805  -0.38532606 -4.335595  ]\n",
      "Currrent Action : tensor([[1.2339]])\n",
      "Next State : [[-0.9227805 ]\n",
      " [-0.38532606]\n",
      " [-4.335595  ]]\n",
      "Reward : [-8.07110776]\n",
      "--------------------------------------------------\n",
      "Step : 90\n",
      "Current State : [-0.9876211 -0.1568583 -4.761048 ]\n",
      "Currrent Action : tensor([[-0.9097]])\n",
      "Next State : [[-0.9876211]\n",
      " [-0.1568583]\n",
      " [-4.761048 ]]\n",
      "Reward : [-9.42125512]\n",
      "--------------------------------------------------\n",
      "Step : 91\n",
      "Current State : [-0.9955001   0.09476088 -5.0482407 ]\n",
      "Currrent Action : tensor([[-1.1303]])\n",
      "Next State : [[-0.9955001 ]\n",
      " [ 0.09476088]\n",
      " [-5.0482407 ]]\n",
      "Reward : [-11.17279174]\n",
      "--------------------------------------------------\n",
      "Step : 92\n",
      "Current State : [-0.93992084  0.34139252 -5.0698953 ]\n",
      "Currrent Action : tensor([[-0.6182]])\n",
      "Next State : [[-0.93992084]\n",
      " [ 0.34139252]\n",
      " [-5.0698953 ]]\n",
      "Reward : [-11.83117179]\n",
      "--------------------------------------------------\n",
      "Step : 93\n",
      "Current State : [-0.83479184  0.5505657  -4.692874  ]\n",
      "Currrent Action : tensor([[0.8065]])\n",
      "Next State : [[-0.83479184]\n",
      " [ 0.5505657 ]\n",
      " [-4.692874  ]]\n",
      "Reward : [-10.37297044]\n",
      "--------------------------------------------------\n",
      "Step : 94\n",
      "Current State : [-0.69957364  0.7145605  -4.2590785 ]\n",
      "Currrent Action : tensor([[0.1391]])\n",
      "Next State : [[-0.69957364]\n",
      " [ 0.7145605 ]\n",
      " [-4.2590785 ]]\n",
      "Reward : [-8.74850889]\n",
      "--------------------------------------------------\n",
      "Step : 95\n",
      "Current State : [-0.54862833  0.83606637 -3.8815591 ]\n",
      "Currrent Action : tensor([[-1.0560]])\n",
      "Next State : [[-0.54862833]\n",
      " [ 0.83606637]\n",
      " [-3.8815591 ]]\n",
      "Reward : [-7.31691525]\n",
      "--------------------------------------------------\n",
      "Step : 96\n",
      "Current State : [-0.406509    0.91364676 -3.241857  ]\n",
      "Currrent Action : tensor([[0.0843]])\n",
      "Next State : [[-0.406509  ]\n",
      " [ 0.91364676]\n",
      " [-3.241857  ]]\n",
      "Reward : [-6.1356914]\n",
      "--------------------------------------------------\n",
      "Step : 97\n",
      "Current State : [-0.29413062  0.95576525 -2.4016817 ]\n",
      "Currrent Action : tensor([[1.0329]])\n",
      "Next State : [[-0.29413062]\n",
      " [ 0.95576525]\n",
      " [-2.4016817 ]]\n",
      "Reward : [-5.00984716]\n",
      "--------------------------------------------------\n",
      "Step : 98\n",
      "Current State : [-0.21862972  0.9758079  -1.5627155 ]\n",
      "Currrent Action : tensor([[0.8143]])\n",
      "Next State : [[-0.21862972]\n",
      " [ 0.9758079 ]\n",
      " [-1.5627155 ]]\n",
      "Reward : [-4.07191041]\n",
      "--------------------------------------------------\n",
      "Step : 99\n",
      "Current State : [-0.18533935  0.9826746  -0.67985624]\n",
      "Currrent Action : tensor([[1.0067]])\n",
      "Next State : [[-0.18533935]\n",
      " [ 0.9826746 ]\n",
      " [-0.67985624]]\n",
      "Reward : [-3.45364148]\n",
      "learning iteration : 6\n",
      "--------------------------------------------------\n",
      "Step : 0\n",
      "Current State : [0.67314154 0.73951364 0.5862952 ]\n",
      "Currrent Action : tensor([[-0.5605]])\n",
      "Next State : [[0.67314154]\n",
      " [0.73951364]\n",
      " [0.5862952 ]]\n",
      "Reward : [-0.64688587]\n",
      "--------------------------------------------------\n",
      "Step : 1\n",
      "Current State : [0.63572127 0.7719187  0.9901246 ]\n",
      "Currrent Action : tensor([[-1.0054]])\n",
      "Next State : [[0.63572127]\n",
      " [0.7719187 ]\n",
      " [0.9901246 ]]\n",
      "Reward : [-0.72818744]\n",
      "--------------------------------------------------\n",
      "Step : 2\n",
      "Current State : [0.5740445 0.8188241 1.5501152]\n",
      "Currrent Action : tensor([[-0.1263]])\n",
      "Next State : [[0.5740445]\n",
      " [0.8188241]\n",
      " [1.5501152]]\n",
      "Reward : [-0.87571673]\n",
      "--------------------------------------------------\n",
      "Step : 3\n",
      "Current State : [0.48712122 0.87333435 2.0529275 ]\n",
      "Currrent Action : tensor([[-0.7420]])\n",
      "Next State : [[0.48712122]\n",
      " [0.87333435]\n",
      " [2.0529275 ]]\n",
      "Reward : [-1.16120708]\n",
      "--------------------------------------------------\n",
      "Step : 4\n",
      "Current State : [0.35839438 0.93357027 2.8448584 ]\n",
      "Currrent Action : tensor([[0.9129]])\n",
      "Next State : [[0.35839438]\n",
      " [0.93357027]\n",
      " [2.8448584 ]]\n",
      "Reward : [-1.55014109]\n",
      "--------------------------------------------------\n",
      "Step : 5\n",
      "Current State : [0.1885286 0.9820677 3.537676 ]\n",
      "Currrent Action : tensor([[-0.0491]])\n",
      "Next State : [[0.1885286]\n",
      " [0.9820677]\n",
      " [3.537676 ]]\n",
      "Reward : [-2.25953971]\n",
      "--------------------------------------------------\n",
      "Step : 6\n",
      "Current State : [-0.0266812  0.999644   4.3269606]\n",
      "Currrent Action : tensor([[0.3516]])\n",
      "Next State : [[-0.0266812]\n",
      " [ 0.999644 ]\n",
      " [ 4.3269606]]\n",
      "Reward : [-3.15916629]\n",
      "--------------------------------------------------\n",
      "Step : 7\n",
      "Current State : [-0.27237797  0.96219033  4.983584  ]\n",
      "Currrent Action : tensor([[-0.6207]])\n",
      "Next State : [[-0.27237797]\n",
      " [ 0.96219033]\n",
      " [ 4.983584  ]]\n",
      "Reward : [-4.42458867]\n",
      "--------------------------------------------------\n",
      "Step : 8\n",
      "Current State : [-0.53532714  0.8446448   5.7806287 ]\n",
      "Currrent Action : tensor([[0.5027]])\n",
      "Next State : [[-0.53532714]\n",
      " [ 0.8446448 ]\n",
      " [ 5.7806287 ]]\n",
      "Reward : [-5.89401651]\n",
      "--------------------------------------------------\n",
      "Step : 9\n",
      "Current State : [-0.7743306   0.63278127  6.415231  ]\n",
      "Currrent Action : tensor([[0.0075]])\n",
      "Next State : [[-0.7743306 ]\n",
      " [ 0.63278127]\n",
      " [ 6.415231  ]]\n",
      "Reward : [-7.90274426]\n",
      "--------------------------------------------------\n",
      "Step : 10\n",
      "Current State : [-0.9374207   0.34819874  6.589817  ]\n",
      "Currrent Action : tensor([[-2.3968]])\n",
      "Next State : [[-0.9374207 ]\n",
      " [ 0.34819874]\n",
      " [ 6.589817  ]]\n",
      "Reward : [-10.15367978]\n",
      "--------------------------------------------------\n",
      "Step : 11\n",
      "Current State : [-0.99988127  0.01540818  6.8048024 ]\n",
      "Currrent Action : tensor([[-0.3078]])\n",
      "Next State : [[-0.99988127]\n",
      " [ 0.01540818]\n",
      " [ 6.8048024 ]]\n",
      "Reward : [-12.10414607]\n",
      "--------------------------------------------------\n",
      "Step : 12\n",
      "Current State : [-0.94658947 -0.32244125  6.874323  ]\n",
      "Currrent Action : tensor([[0.3864]])\n",
      "Next State : [[-0.94658947]\n",
      " [-0.32244125]\n",
      " [ 6.874323  ]]\n",
      "Reward : [-14.40370869]\n",
      "--------------------------------------------------\n",
      "Step : 13\n",
      "Current State : [-0.78874654 -0.61471856  6.6744347 ]\n",
      "Currrent Action : tensor([[0.2796]])\n",
      "Next State : [[-0.78874654]\n",
      " [-0.61471856]\n",
      " [ 6.6744347 ]]\n",
      "Reward : [-12.64028385]\n",
      "--------------------------------------------------\n",
      "Step : 14\n",
      "Current State : [-0.56769854 -0.8232365   6.101194  ]\n",
      "Currrent Action : tensor([[-0.7480]])\n",
      "Next State : [[-0.56769854]\n",
      " [-0.8232365 ]\n",
      " [ 6.101194  ]]\n",
      "Reward : [-10.60360305]\n",
      "--------------------------------------------------\n",
      "Step : 15\n",
      "Current State : [-0.31869382 -0.94785774  5.587128  ]\n",
      "Currrent Action : tensor([[0.6891]])\n",
      "Next State : [[-0.31869382]\n",
      " [-0.94785774]\n",
      " [ 5.587128  ]]\n",
      "Reward : [-8.45139872]\n",
      "--------------------------------------------------\n",
      "Step : 16\n",
      "Current State : [-0.07634725 -0.9970813   4.9585905 ]\n",
      "Currrent Action : tensor([[0.5490]])\n",
      "Next State : [[-0.07634725]\n",
      " [-0.9970813 ]\n",
      " [ 4.9585905 ]]\n",
      "Reward : [-6.7134852]\n",
      "--------------------------------------------------\n",
      "Step : 17\n",
      "Current State : [ 0.121387   -0.99260527  3.962174  ]\n",
      "Currrent Action : tensor([[-1.6574]])\n",
      "Next State : [[ 0.121387  ]\n",
      " [-0.99260527]\n",
      " [ 3.962174  ]]\n",
      "Reward : [-5.17483561]\n",
      "--------------------------------------------------\n",
      "Step : 18\n",
      "Current State : [ 0.27994215 -0.96001685  3.2409356 ]\n",
      "Currrent Action : tensor([[0.1548]])\n",
      "Next State : [[ 0.27994215]\n",
      " [-0.96001685]\n",
      " [ 3.2409356 ]]\n",
      "Reward : [-3.66982383]\n",
      "--------------------------------------------------\n",
      "Step : 19\n",
      "Current State : [ 0.40125668 -0.9159656   2.583091  ]\n",
      "Currrent Action : tensor([[0.4145]])\n",
      "Next State : [[ 0.40125668]\n",
      " [-0.9159656 ]\n",
      " [ 2.583091  ]]\n",
      "Reward : [-2.70706786]\n",
      "--------------------------------------------------\n",
      "Step : 20\n",
      "Current State : [ 0.4819631 -0.8761915  1.8001078]\n",
      "Currrent Action : tensor([[-0.6401]])\n",
      "Next State : [[ 0.4819631]\n",
      " [-0.8761915]\n",
      " [ 1.8001078]]\n",
      "Reward : [-2.00839635]\n",
      "--------------------------------------------------\n",
      "Step : 21\n",
      "Current State : [ 0.5402921 -0.8414776  1.3578067]\n",
      "Currrent Action : tensor([[1.4323]])\n",
      "Next State : [[ 0.5402921]\n",
      " [-0.8414776]\n",
      " [ 1.3578067]]\n",
      "Reward : [-1.46650604]\n",
      "--------------------------------------------------\n",
      "Step : 22\n",
      "Current State : [ 0.56768376 -0.8232467   0.65810835]\n",
      "Currrent Action : tensor([[-0.4573]])\n",
      "Next State : [[ 0.56768376]\n",
      " [-0.8232467 ]\n",
      " [ 0.65810835]]\n",
      "Reward : [-1.18459734]\n",
      "--------------------------------------------------\n",
      "Step : 23\n",
      "Current State : [ 0.55696183 -0.8305381  -0.2593267 ]\n",
      "Currrent Action : tensor([[-2.2578]])\n",
      "Next State : [[ 0.55696183]\n",
      " [-0.8305381 ]\n",
      " [-0.2593267 ]]\n",
      "Reward : [-0.98260614]\n",
      "--------------------------------------------------\n",
      "Step : 24\n",
      "Current State : [ 0.5278725  -0.84932363 -0.69259125]\n",
      "Currrent Action : tensor([[1.2643]])\n",
      "Next State : [[ 0.5278725 ]\n",
      " [-0.84932363]\n",
      " [-0.69259125]]\n",
      "Reward : [-0.96886666]\n",
      "--------------------------------------------------\n",
      "Step : 25\n",
      "Current State : [ 0.45718342 -0.8893724  -1.6253598 ]\n",
      "Currrent Action : tensor([[-1.9718]])\n",
      "Next State : [[ 0.45718342]\n",
      " [-0.8893724 ]\n",
      " [-1.6253598 ]]\n",
      "Reward : [-1.0814779]\n",
      "--------------------------------------------------\n",
      "Step : 26\n",
      "Current State : [ 0.3664644 -0.9304321 -1.9923891]\n",
      "Currrent Action : tensor([[2.3413]])\n",
      "Next State : [[ 0.3664644]\n",
      " [-0.9304321]\n",
      " [-1.9923891]]\n",
      "Reward : [-1.46933112]\n",
      "--------------------------------------------------\n",
      "Step : 27\n",
      "Current State : [ 0.23328122 -0.9724093  -2.7951117 ]\n",
      "Currrent Action : tensor([[-0.6993]])\n",
      "Next State : [[ 0.23328122]\n",
      " [-0.9724093 ]\n",
      " [-2.7951117 ]]\n",
      "Reward : [-1.8268862]\n",
      "--------------------------------------------------\n",
      "Step : 28\n",
      "Current State : [ 0.05047762 -0.9987252  -3.699031  ]\n",
      "Currrent Action : tensor([[-1.1641]])\n",
      "Next State : [[ 0.05047762]\n",
      " [-0.9987252 ]\n",
      " [-3.699031  ]]\n",
      "Reward : [-2.56576806]\n",
      "--------------------------------------------------\n",
      "Step : 29\n",
      "Current State : [-0.17006661 -0.98543257 -4.427927  ]\n",
      "Currrent Action : tensor([[0.1343]])\n",
      "Next State : [[-0.17006661]\n",
      " [-0.98543257]\n",
      " [-4.427927  ]]\n",
      "Reward : [-3.67960493]\n",
      "--------------------------------------------------\n",
      "Step : 30\n",
      "Current State : [-0.41780916 -0.9085348  -5.2027063 ]\n",
      "Currrent Action : tensor([[-0.2380]])\n",
      "Next State : [[-0.41780916]\n",
      " [-0.9085348 ]\n",
      " [-5.2027063 ]]\n",
      "Reward : [-4.99420696]\n",
      "--------------------------------------------------\n",
      "Step : 31\n",
      "Current State : [-0.65659684 -0.75424176 -5.7053037 ]\n",
      "Currrent Action : tensor([[1.1920]])\n",
      "Next State : [[-0.65659684]\n",
      " [-0.75424176]\n",
      " [-5.7053037 ]]\n",
      "Reward : [-6.71555552]\n",
      "--------------------------------------------------\n",
      "Step : 32\n",
      "Current State : [-0.85169953 -0.52403045 -6.0584445 ]\n",
      "Currrent Action : tensor([[1.4169]])\n",
      "Next State : [[-0.85169953]\n",
      " [-0.52403045]\n",
      " [-6.0584445 ]]\n",
      "Reward : [-8.48785631]\n",
      "--------------------------------------------------\n",
      "Step : 33\n",
      "Current State : [-0.97169864 -0.23622386 -6.2619686 ]\n",
      "Currrent Action : tensor([[1.2633]])\n",
      "Next State : [[-0.97169864]\n",
      " [-0.23622386]\n",
      " [-6.2619686 ]]\n",
      "Reward : [-10.38025591]\n",
      "--------------------------------------------------\n",
      "Step : 34\n",
      "Current State : [-0.99660736  0.08230262 -6.417474  ]\n",
      "Currrent Action : tensor([[0.1444]])\n",
      "Next State : [[-0.99660736]\n",
      " [ 0.08230262]\n",
      " [-6.417474  ]]\n",
      "Reward : [-12.34932131]\n",
      "--------------------------------------------------\n",
      "Step : 35\n",
      "Current State : [-0.92074     0.39017665 -6.368552  ]\n",
      "Currrent Action : tensor([[-0.0854]])\n",
      "Next State : [[-0.92074   ]\n",
      " [ 0.39017665]\n",
      " [-6.368552  ]]\n",
      "Reward : [-13.4770898]\n",
      "--------------------------------------------------\n",
      "Step : 36\n",
      "Current State : [-0.76761013  0.64091706 -5.897379  ]\n",
      "Currrent Action : tensor([[1.1903]])\n",
      "Next State : [[-0.76761013]\n",
      " [ 0.64091706]\n",
      " [-5.897379  ]]\n",
      "Reward : [-11.5690786]\n",
      "--------------------------------------------------\n",
      "Step : 37\n",
      "Current State : [-0.56158555  0.8274187  -5.576063  ]\n",
      "Currrent Action : tensor([[-1.0625]])\n",
      "Next State : [[-0.56158555]\n",
      " [ 0.8274187 ]\n",
      " [-5.576063  ]]\n",
      "Reward : [-9.4614647]\n",
      "--------------------------------------------------\n",
      "Step : 38\n",
      "Current State : [-0.34043792  0.940267   -4.978364  ]\n",
      "Currrent Action : tensor([[-0.1524]])\n",
      "Next State : [[-0.34043792]\n",
      " [ 0.940267  ]\n",
      " [-4.978364  ]]\n",
      "Reward : [-7.80558126]\n",
      "--------------------------------------------------\n",
      "Step : 39\n",
      "Current State : [-0.1428577   0.98974323 -4.080689  ]\n",
      "Currrent Action : tensor([[1.2832]])\n",
      "Next State : [[-0.1428577 ]\n",
      " [ 0.98974323]\n",
      " [-4.080689  ]]\n",
      "Reward : [-6.15946756]\n",
      "--------------------------------------------------\n",
      "Step : 40\n",
      "Current State : [ 0.01232844  0.999924   -3.1135375 ]\n",
      "Currrent Action : tensor([[1.4990]])\n",
      "Next State : [[ 0.01232844]\n",
      " [ 0.999924  ]\n",
      " [-3.1135375 ]]\n",
      "Reward : [-4.60574044]\n",
      "--------------------------------------------------\n",
      "Step : 41\n",
      "Current State : [ 0.1338083  0.9910072 -2.437642 ]\n",
      "Currrent Action : tensor([[-0.4937]])\n",
      "Next State : [[ 0.1338083]\n",
      " [ 0.9910072]\n",
      " [-2.437642 ]]\n",
      "Reward : [-3.39847651]\n",
      "--------------------------------------------------\n",
      "Step : 42\n",
      "Current State : [ 0.2133995  0.976965  -1.6168487]\n",
      "Currrent Action : tensor([[0.5169]])\n",
      "Next State : [[ 0.2133995]\n",
      " [ 0.976965 ]\n",
      " [-1.6168487]]\n",
      "Reward : [-2.65825492]\n",
      "--------------------------------------------------\n",
      "Step : 43\n",
      "Current State : [ 0.2538496   0.96724373 -0.832097  ]\n",
      "Currrent Action : tensor([[0.3469]])\n",
      "Next State : [[ 0.2538496 ]\n",
      " [ 0.96724373]\n",
      " [-0.832097  ]]\n",
      "Reward : [-2.09957946]\n",
      "--------------------------------------------------\n",
      "Step : 44\n",
      "Current State : [0.24992403 0.9682655  0.08112688]\n",
      "Currrent Action : tensor([[1.2519]])\n",
      "Next State : [[0.24992403]\n",
      " [0.9682655 ]\n",
      " [0.08112688]]\n",
      "Reward : [-1.79776507]\n",
      "--------------------------------------------------\n",
      "Step : 45\n",
      "Current State : [0.21190633 0.97729    0.7815326 ]\n",
      "Currrent Action : tensor([[-0.1720]])\n",
      "Next State : [[0.21190633]\n",
      " [0.97729   ]\n",
      " [0.7815326 ]]\n",
      "Reward : [-1.73832454]\n",
      "--------------------------------------------------\n",
      "Step : 46\n",
      "Current State : [0.1253961  0.99210674 1.755962  ]\n",
      "Currrent Action : tensor([[1.6097]])\n",
      "Next State : [[0.1253961 ]\n",
      " [0.99210674]\n",
      " [1.755962  ]]\n",
      "Reward : [-1.9058556]\n",
      "--------------------------------------------------\n",
      "Step : 47\n",
      "Current State : [0.00472684 0.99998885 2.4200041 ]\n",
      "Currrent Action : tensor([[-0.5336]])\n",
      "Next State : [[0.00472684]\n",
      " [0.99998885]\n",
      " [2.4200041 ]]\n",
      "Reward : [-2.39685014]\n",
      "--------------------------------------------------\n",
      "Step : 48\n",
      "Current State : [-0.16797285  0.9857916   3.4699957 ]\n",
      "Currrent Action : tensor([[3.5256]])\n",
      "Next State : [[-0.16797285]\n",
      " [ 0.9857916 ]\n",
      " [ 3.4699957 ]]\n",
      "Reward : [-3.0422156]\n",
      "--------------------------------------------------\n",
      "Step : 49\n",
      "Current State : [-0.37191936  0.92826504  4.2460585 ]\n",
      "Currrent Action : tensor([[0.2448]])\n",
      "Next State : [[-0.37191936]\n",
      " [ 0.92826504]\n",
      " [ 4.2460585 ]]\n",
      "Reward : [-4.23024821]\n",
      "--------------------------------------------------\n",
      "Step : 50\n",
      "Current State : [-0.5912076  0.8065194  5.0295906]\n",
      "Currrent Action : tensor([[0.5822]])\n",
      "Next State : [[-0.5912076]\n",
      " [ 0.8065194]\n",
      " [ 5.0295906]]\n",
      "Reward : [-5.61304507]\n",
      "--------------------------------------------------\n",
      "Step : 51\n",
      "Current State : [-0.79318464  0.6089812   5.669309  ]\n",
      "Currrent Action : tensor([[0.2322]])\n",
      "Next State : [[-0.79318464]\n",
      " [ 0.6089812 ]\n",
      " [ 5.669309  ]]\n",
      "Reward : [-7.38449069]\n",
      "--------------------------------------------------\n",
      "Step : 52\n",
      "Current State : [-0.94429195  0.32910895  6.3883142 ]\n",
      "Currrent Action : tensor([[1.7485]])\n",
      "Next State : [[-0.94429195]\n",
      " [ 0.32910895]\n",
      " [ 6.3883142 ]]\n",
      "Reward : [-9.40142317]\n",
      "--------------------------------------------------\n",
      "Step : 53\n",
      "Current State : [-0.9999635   0.00853993  6.5363955 ]\n",
      "Currrent Action : tensor([[-0.6583]])\n",
      "Next State : [[-0.9999635 ]\n",
      " [ 0.00853993]\n",
      " [ 6.5363955 ]]\n",
      "Reward : [-11.95643196]\n",
      "--------------------------------------------------\n",
      "Step : 54\n",
      "Current State : [-0.94936585 -0.3141726   6.5625005 ]\n",
      "Currrent Action : tensor([[0.1313]])\n",
      "Next State : [[-0.94936585]\n",
      " [-0.3141726 ]\n",
      " [ 6.5625005 ]]\n",
      "Reward : [-14.08848236]\n",
      "--------------------------------------------------\n",
      "Step : 55\n",
      "Current State : [-0.80462635 -0.5937815   6.323308  ]\n",
      "Currrent Action : tensor([[-0.0238]])\n",
      "Next State : [[-0.80462635]\n",
      " [-0.5937815 ]\n",
      " [ 6.323308  ]]\n",
      "Reward : [-12.27036926]\n",
      "--------------------------------------------------\n",
      "Step : 56\n",
      "Current State : [-0.58872986 -0.8083299   6.1111865 ]\n",
      "Currrent Action : tensor([[1.5548]])\n",
      "Next State : [[-0.58872986]\n",
      " [-0.8083299 ]\n",
      " [ 6.1111865 ]]\n",
      "Reward : [-10.28008497]\n",
      "--------------------------------------------------\n",
      "Step : 57\n",
      "Current State : [-0.34541008 -0.9384518   5.5362177 ]\n",
      "Currrent Action : tensor([[0.2085]])\n",
      "Next State : [[-0.34541008]\n",
      " [-0.9384518 ]\n",
      " [ 5.5362177 ]]\n",
      "Reward : [-8.57594839]\n",
      "--------------------------------------------------\n",
      "Step : 58\n",
      "Current State : [-0.10429683 -0.99454623  4.9637775 ]\n",
      "Currrent Action : tensor([[0.8760]])\n",
      "Next State : [[-0.10429683]\n",
      " [-0.99454623]\n",
      " [ 4.9637775 ]]\n",
      "Reward : [-6.76548279]\n",
      "--------------------------------------------------\n",
      "Step : 59\n",
      "Current State : [ 0.11259623 -0.99364084  4.346447  ]\n",
      "Currrent Action : tensor([[0.8572]])\n",
      "Next State : [[ 0.11259623]\n",
      " [-0.99364084]\n",
      " [ 4.346447  ]]\n",
      "Reward : [-5.2712173]\n",
      "--------------------------------------------------\n",
      "Step : 60\n",
      "Current State : [ 0.28278133 -0.9591844   3.4771411 ]\n",
      "Currrent Action : tensor([[-0.8272]])\n",
      "Next State : [[ 0.28278133]\n",
      " [-0.9591844 ]\n",
      " [ 3.4771411 ]]\n",
      "Reward : [-4.01549421]\n",
      "--------------------------------------------------\n",
      "Step : 61\n",
      "Current State : [ 0.41224822 -0.91107154  2.764556  ]\n",
      "Currrent Action : tensor([[0.0454]])\n",
      "Next State : [[ 0.41224822]\n",
      " [-0.91107154]\n",
      " [ 2.764556  ]]\n",
      "Reward : [-2.8579755]\n",
      "--------------------------------------------------\n",
      "Step : 62\n",
      "Current State : [ 0.49210712 -0.87053466  1.7917645 ]\n",
      "Currrent Action : tensor([[-1.9299]])\n",
      "Next State : [[ 0.49210712]\n",
      " [-0.87053466]\n",
      " [ 1.7917645 ]]\n",
      "Reward : [-2.08103331]\n",
      "--------------------------------------------------\n",
      "Step : 63\n",
      "Current State : [ 0.5379437  -0.84298074  1.0697458 ]\n",
      "Currrent Action : tensor([[-0.4608]])\n",
      "Next State : [[ 0.5379437 ]\n",
      " [-0.84298074]\n",
      " [ 1.0697458 ]]\n",
      "Reward : [-1.4369981]\n",
      "--------------------------------------------------\n",
      "Step : 64\n",
      "Current State : [ 0.556571   -0.83080006  0.4451365 ]\n",
      "Currrent Action : tensor([[0.0508]])\n",
      "Next State : [[ 0.556571  ]\n",
      " [-0.83080006]\n",
      " [ 0.4451365 ]]\n",
      "Reward : [-1.12004693]\n",
      "--------------------------------------------------\n",
      "Step : 65\n",
      "Current State : [ 0.54363483 -0.83932185 -0.30981895]\n",
      "Currrent Action : tensor([[-0.8790]])\n",
      "Next State : [[ 0.54363483]\n",
      " [-0.83932185]\n",
      " [-0.30981895]]\n",
      "Reward : [-0.98205315]\n",
      "--------------------------------------------------\n",
      "Step : 66\n",
      "Current State : [ 0.50708014 -0.8618989  -0.85936177]\n",
      "Currrent Action : tensor([[0.5330]])\n",
      "Next State : [[ 0.50708014]\n",
      " [-0.8618989 ]\n",
      " [-0.85936177]]\n",
      "Reward : [-1.00196773]\n",
      "--------------------------------------------------\n",
      "Step : 67\n",
      "Current State : [ 0.44381955 -0.8961162  -1.4387428 ]\n",
      "Currrent Action : tensor([[0.4470]])\n",
      "Next State : [[ 0.44381955]\n",
      " [-0.8961162 ]\n",
      " [-1.4387428 ]]\n",
      "Reward : [-1.15357657]\n",
      "--------------------------------------------------\n",
      "Step : 68\n",
      "Current State : [ 0.35421318 -0.9351647  -1.955679  ]\n",
      "Currrent Action : tensor([[1.0343]])\n",
      "Next State : [[ 0.35421318]\n",
      " [-0.9351647 ]\n",
      " [-1.955679  ]]\n",
      "Reward : [-1.44225517]\n",
      "--------------------------------------------------\n",
      "Step : 69\n",
      "Current State : [ 0.23819889 -0.9712164  -2.4312327 ]\n",
      "Currrent Action : tensor([[1.5055]])\n",
      "Next State : [[ 0.23819889]\n",
      " [-0.9712164 ]\n",
      " [-2.4312327 ]]\n",
      "Reward : [-1.84574761]\n",
      "--------------------------------------------------\n",
      "Step : 70\n",
      "Current State : [ 0.08819196 -0.9961035  -3.0440853 ]\n",
      "Currrent Action : tensor([[0.7704]])\n",
      "Next State : [[ 0.08819196]\n",
      " [-0.9961035 ]\n",
      " [-3.0440853 ]]\n",
      "Reward : [-2.36134191]\n",
      "--------------------------------------------------\n",
      "Step : 71\n",
      "Current State : [-0.09361717 -0.99560827 -3.641223  ]\n",
      "Currrent Action : tensor([[0.9996]])\n",
      "Next State : [[-0.09361717]\n",
      " [-0.99560827]\n",
      " [-3.641223  ]]\n",
      "Reward : [-3.12542019]\n",
      "--------------------------------------------------\n",
      "Step : 72\n",
      "Current State : [-0.30817515 -0.95132965 -4.3903956 ]\n",
      "Currrent Action : tensor([[-0.0164]])\n",
      "Next State : [[-0.30817515]\n",
      " [-0.95132965]\n",
      " [-4.3903956 ]]\n",
      "Reward : [-4.09658003]\n",
      "--------------------------------------------------\n",
      "Step : 73\n",
      "Current State : [-0.53824145 -0.84279066 -5.101498  ]\n",
      "Currrent Action : tensor([[0.0160]])\n",
      "Next State : [[-0.53824145]\n",
      " [-0.84279066]\n",
      " [-5.101498  ]]\n",
      "Reward : [-5.4772794]\n",
      "--------------------------------------------------\n",
      "Step : 74\n",
      "Current State : [-0.7509386 -0.660372  -5.622662 ]\n",
      "Currrent Action : tensor([[0.7395]])\n",
      "Next State : [[-0.7509386]\n",
      " [-0.660372 ]\n",
      " [-5.622662 ]]\n",
      "Reward : [-7.17901877]\n",
      "--------------------------------------------------\n",
      "Step : 75\n",
      "Current State : [-0.91623527 -0.4006406  -6.1819634 ]\n",
      "Currrent Action : tensor([[-0.4268]])\n",
      "Next State : [[-0.91623527]\n",
      " [-0.4006406 ]\n",
      " [-6.1819634 ]]\n",
      "Reward : [-9.01936318]\n",
      "--------------------------------------------------\n",
      "Step : 76\n",
      "Current State : [-0.99547285 -0.09504663 -6.340512  ]\n",
      "Currrent Action : tensor([[0.9462]])\n",
      "Next State : [[-0.99547285]\n",
      " [-0.09504663]\n",
      " [-6.340512  ]]\n",
      "Reward : [-11.27205962]\n",
      "--------------------------------------------------\n",
      "Step : 77\n",
      "Current State : [-0.9763221   0.21632177 -6.264715  ]\n",
      "Currrent Action : tensor([[0.9805]])\n",
      "Next State : [[-0.9763221 ]\n",
      " [ 0.21632177]\n",
      " [-6.264715  ]]\n",
      "Reward : [-13.30173779]\n",
      "--------------------------------------------------\n",
      "Step : 78\n",
      "Current State : [-0.8712371   0.49086246 -5.900679  ]\n",
      "Currrent Action : tensor([[1.3453]])\n",
      "Next State : [[-0.8712371 ]\n",
      " [ 0.49086246]\n",
      " [-5.900679  ]]\n",
      "Reward : [-12.47360396]\n",
      "--------------------------------------------------\n",
      "Step : 79\n",
      "Current State : [-0.70707595  0.7071376  -5.4472466 ]\n",
      "Currrent Action : tensor([[0.5686]])\n",
      "Next State : [[-0.70707595]\n",
      " [ 0.7071376 ]\n",
      " [-5.4472466 ]]\n",
      "Reward : [-10.39120669]\n",
      "--------------------------------------------------\n",
      "Step : 80\n",
      "Current State : [-0.51776433  0.8555233  -4.8223786 ]\n",
      "Currrent Action : tensor([[0.6301]])\n",
      "Next State : [[-0.51776433]\n",
      " [ 0.8555233 ]\n",
      " [-4.8223786 ]]\n",
      "Reward : [-8.51909365]\n",
      "--------------------------------------------------\n",
      "Step : 81\n",
      "Current State : [-0.32706496  0.94500184 -4.2207923 ]\n",
      "Currrent Action : tensor([[-0.2670]])\n",
      "Next State : [[-0.32706496]\n",
      " [ 0.94500184]\n",
      " [-4.2207923 ]]\n",
      "Reward : [-6.79896537]\n",
      "--------------------------------------------------\n",
      "Step : 82\n",
      "Current State : [-0.15841791  0.98737216 -3.4821584 ]\n",
      "Currrent Action : tensor([[0.1992]])\n",
      "Next State : [[-0.15841791]\n",
      " [ 0.98737216]\n",
      " [-3.4821584 ]]\n",
      "Reward : [-5.40673524]\n",
      "--------------------------------------------------\n",
      "Step : 83\n",
      "Current State : [-0.01868276  0.9998255  -2.8080857 ]\n",
      "Currrent Action : tensor([[-0.4430]])\n",
      "Next State : [[-0.01868276]\n",
      " [ 0.9998255 ]\n",
      " [-2.8080857 ]]\n",
      "Reward : [-4.20523928]\n",
      "--------------------------------------------------\n",
      "Step : 84\n",
      "Current State : [ 0.07352219  0.9972936  -1.8454486 ]\n",
      "Currrent Action : tensor([[1.4185]])\n",
      "Next State : [[ 0.07352219]\n",
      " [ 0.9972936 ]\n",
      " [-1.8454486 ]]\n",
      "Reward : [-3.31699369]\n",
      "--------------------------------------------------\n",
      "Step : 85\n",
      "Current State : [ 0.12752831  0.99183494 -1.085759  ]\n",
      "Currrent Action : tensor([[0.0781]])\n",
      "Next State : [[ 0.12752831]\n",
      " [ 0.99183494]\n",
      " [-1.085759  ]]\n",
      "Reward : [-2.58220519]\n",
      "--------------------------------------------------\n",
      "Step : 86\n",
      "Current State : [ 0.1340522   0.99097425 -0.13160872]\n",
      "Currrent Action : tensor([[1.4018]])\n",
      "Next State : [[ 0.1340522 ]\n",
      " [ 0.99097425]\n",
      " [-0.13160872]]\n",
      "Reward : [-2.20186991]\n",
      "--------------------------------------------------\n",
      "Step : 87\n",
      "Current State : [0.10430937 0.9945449  0.59915024]\n",
      "Currrent Action : tensor([[-0.0831]])\n",
      "Next State : [[0.10430937]\n",
      " [0.9945449 ]\n",
      " [0.59915024]]\n",
      "Reward : [-2.06480974]\n",
      "--------------------------------------------------\n",
      "Step : 88\n",
      "Current State : [0.04847885 0.99882424 1.1200321 ]\n",
      "Currrent Action : tensor([[-1.5002]])\n",
      "Next State : [[0.04847885]\n",
      " [0.99882424]\n",
      " [1.1200321 ]]\n",
      "Reward : [-2.18817513]\n",
      "--------------------------------------------------\n",
      "Step : 89\n",
      "Current State : [-0.04172812  0.999129    1.8047619 ]\n",
      "Currrent Action : tensor([[-0.4293]])\n",
      "Next State : [[-0.04172812]\n",
      " [ 0.999129  ]\n",
      " [ 1.8047619 ]]\n",
      "Reward : [-2.44302407]\n",
      "--------------------------------------------------\n",
      "Step : 90\n",
      "Current State : [-0.15383239  0.98809695  2.2541087 ]\n",
      "Currrent Action : tensor([[-2.8906]])\n",
      "Next State : [[-0.15383239]\n",
      " [ 0.98809695]\n",
      " [ 2.2541087 ]]\n",
      "Reward : [-2.9299907]\n",
      "--------------------------------------------------\n",
      "Step : 91\n",
      "Current State : [-0.2932497  0.9560359  2.863571 ]\n",
      "Currrent Action : tensor([[-0.8774]])\n",
      "Next State : [[-0.2932497]\n",
      " [ 0.9560359]\n",
      " [ 2.863571 ]]\n",
      "Reward : [-3.48533033]\n",
      "--------------------------------------------------\n",
      "Step : 92\n",
      "Current State : [-0.4505717   0.89274025  3.3956285 ]\n",
      "Currrent Action : tensor([[-1.2331]])\n",
      "Next State : [[-0.4505717 ]\n",
      " [ 0.89274025]\n",
      " [ 3.3956285 ]]\n",
      "Reward : [-4.31251969]\n",
      "--------------------------------------------------\n",
      "Step : 93\n",
      "Current State : [-0.61971664  0.7848256   4.0195193 ]\n",
      "Currrent Action : tensor([[-0.3044]])\n",
      "Next State : [[-0.61971664]\n",
      " [ 0.7848256 ]\n",
      " [ 4.0195193 ]]\n",
      "Reward : [-5.30738906]\n",
      "--------------------------------------------------\n",
      "Step : 94\n",
      "Current State : [-0.778588   0.6275354  4.48062  ]\n",
      "Currrent Action : tensor([[-0.8501]])\n",
      "Next State : [[-0.778588 ]\n",
      " [ 0.6275354]\n",
      " [ 4.48062  ]]\n",
      "Reward : [-6.63029422]\n",
      "--------------------------------------------------\n",
      "Step : 95\n",
      "Current State : [-0.9070871   0.42094302  4.8779807 ]\n",
      "Currrent Action : tensor([[-0.4886]])\n",
      "Next State : [[-0.9070871 ]\n",
      " [ 0.42094302]\n",
      " [ 4.8779807 ]]\n",
      "Reward : [-8.07523262]\n",
      "--------------------------------------------------\n",
      "Step : 96\n",
      "Current State : [-0.9839776   0.17829192  5.1046886 ]\n",
      "Currrent Action : tensor([[-0.5933]])\n",
      "Next State : [[-0.9839776 ]\n",
      " [ 0.17829192]\n",
      " [ 5.1046886 ]]\n",
      "Reward : [-9.70825491]\n",
      "--------------------------------------------------\n",
      "Step : 97\n",
      "Current State : [-0.9972122 -0.0746185  5.078764 ]\n",
      "Currrent Action : tensor([[-1.0643]])\n",
      "Next State : [[-0.9972122]\n",
      " [-0.0746185]\n",
      " [ 5.078764 ]]\n",
      "Reward : [-11.38238949]\n",
      "--------------------------------------------------\n",
      "Step : 98\n",
      "Current State : [-0.9479139  -0.31852666  4.989737  ]\n",
      "Currrent Action : tensor([[-0.2204]])\n",
      "Next State : [[-0.9479139 ]\n",
      " [-0.31852666]\n",
      " [ 4.989737  ]]\n",
      "Reward : [-11.98533758]\n",
      "--------------------------------------------------\n",
      "Step : 99\n",
      "Current State : [-0.84453434 -0.5355014   4.8185315 ]\n",
      "Currrent Action : tensor([[0.4513]])\n",
      "Next State : [[-0.84453434]\n",
      " [-0.5355014 ]\n",
      " [ 4.8185315 ]]\n",
      "Reward : [-10.42779475]\n",
      "learning iteration : 7\n",
      "--------------------------------------------------\n",
      "Step : 0\n",
      "Current State : [0.8589392 0.5120777 0.2692957]\n",
      "Currrent Action : tensor([[0.5920]])\n",
      "Next State : [[0.8589392]\n",
      " [0.5120777]\n",
      " [0.2692957]]\n",
      "Reward : [-0.27886669]\n",
      "--------------------------------------------------\n",
      "Step : 1\n",
      "Current State : [0.8369017 0.5473532 0.8319286]\n",
      "Currrent Action : tensor([[1.1905]])\n",
      "Next State : [[0.8369017]\n",
      " [0.5473532]\n",
      " [0.8319286]]\n",
      "Reward : [-0.29768513]\n",
      "--------------------------------------------------\n",
      "Step : 2\n",
      "Current State : [0.7977771 0.6029525 1.3599699]\n",
      "Currrent Action : tensor([[0.7835]])\n",
      "Next State : [[0.7977771]\n",
      " [0.6029525]\n",
      " [1.3599699]]\n",
      "Reward : [-0.40529515]\n",
      "--------------------------------------------------\n",
      "Step : 3\n",
      "Current State : [0.74089444 0.6716215  1.7839664 ]\n",
      "Currrent Action : tensor([[-0.1881]])\n",
      "Next State : [[0.74089444]\n",
      " [0.6716215 ]\n",
      " [1.7839664 ]]\n",
      "Reward : [-0.60385096]\n",
      "--------------------------------------------------\n",
      "Step : 4\n",
      "Current State : [0.66169494 0.74977314 2.2264807 ]\n",
      "Currrent Action : tensor([[-0.4080]])\n",
      "Next State : [[0.66169494]\n",
      " [0.74977314]\n",
      " [2.2264807 ]]\n",
      "Reward : [-0.86069793]\n",
      "--------------------------------------------------\n",
      "Step : 5\n",
      "Current State : [0.5549691 0.831871  2.695027 ]\n",
      "Currrent Action : tensor([[-0.6252]])\n",
      "Next State : [[0.5549691]\n",
      " [0.831871 ]\n",
      " [2.695027 ]]\n",
      "Reward : [-1.21474034]\n",
      "--------------------------------------------------\n",
      "Step : 6\n",
      "Current State : [0.40848732 0.912764   3.3505917 ]\n",
      "Currrent Action : tensor([[0.2111]])\n",
      "Next State : [[0.40848732]\n",
      " [0.912764  ]\n",
      " [3.3505917 ]]\n",
      "Reward : [-1.69161007]\n",
      "--------------------------------------------------\n",
      "Step : 7\n",
      "Current State : [0.2269695  0.97390187 3.8366272 ]\n",
      "Currrent Action : tensor([[-1.3236]])\n",
      "Next State : [[0.2269695 ]\n",
      " [0.97390187]\n",
      " [3.8366272 ]]\n",
      "Reward : [-2.44689868]\n",
      "--------------------------------------------------\n",
      "Step : 8\n",
      "Current State : [-2.4497162e-03  9.9999702e-01  4.6282911e+00]\n",
      "Currrent Action : tensor([[0.4082]])\n",
      "Next State : [[-2.4497162e-03]\n",
      " [ 9.9999702e-01]\n",
      " [ 4.6282911e+00]]\n",
      "Reward : [-3.27264921]\n",
      "--------------------------------------------------\n",
      "Step : 9\n",
      "Current State : [-0.26982474  0.96290946  5.415226  ]\n",
      "Currrent Action : tensor([[0.2462]])\n",
      "Next State : [[-0.26982474]\n",
      " [ 0.96290946]\n",
      " [ 5.415226  ]]\n",
      "Reward : [-4.61727144]\n",
      "--------------------------------------------------\n",
      "Step : 10\n",
      "Current State : [-0.549245    0.83566135  6.16499   ]\n",
      "Currrent Action : tensor([[0.1839]])\n",
      "Next State : [[-0.549245  ]\n",
      " [ 0.83566135]\n",
      " [ 6.16499   ]]\n",
      "Reward : [-6.33286391]\n",
      "--------------------------------------------------\n",
      "Step : 11\n",
      "Current State : [-0.7982126   0.60237586  6.8572307 ]\n",
      "Currrent Action : tensor([[0.4366]])\n",
      "Next State : [[-0.7982126 ]\n",
      " [ 0.60237586]\n",
      " [ 6.8572307 ]]\n",
      "Reward : [-8.43311016]\n",
      "--------------------------------------------------\n",
      "Step : 12\n",
      "Current State : [-0.9618447   0.27359605  7.3868904 ]\n",
      "Currrent Action : tensor([[0.5192]])\n",
      "Next State : [[-0.9618447 ]\n",
      " [ 0.27359605]\n",
      " [ 7.3868904 ]]\n",
      "Reward : [-10.92804662]\n",
      "--------------------------------------------------\n",
      "Step : 13\n",
      "Current State : [-0.9953562  -0.09626033  7.4707875 ]\n",
      "Currrent Action : tensor([[-0.8087]])\n",
      "Next State : [[-0.9953562 ]\n",
      " [-0.09626033]\n",
      " [ 7.4707875 ]]\n",
      "Reward : [-13.6624169]\n",
      "--------------------------------------------------\n",
      "Step : 14\n",
      "Current State : [-0.8936903  -0.44868436  7.377661  ]\n",
      "Currrent Action : tensor([[-0.1395]])\n",
      "Next State : [[-0.8936903 ]\n",
      " [-0.44868436]\n",
      " [ 7.377661  ]]\n",
      "Reward : [-14.85442558]\n",
      "--------------------------------------------------\n",
      "Step : 15\n",
      "Current State : [-0.6822141 -0.7311525  7.094341 ]\n",
      "Currrent Action : tensor([[0.3546]])\n",
      "Next State : [[-0.6822141]\n",
      " [-0.7311525]\n",
      " [ 7.094341 ]]\n",
      "Reward : [-12.60569571]\n",
      "--------------------------------------------------\n",
      "Step : 16\n",
      "Current State : [-0.40770862 -0.9131121   6.6168675 ]\n",
      "Currrent Action : tensor([[0.4726]])\n",
      "Next State : [[-0.40770862]\n",
      " [-0.9131121 ]\n",
      " [ 6.6168675 ]]\n",
      "Reward : [-10.42293805]\n",
      "--------------------------------------------------\n",
      "Step : 17\n",
      "Current State : [-0.10895771 -0.9940464   6.2153754 ]\n",
      "Currrent Action : tensor([[1.8889]])\n",
      "Next State : [[-0.10895771]\n",
      " [-0.9940464 ]\n",
      " [ 6.2153754 ]]\n",
      "Reward : [-8.34490596]\n",
      "--------------------------------------------------\n",
      "Step : 18\n",
      "Current State : [ 0.16277984 -0.9866624   5.4536376 ]\n",
      "Currrent Action : tensor([[-0.1080]])\n",
      "Next State : [[ 0.16277984]\n",
      " [-0.9866624 ]\n",
      " [ 5.4536376 ]]\n",
      "Reward : [-6.68540261]\n",
      "--------------------------------------------------\n",
      "Step : 19\n",
      "Current State : [ 0.37903318 -0.9253831   4.5048776 ]\n",
      "Currrent Action : tensor([[-1.3918]])\n",
      "Next State : [[ 0.37903318]\n",
      " [-0.9253831 ]\n",
      " [ 4.5048776 ]]\n",
      "Reward : [-4.95661529]\n",
      "--------------------------------------------------\n",
      "Step : 20\n",
      "Current State : [ 0.5392141  -0.84216875  3.6150463 ]\n",
      "Currrent Action : tensor([[-1.3053]])\n",
      "Next State : [[ 0.5392141 ]\n",
      " [-0.84216875]\n",
      " [ 3.6150463 ]]\n",
      "Reward : [-3.4283265]\n",
      "--------------------------------------------------\n",
      "Step : 21\n",
      "Current State : [ 0.6482506  -0.76142704  2.7156205 ]\n",
      "Currrent Action : tensor([[-1.7853]])\n",
      "Next State : [[ 0.6482506 ]\n",
      " [-0.76142704]\n",
      " [ 2.7156205 ]]\n",
      "Reward : [-2.31263044]\n",
      "--------------------------------------------------\n",
      "Step : 22\n",
      "Current State : [ 0.7244338 -0.6893444  2.0985594]\n",
      "Currrent Action : tensor([[-0.3066]])\n",
      "Next State : [[ 0.7244338]\n",
      " [-0.6893444]\n",
      " [ 2.0985594]]\n",
      "Reward : [-1.48666399]\n",
      "--------------------------------------------------\n",
      "Step : 23\n",
      "Current State : [ 0.77572864 -0.6310666   1.5531247 ]\n",
      "Currrent Action : tensor([[-0.1895]])\n",
      "Next State : [[ 0.77572864]\n",
      " [-0.6310666 ]\n",
      " [ 1.5531247 ]]\n",
      "Reward : [-1.01891866]\n",
      "--------------------------------------------------\n",
      "Step : 24\n",
      "Current State : [ 0.80299973 -0.5959794   0.88885427]\n",
      "Currrent Action : tensor([[-1.2731]])\n",
      "Next State : [[ 0.80299973]\n",
      " [-0.5959794 ]\n",
      " [ 0.88885427]]\n",
      "Reward : [-0.70923043]\n",
      "--------------------------------------------------\n",
      "Step : 25\n",
      "Current State : [ 0.8133502  -0.58177435  0.35152403]\n",
      "Currrent Action : tensor([[-0.6023]])\n",
      "Next State : [[ 0.8133502 ]\n",
      " [-0.58177435]\n",
      " [ 0.35152403]]\n",
      "Reward : [-0.48703175]\n",
      "--------------------------------------------------\n",
      "Step : 26\n",
      "Current State : [ 0.80200684 -0.59731483 -0.38480675]\n",
      "Currrent Action : tensor([[-2.5548]])\n",
      "Next State : [[ 0.80200684]\n",
      " [-0.59731483]\n",
      " [-0.38480675]]\n",
      "Reward : [-0.40188435]\n",
      "--------------------------------------------------\n",
      "Step : 27\n",
      "Current State : [ 0.7689461  -0.63931364 -1.0691283 ]\n",
      "Currrent Action : tensor([[-1.5756]])\n",
      "Next State : [[ 0.7689461 ]\n",
      " [-0.63931364]\n",
      " [-1.0691283 ]]\n",
      "Reward : [-0.42708065]\n",
      "--------------------------------------------------\n",
      "Step : 28\n",
      "Current State : [ 0.71055335 -0.7036434  -1.738135  ]\n",
      "Currrent Action : tensor([[-1.2635]])\n",
      "Next State : [[ 0.71055335]\n",
      " [-0.7036434 ]\n",
      " [-1.738135  ]]\n",
      "Reward : [-0.59698823]\n",
      "--------------------------------------------------\n",
      "Step : 29\n",
      "Current State : [ 0.6245183 -0.7810102 -2.315394 ]\n",
      "Currrent Action : tensor([[-0.3302]])\n",
      "Next State : [[ 0.6245183]\n",
      " [-0.7810102]\n",
      " [-2.315394 ]]\n",
      "Reward : [-0.91141939]\n",
      "--------------------------------------------------\n",
      "Step : 30\n",
      "Current State : [ 0.51291007 -0.85844237 -2.7188659 ]\n",
      "Currrent Action : tensor([[1.2152]])\n",
      "Next State : [[ 0.51291007]\n",
      " [-0.85844237]\n",
      " [-2.7188659 ]]\n",
      "Reward : [-1.34090269]\n",
      "--------------------------------------------------\n",
      "Step : 31\n",
      "Current State : [ 0.35922325 -0.9332517  -3.4227161 ]\n",
      "Currrent Action : tensor([[-0.4001]])\n",
      "Next State : [[ 0.35922325]\n",
      " [-0.9332517 ]\n",
      " [-3.4227161 ]]\n",
      "Reward : [-1.80487181]\n",
      "--------------------------------------------------\n",
      "Step : 32\n",
      "Current State : [ 0.15698345 -0.9876012  -4.1959996 ]\n",
      "Currrent Action : tensor([[-0.4890]])\n",
      "Next State : [[ 0.15698345]\n",
      " [-0.9876012 ]\n",
      " [-4.1959996 ]]\n",
      "Reward : [-2.6198151]\n",
      "--------------------------------------------------\n",
      "Step : 33\n",
      "Current State : [-0.07862413 -0.9969043  -4.726817  ]\n",
      "Currrent Action : tensor([[1.3992]])\n",
      "Next State : [[-0.07862413]\n",
      " [-0.9969043 ]\n",
      " [-4.726817  ]]\n",
      "Reward : [-3.75962261]\n",
      "--------------------------------------------------\n",
      "Step : 34\n",
      "Current State : [-0.33180103 -0.9433494  -5.190135  ]\n",
      "Currrent Action : tensor([[1.8957]])\n",
      "Next State : [[-0.33180103]\n",
      " [-0.9433494 ]\n",
      " [-5.190135  ]]\n",
      "Reward : [-4.95872948]\n",
      "--------------------------------------------------\n",
      "Step : 35\n",
      "Current State : [-0.59203476 -0.80591244 -5.9073834 ]\n",
      "Currrent Action : tensor([[-0.0649]])\n",
      "Next State : [[-0.59203476]\n",
      " [-0.80591244]\n",
      " [-5.9073834 ]]\n",
      "Reward : [-6.3380676]\n",
      "--------------------------------------------------\n",
      "Step : 36\n",
      "Current State : [-0.81348395 -0.5815873  -6.3307285 ]\n",
      "Currrent Action : tensor([[1.2073]])\n",
      "Next State : [[-0.81348395]\n",
      " [-0.5815873 ]\n",
      " [-6.3307285 ]]\n",
      "Reward : [-8.35045588]\n",
      "--------------------------------------------------\n",
      "Step : 37\n",
      "Current State : [-0.96064097 -0.27779287 -6.7836566 ]\n",
      "Currrent Action : tensor([[-0.1116]])\n",
      "Next State : [[-0.96064097]\n",
      " [-0.27779287]\n",
      " [-6.7836566 ]]\n",
      "Reward : [-10.36283271]\n",
      "--------------------------------------------------\n",
      "Step : 38\n",
      "Current State : [-0.9985157   0.05446421 -6.719739  ]\n",
      "Currrent Action : tensor([[1.8151]])\n",
      "Next State : [[-0.9985157 ]\n",
      " [ 0.05446421]\n",
      " [-6.719739  ]]\n",
      "Reward : [-12.78524803]\n",
      "--------------------------------------------------\n",
      "Step : 39\n",
      "Current State : [-0.93107945  0.36481652 -6.378891  ]\n",
      "Currrent Action : tensor([[3.6633]])\n",
      "Next State : [[-0.93107945]\n",
      " [ 0.36481652]\n",
      " [-6.378891  ]]\n",
      "Reward : [-14.04968501]\n",
      "--------------------------------------------------\n",
      "Step : 40\n",
      "Current State : [-0.7845164   0.62010807 -5.9088945 ]\n",
      "Currrent Action : tensor([[1.3092]])\n",
      "Next State : [[-0.7845164 ]\n",
      " [ 0.62010807]\n",
      " [-5.9088945 ]]\n",
      "Reward : [-11.73343194]\n",
      "--------------------------------------------------\n",
      "Step : 41\n",
      "Current State : [-0.5939025  0.804537  -5.3203   ]\n",
      "Currrent Action : tensor([[0.8234]])\n",
      "Next State : [[-0.5939025]\n",
      " [ 0.804537 ]\n",
      " [-5.3203   ]]\n",
      "Reward : [-9.60648696]\n",
      "--------------------------------------------------\n",
      "Step : 42\n",
      "Current State : [-0.3980657  0.9173569 -4.529872 ]\n",
      "Currrent Action : tensor([[1.2468]])\n",
      "Next State : [[-0.3980657]\n",
      " [ 0.9173569]\n",
      " [-4.529872 ]]\n",
      "Reward : [-7.7016265]\n",
      "--------------------------------------------------\n",
      "Step : 43\n",
      "Current State : [-0.23023054  0.9731361  -3.5418541 ]\n",
      "Currrent Action : tensor([[2.1707]])\n",
      "Next State : [[-0.23023054]\n",
      " [ 0.9731361 ]\n",
      " [-3.5418541 ]]\n",
      "Reward : [-5.97718036]\n",
      "--------------------------------------------------\n",
      "Step : 44\n",
      "Current State : [-0.09728262  0.9952568  -2.6975572 ]\n",
      "Currrent Action : tensor([[0.7630]])\n",
      "Next State : [[-0.09728262]\n",
      " [ 0.9952568 ]\n",
      " [-2.6975572 ]]\n",
      "Reward : [-4.50626422]\n",
      "--------------------------------------------------\n",
      "Step : 45\n",
      "Current State : [ 5.5814304e-05  1.0000000e+00 -1.9498508e+00]\n",
      "Currrent Action : tensor([[0.0084]])\n",
      "Next State : [[ 5.5814304e-05]\n",
      " [ 1.0000000e+00]\n",
      " [-1.9498508e+00]]\n",
      "Reward : [-3.51068302]\n",
      "--------------------------------------------------\n",
      "Step : 46\n",
      "Current State : [ 0.05573804  0.99844545 -1.1142225 ]\n",
      "Currrent Action : tensor([[0.5709]])\n",
      "Next State : [[ 0.05573804]\n",
      " [ 0.99844545]\n",
      " [-1.1142225 ]]\n",
      "Reward : [-2.84774343]\n",
      "--------------------------------------------------\n",
      "Step : 47\n",
      "Current State : [ 0.06788344  0.99769324 -0.2433748 ]\n",
      "Currrent Action : tensor([[0.8134]])\n",
      "Next State : [[ 0.06788344]\n",
      " [ 0.99769324]\n",
      " [-0.2433748 ]]\n",
      "Reward : [-2.42012489]\n",
      "--------------------------------------------------\n",
      "Step : 48\n",
      "Current State : [0.04578234 0.99895144 0.44274664]\n",
      "Currrent Action : tensor([[-0.4143]])\n",
      "Next State : [[0.04578234]\n",
      " [0.99895144]\n",
      " [0.44274664]]\n",
      "Reward : [-2.26468491]\n",
      "--------------------------------------------------\n",
      "Step : 49\n",
      "Current State : [-0.00952427  0.99995464  1.1064552 ]\n",
      "Currrent Action : tensor([[-0.5700]])\n",
      "Next State : [[-0.00952427]\n",
      " [ 0.99995464]\n",
      " [ 1.1064552 ]]\n",
      "Reward : [-2.34554622]\n",
      "--------------------------------------------------\n",
      "Step : 50\n",
      "Current State : [-0.10569772  0.9943983   1.9274224 ]\n",
      "Currrent Action : tensor([[0.4733]])\n",
      "Next State : [[-0.10569772]\n",
      " [ 0.9943983 ]\n",
      " [ 1.9274224 ]]\n",
      "Reward : [-2.62006201]\n",
      "--------------------------------------------------\n",
      "Step : 51\n",
      "Current State : [-0.24215542  0.9702375   2.773825  ]\n",
      "Currrent Action : tensor([[0.6707]])\n",
      "Next State : [[-0.24215542]\n",
      " [ 0.9702375 ]\n",
      " [ 2.773825  ]]\n",
      "Reward : [-3.18324111]\n",
      "--------------------------------------------------\n",
      "Step : 52\n",
      "Current State : [-0.39694554  0.91784215  3.2719958 ]\n",
      "Currrent Action : tensor([[-1.5300]])\n",
      "Next State : [[-0.39694554]\n",
      " [ 0.91784215]\n",
      " [ 3.2719958 ]]\n",
      "Reward : [-4.06736726]\n",
      "--------------------------------------------------\n",
      "Step : 53\n",
      "Current State : [-0.56343985  0.8261571   3.807141  ]\n",
      "Currrent Action : tensor([[-1.0216]])\n",
      "Next State : [[-0.56343985]\n",
      " [ 0.8261571 ]\n",
      " [ 3.807141  ]]\n",
      "Reward : [-4.9880125]\n",
      "--------------------------------------------------\n",
      "Step : 54\n",
      "Current State : [-0.73438984  0.67872787  4.5244737 ]\n",
      "Currrent Action : tensor([[0.6514]])\n",
      "Next State : [[-0.73438984]\n",
      " [ 0.67872787]\n",
      " [ 4.5244737 ]]\n",
      "Reward : [-6.15589234]\n",
      "--------------------------------------------------\n",
      "Step : 55\n",
      "Current State : [-0.885563    0.46451923  5.258748  ]\n",
      "Currrent Action : tensor([[1.5015]])\n",
      "Next State : [[-0.885563  ]\n",
      " [ 0.46451923]\n",
      " [ 5.258748  ]]\n",
      "Reward : [-7.78806577]\n",
      "--------------------------------------------------\n",
      "Step : 56\n",
      "Current State : [-0.9807049  0.1954939  5.726612 ]\n",
      "Currrent Action : tensor([[0.7965]])\n",
      "Next State : [[-0.9807049]\n",
      " [ 0.1954939]\n",
      " [ 5.726612 ]]\n",
      "Reward : [-9.83370498]\n",
      "--------------------------------------------------\n",
      "Step : 57\n",
      "Current State : [-0.99612874 -0.08790644  5.695622  ]\n",
      "Currrent Action : tensor([[-1.1841]])\n",
      "Next State : [[-0.99612874]\n",
      " [-0.08790644]\n",
      " [ 5.695622  ]]\n",
      "Reward : [-11.95284375]\n",
      "--------------------------------------------------\n",
      "Step : 58\n",
      "Current State : [-0.93048626 -0.36632687  5.7407665 ]\n",
      "Currrent Action : tensor([[0.7405]])\n",
      "Next State : [[-0.93048626]\n",
      " [-0.36632687]\n",
      " [ 5.7407665 ]]\n",
      "Reward : [-12.56886491]\n",
      "--------------------------------------------------\n",
      "Step : 59\n",
      "Current State : [-0.79585886 -0.6054822   5.5062647 ]\n",
      "Currrent Action : tensor([[0.2683]])\n",
      "Next State : [[-0.79585886]\n",
      " [-0.6054822 ]\n",
      " [ 5.5062647 ]]\n",
      "Reward : [-10.94942401]\n",
      "--------------------------------------------------\n",
      "Step : 60\n",
      "Current State : [-0.6170526 -0.7869219  5.1086597]\n",
      "Currrent Action : tensor([[0.3767]])\n",
      "Next State : [[-0.6170526]\n",
      " [-0.7869219]\n",
      " [ 5.1086597]]\n",
      "Reward : [-9.23821928]\n",
      "--------------------------------------------------\n",
      "Step : 61\n",
      "Current State : [-0.42418188 -0.905577    4.538669  ]\n",
      "Currrent Action : tensor([[0.1347]])\n",
      "Next State : [[-0.42418188]\n",
      " [-0.905577  ]\n",
      " [ 4.538669  ]]\n",
      "Reward : [-7.60860657]\n",
      "--------------------------------------------------\n",
      "Step : 62\n",
      "Current State : [-0.23798992 -0.97126764  3.9552495 ]\n",
      "Currrent Action : tensor([[0.6384]])\n",
      "Next State : [[-0.23798992]\n",
      " [-0.97126764]\n",
      " [ 3.9552495 ]]\n",
      "Reward : [-6.09585624]\n",
      "--------------------------------------------------\n",
      "Step : 63\n",
      "Current State : [-0.06599982 -0.99781966  3.484959  ]\n",
      "Currrent Action : tensor([[1.7211]])\n",
      "Next State : [[-0.06599982]\n",
      " [-0.99781966]\n",
      " [ 3.484959  ]]\n",
      "Reward : [-4.84741655]\n",
      "--------------------------------------------------\n",
      "Step : 64\n",
      "Current State : [ 0.07139669 -0.997448    2.7501066 ]\n",
      "Currrent Action : tensor([[0.0901]])\n",
      "Next State : [[ 0.07139669]\n",
      " [-0.997448  ]\n",
      " [ 2.7501066 ]]\n",
      "Reward : [-3.89376082]\n",
      "--------------------------------------------------\n",
      "Step : 65\n",
      "Current State : [ 0.16340306 -0.9865594   1.8536322 ]\n",
      "Currrent Action : tensor([[-0.9893]])\n",
      "Next State : [[ 0.16340306]\n",
      " [-0.9865594 ]\n",
      " [ 1.8536322 ]]\n",
      "Reward : [-3.00530416]\n",
      "--------------------------------------------------\n",
      "Step : 66\n",
      "Current State : [ 0.20998472 -0.97770464  0.94840455]\n",
      "Currrent Action : tensor([[-1.1021]])\n",
      "Next State : [[ 0.20998472]\n",
      " [-0.97770464]\n",
      " [ 0.94840455]]\n",
      "Reward : [-2.3234943]\n",
      "--------------------------------------------------\n",
      "Step : 67\n",
      "Current State : [ 0.21172608 -0.977329    0.03562842]\n",
      "Currrent Action : tensor([[-1.1967]])\n",
      "Next State : [[ 0.21172608]\n",
      " [-0.977329  ]\n",
      " [ 0.03562842]]\n",
      "Reward : [-1.93890431]\n",
      "--------------------------------------------------\n",
      "Step : 68\n",
      "Current State : [ 0.1833315  -0.9830511  -0.57932854]\n",
      "Currrent Action : tensor([[0.7869]])\n",
      "Next State : [[ 0.1833315 ]\n",
      " [-0.9830511 ]\n",
      " [-0.57932854]]\n",
      "Reward : [-1.84343184]\n",
      "--------------------------------------------------\n",
      "Step : 69\n",
      "Current State : [ 0.10964247 -0.9939711  -1.4902196 ]\n",
      "Currrent Action : tensor([[-1.1574]])\n",
      "Next State : [[ 0.10964247]\n",
      " [-0.9939711 ]\n",
      " [-1.4902196 ]]\n",
      "Reward : [-1.95706758]\n",
      "--------------------------------------------------\n",
      "Step : 70\n",
      "Current State : [ 0.01307808 -0.99991447 -1.9356979 ]\n",
      "Currrent Action : tensor([[3.0324]])\n",
      "Next State : [[ 0.01307808]\n",
      " [-0.99991447]\n",
      " [-1.9356979 ]]\n",
      "Reward : [-2.36040062]\n",
      "--------------------------------------------------\n",
      "Step : 71\n",
      "Current State : [-0.111704  -0.9937415 -2.5003214]\n",
      "Currrent Action : tensor([[1.2354]])\n",
      "Next State : [[-0.111704 ]\n",
      " [-0.9937415]\n",
      " [-2.5003214]]\n",
      "Reward : [-2.80270388]\n",
      "--------------------------------------------------\n",
      "Step : 72\n",
      "Current State : [-0.27342683 -0.9618928  -3.3003244 ]\n",
      "Currrent Action : tensor([[-0.3646]])\n",
      "Next State : [[-0.27342683]\n",
      " [-0.9618928 ]\n",
      " [-3.3003244 ]]\n",
      "Reward : [-3.45688715]\n",
      "--------------------------------------------------\n",
      "Step : 73\n",
      "Current State : [-0.45690283 -0.8895166  -3.9511278 ]\n",
      "Currrent Action : tensor([[0.4708]])\n",
      "Next State : [[-0.45690283]\n",
      " [-0.8895166 ]\n",
      " [-3.9511278 ]]\n",
      "Reward : [-4.50361641]\n",
      "--------------------------------------------------\n",
      "Step : 74\n",
      "Current State : [-0.64770865 -0.76188815 -4.6012573 ]\n",
      "Currrent Action : tensor([[0.1134]])\n",
      "Next State : [[-0.64770865]\n",
      " [-0.76188815]\n",
      " [-4.6012573 ]]\n",
      "Reward : [-5.7444329]\n",
      "--------------------------------------------------\n",
      "Step : 75\n",
      "Current State : [-0.813816   -0.58112264 -4.9223146 ]\n",
      "Currrent Action : tensor([[1.6691]])\n",
      "Next State : [[-0.813816  ]\n",
      " [-0.58112264]\n",
      " [-4.9223146 ]]\n",
      "Reward : [-7.29724884]\n",
      "--------------------------------------------------\n",
      "Step : 76\n",
      "Current State : [-0.9428661 -0.333172  -5.608834 ]\n",
      "Currrent Action : tensor([[-1.6712]])\n",
      "Next State : [[-0.9428661]\n",
      " [-0.333172 ]\n",
      " [-5.608834 ]]\n",
      "Reward : [-8.78359867]\n",
      "--------------------------------------------------\n",
      "Step : 77\n",
      "Current State : [-0.9982833  -0.05856917 -5.6212616 ]\n",
      "Currrent Action : tensor([[1.5830]])\n",
      "Next State : [[-0.9982833 ]\n",
      " [-0.05856917]\n",
      " [-5.6212616 ]]\n",
      "Reward : [-10.99920161]\n",
      "--------------------------------------------------\n",
      "Step : 78\n",
      "Current State : [-0.9743134   0.22519639 -5.7149463 ]\n",
      "Currrent Action : tensor([[-0.3317]])\n",
      "Next State : [[-0.9743134 ]\n",
      " [ 0.22519639]\n",
      " [-5.7149463 ]]\n",
      "Reward : [-12.66479527]\n",
      "--------------------------------------------------\n",
      "Step : 79\n",
      "Current State : [-0.8809243  0.4732572 -5.3168006]\n",
      "Currrent Action : tensor([[1.5283]])\n",
      "Next State : [[-0.8809243]\n",
      " [ 0.4732572]\n",
      " [-5.3168006]]\n",
      "Reward : [-11.76240436]\n",
      "--------------------------------------------------\n",
      "Step : 80\n",
      "Current State : [-0.7318535   0.68146205 -5.135477  ]\n",
      "Currrent Action : tensor([[-1.1575]])\n",
      "Next State : [[-0.7318535 ]\n",
      " [ 0.68146205]\n",
      " [-5.135477  ]]\n",
      "Reward : [-9.8433009]\n",
      "--------------------------------------------------\n",
      "Step : 81\n",
      "Current State : [-0.5628775   0.82654035 -4.463496  ]\n",
      "Currrent Action : tensor([[1.0726]])\n",
      "Next State : [[-0.5628775 ]\n",
      " [ 0.82654035]\n",
      " [-4.463496  ]]\n",
      "Reward : [-8.35933357]\n",
      "--------------------------------------------------\n",
      "Step : 82\n",
      "Current State : [-0.3813883   0.92441493 -4.1313076 ]\n",
      "Currrent Action : tensor([[-1.9181]])\n",
      "Next State : [[-0.3813883 ]\n",
      " [ 0.92441493]\n",
      " [-4.1313076 ]]\n",
      "Reward : [-6.69904246]\n",
      "--------------------------------------------------\n",
      "Step : 83\n",
      "Current State : [-0.23225732  0.9726544  -3.1379964 ]\n",
      "Currrent Action : tensor([[2.4036]])\n",
      "Next State : [[-0.23225732]\n",
      " [ 0.9726544 ]\n",
      " [-3.1379964 ]]\n",
      "Reward : [-5.56058311]\n",
      "--------------------------------------------------\n",
      "Step : 84\n",
      "Current State : [-0.12194178  0.99253726 -2.2430358 ]\n",
      "Currrent Action : tensor([[1.1031]])\n",
      "Next State : [[-0.12194178]\n",
      " [ 0.99253726]\n",
      " [-2.2430358 ]]\n",
      "Reward : [-4.24464503]\n",
      "--------------------------------------------------\n",
      "Step : 85\n",
      "Current State : [-0.05750008  0.9983455  -1.2942843 ]\n",
      "Currrent Action : tensor([[1.3623]])\n",
      "Next State : [[-0.05750008]\n",
      " [ 0.9983455 ]\n",
      " [-1.2942843 ]]\n",
      "Reward : [-3.37136933]\n",
      "--------------------------------------------------\n",
      "Step : 86\n",
      "Current State : [-0.03485329  0.99939245 -0.4534293 ]\n",
      "Currrent Action : tensor([[0.6140]])\n",
      "Next State : [[-0.03485329]\n",
      " [ 0.99939245]\n",
      " [-0.4534293 ]]\n",
      "Reward : [-2.81934669]\n",
      "--------------------------------------------------\n",
      "Step : 87\n",
      "Current State : [-0.05541198  0.9984636   0.41160062]\n",
      "Currrent Action : tensor([[0.7699]])\n",
      "Next State : [[-0.05541198]\n",
      " [ 0.9984636 ]\n",
      " [ 0.41160062]]\n",
      "Reward : [-2.59928591]\n",
      "--------------------------------------------------\n",
      "Step : 88\n",
      "Current State : [-0.11800181  0.9930134   1.2567403 ]\n",
      "Currrent Action : tensor([[0.6419]])\n",
      "Next State : [[-0.11800181]\n",
      " [ 0.9930134 ]\n",
      " [ 1.2567403 ]]\n",
      "Reward : [-2.66199942]\n",
      "--------------------------------------------------\n",
      "Step : 89\n",
      "Current State : [-0.21621779  0.9763452   1.993231  ]\n",
      "Currrent Action : tensor([[-0.0551]])\n",
      "Next State : [[-0.21621779]\n",
      " [ 0.9763452 ]\n",
      " [ 1.993231  ]]\n",
      "Reward : [-3.01091268]\n",
      "--------------------------------------------------\n",
      "Step : 90\n",
      "Current State : [-0.3452232  0.9385206  2.6907525]\n",
      "Currrent Action : tensor([[-0.2316]])\n",
      "Next State : [[-0.3452232]\n",
      " [ 0.9385206]\n",
      " [ 2.6907525]]\n",
      "Reward : [-3.59692451]\n",
      "--------------------------------------------------\n",
      "Step : 91\n",
      "Current State : [-0.49130479  0.8709877   3.2222123 ]\n",
      "Currrent Action : tensor([[-1.1495]])\n",
      "Next State : [[-0.49130479]\n",
      " [ 0.8709877 ]\n",
      " [ 3.2222123 ]]\n",
      "Reward : [-4.42431503]\n",
      "--------------------------------------------------\n",
      "Step : 92\n",
      "Current State : [-0.64326453  0.765644    3.7033455 ]\n",
      "Currrent Action : tensor([[-1.1474]])\n",
      "Next State : [[-0.64326453]\n",
      " [ 0.765644  ]\n",
      " [ 3.7033455 ]]\n",
      "Reward : [-5.38423635]\n",
      "--------------------------------------------------\n",
      "Step : 93\n",
      "Current State : [-0.7928959   0.60935706  4.3358474 ]\n",
      "Currrent Action : tensor([[0.3885]])\n",
      "Next State : [[-0.7928959 ]\n",
      " [ 0.60935706]\n",
      " [ 4.3358474 ]]\n",
      "Reward : [-6.52248842]\n",
      "--------------------------------------------------\n",
      "Step : 94\n",
      "Current State : [-0.91861564  0.39515233  4.9803166 ]\n",
      "Currrent Action : tensor([[1.2497]])\n",
      "Next State : [[-0.91861564]\n",
      " [ 0.39515233]\n",
      " [ 4.9803166 ]]\n",
      "Reward : [-8.06342141]\n",
      "--------------------------------------------------\n",
      "Step : 95\n",
      "Current State : [-0.98912555  0.14707346  5.1724954 ]\n",
      "Currrent Action : tensor([[-0.6946]])\n",
      "Next State : [[-0.98912555]\n",
      " [ 0.14707346]\n",
      " [ 5.1724954 ]]\n",
      "Reward : [-9.96302642]\n",
      "--------------------------------------------------\n",
      "Step : 96\n",
      "Current State : [-0.9931368  -0.11695825  5.296709  ]\n",
      "Currrent Action : tensor([[0.0927]])\n",
      "Next State : [[-0.9931368 ]\n",
      " [-0.11695825]\n",
      " [ 5.296709  ]]\n",
      "Reward : [-11.63941815]\n",
      "--------------------------------------------------\n",
      "Step : 97\n",
      "Current State : [-0.92989475 -0.3678257   5.1888633 ]\n",
      "Currrent Action : tensor([[-0.1342]])\n",
      "Next State : [[-0.92989475]\n",
      " [-0.3678257 ]\n",
      " [ 5.1888633 ]]\n",
      "Reward : [-11.9523209]\n",
      "--------------------------------------------------\n",
      "Step : 98\n",
      "Current State : [-0.81258607 -0.5828413   4.9110236 ]\n",
      "Currrent Action : tensor([[-0.0131]])\n",
      "Next State : [[-0.81258607]\n",
      " [-0.5828413 ]\n",
      " [ 4.9110236 ]]\n",
      "Reward : [-10.33722915]\n",
      "--------------------------------------------------\n",
      "Step : 99\n",
      "Current State : [-0.668487  -0.7437238  4.3280563]\n",
      "Currrent Action : tensor([[-0.9722]])\n",
      "Next State : [[-0.668487 ]\n",
      " [-0.7437238]\n",
      " [ 4.3280563]]\n",
      "Reward : [-8.75999454]\n",
      "learning iteration : 8\n",
      "--------------------------------------------------\n",
      "Step : 0\n",
      "Current State : [0.80285347 0.59617645 0.69690955]\n",
      "Currrent Action : tensor([[-0.5761]])\n",
      "Next State : [[0.80285347]\n",
      " [0.59617645]\n",
      " [0.69690955]]\n",
      "Reward : [-0.37778452]\n",
      "--------------------------------------------------\n",
      "Step : 1\n",
      "Current State : [0.75995666 0.64997375 1.3763943 ]\n",
      "Currrent Action : tensor([[1.5490]])\n",
      "Next State : [[0.75995666]\n",
      " [0.64997375]\n",
      " [1.3763943 ]]\n",
      "Reward : [-0.45894397]\n",
      "--------------------------------------------------\n",
      "Step : 2\n",
      "Current State : [0.70184666 0.7123281  1.7051964 ]\n",
      "Currrent Action : tensor([[-1.0579]])\n",
      "Next State : [[0.70184666]\n",
      " [0.7123281 ]\n",
      " [1.7051964 ]]\n",
      "Reward : [-0.691192]\n",
      "--------------------------------------------------\n",
      "Step : 3\n",
      "Current State : [0.6210934 0.7837366 2.1569922]\n",
      "Currrent Action : tensor([[-0.5497]])\n",
      "Next State : [[0.6210934]\n",
      " [0.7837366]\n",
      " [2.1569922]]\n",
      "Reward : [-0.91961882]\n",
      "--------------------------------------------------\n",
      "Step : 4\n",
      "Current State : [0.5012021 0.8653303 2.9029925]\n",
      "Currrent Action : tensor([[1.0547]])\n",
      "Next State : [[0.5012021]\n",
      " [0.8653303]\n",
      " [2.9029925]]\n",
      "Reward : [-1.27756096]\n",
      "--------------------------------------------------\n",
      "Step : 5\n",
      "Current State : [0.33587667 0.941906   3.649032  ]\n",
      "Currrent Action : tensor([[0.6469]])\n",
      "Next State : [[0.33587667]\n",
      " [0.941906  ]\n",
      " [3.649032  ]]\n",
      "Reward : [-1.93687134]\n",
      "--------------------------------------------------\n",
      "Step : 6\n",
      "Current State : [0.13506815 0.9908363  4.1410713 ]\n",
      "Currrent Action : tensor([[-1.4293]])\n",
      "Next State : [[0.13506815]\n",
      " [0.9908363 ]\n",
      " [4.1410713 ]]\n",
      "Reward : [-2.84221017]\n",
      "--------------------------------------------------\n",
      "Step : 7\n",
      "Current State : [-0.10757593  0.9941969   4.865335  ]\n",
      "Currrent Action : tensor([[-0.1258]])\n",
      "Next State : [[-0.10757593]\n",
      " [ 0.9941969 ]\n",
      " [ 4.865335  ]]\n",
      "Reward : [-3.77498956]\n",
      "--------------------------------------------------\n",
      "Step : 8\n",
      "Current State : [-0.38073313  0.92468494  5.65609   ]\n",
      "Currrent Action : tensor([[0.3007]])\n",
      "Next State : [[-0.38073313]\n",
      " [ 0.92468494]\n",
      " [ 5.65609   ]]\n",
      "Reward : [-5.18487243]\n",
      "--------------------------------------------------\n",
      "Step : 9\n",
      "Current State : [-0.65637845  0.7544318   6.508386  ]\n",
      "Currrent Action : tensor([[1.0586]])\n",
      "Next State : [[-0.65637845]\n",
      " [ 0.7544318 ]\n",
      " [ 6.508386  ]]\n",
      "Reward : [-7.04728832]\n",
      "--------------------------------------------------\n",
      "Step : 10\n",
      "Current State : [-0.8765247   0.48135695  7.051717  ]\n",
      "Currrent Action : tensor([[-0.1500]])\n",
      "Next State : [[-0.8765247 ]\n",
      " [ 0.48135695]\n",
      " [ 7.051717  ]]\n",
      "Reward : [-9.46540716]\n",
      "--------------------------------------------------\n",
      "Step : 11\n",
      "Current State : [-0.99168605  0.12868111  7.463265  ]\n",
      "Currrent Action : tensor([[0.3369]])\n",
      "Next State : [[-0.99168605]\n",
      " [ 0.12868111]\n",
      " [ 7.463265  ]]\n",
      "Reward : [-11.93916646]\n",
      "--------------------------------------------------\n",
      "Step : 12\n",
      "Current State : [-0.96934074 -0.24572042  7.546035  ]\n",
      "Currrent Action : tensor([[-0.0916]])\n",
      "Next State : [[-0.96934074]\n",
      " [-0.24572042]\n",
      " [ 7.546035  ]]\n",
      "Reward : [-14.6455208]\n",
      "--------------------------------------------------\n",
      "Step : 13\n",
      "Current State : [-0.8216663 -0.5699689  7.1641026]\n",
      "Currrent Action : tensor([[-1.3176]])\n",
      "Next State : [[-0.8216663]\n",
      " [-0.5699689]\n",
      " [ 7.1641026]]\n",
      "Reward : [-14.06735791]\n",
      "--------------------------------------------------\n",
      "Step : 14\n",
      "Current State : [-0.5991965 -0.800602   6.436626 ]\n",
      "Currrent Action : tensor([[-2.2275]])\n",
      "Next State : [[-0.5991965]\n",
      " [-0.800602 ]\n",
      " [ 6.436626 ]]\n",
      "Reward : [-11.56329373]\n",
      "--------------------------------------------------\n",
      "Step : 15\n",
      "Current State : [-0.34888348 -0.9371661   5.7223525 ]\n",
      "Currrent Action : tensor([[-0.7588]])\n",
      "Next State : [[-0.34888348]\n",
      " [-0.9371661 ]\n",
      " [ 5.7223525 ]]\n",
      "Reward : [-9.04225876]\n",
      "--------------------------------------------------\n",
      "Step : 16\n",
      "Current State : [-0.10337684 -0.99464226  5.056352  ]\n",
      "Currrent Action : tensor([[0.2458]])\n",
      "Next State : [[-0.10337684]\n",
      " [-0.99464226]\n",
      " [ 5.056352  ]]\n",
      "Reward : [-6.98859877]\n",
      "--------------------------------------------------\n",
      "Step : 17\n",
      "Current State : [ 0.1030466  -0.99467653  4.135834  ]\n",
      "Currrent Action : tensor([[-1.1636]])\n",
      "Next State : [[ 0.1030466 ]\n",
      " [-0.99467653]\n",
      " [ 4.135834  ]]\n",
      "Reward : [-5.36149875]\n",
      "--------------------------------------------------\n",
      "Step : 18\n",
      "Current State : [ 0.2829632  -0.95913076  3.6730466 ]\n",
      "Currrent Action : tensor([[1.8881]])\n",
      "Next State : [[ 0.2829632 ]\n",
      " [-0.95913076]\n",
      " [ 3.6730466 ]]\n",
      "Reward : [-3.86782874]\n",
      "--------------------------------------------------\n",
      "Step : 19\n",
      "Current State : [ 0.42000502 -0.9075218   2.9313738 ]\n",
      "Currrent Action : tensor([[-0.1488]])\n",
      "Next State : [[ 0.42000502]\n",
      " [-0.9075218 ]\n",
      " [ 2.9313738 ]]\n",
      "Reward : [-2.99758484]\n",
      "--------------------------------------------------\n",
      "Step : 20\n",
      "Current State : [ 0.5301094  -0.84792924  2.505575  ]\n",
      "Currrent Action : tensor([[1.6989]])\n",
      "Next State : [[ 0.5301094 ]\n",
      " [-0.84792924]\n",
      " [ 2.505575  ]]\n",
      "Reward : [-2.15573637]\n",
      "--------------------------------------------------\n",
      "Step : 21\n",
      "Current State : [ 0.6073621  -0.79442513  1.8801252 ]\n",
      "Currrent Action : tensor([[0.0700]])\n",
      "Next State : [[ 0.6073621 ]\n",
      " [-0.79442513]\n",
      " [ 1.8801252 ]]\n",
      "Reward : [-1.65207448]\n",
      "--------------------------------------------------\n",
      "Step : 22\n",
      "Current State : [ 0.665166  -0.7466955  1.4996055]\n",
      "Currrent Action : tensor([[1.4353]])\n",
      "Next State : [[ 0.665166 ]\n",
      " [-0.7466955]\n",
      " [ 1.4996055]]\n",
      "Reward : [-1.19838225]\n",
      "--------------------------------------------------\n",
      "Step : 23\n",
      "Current State : [ 0.69550115 -0.718525    0.8280193 ]\n",
      "Currrent Action : tensor([[-0.7438]])\n",
      "Next State : [[ 0.69550115]\n",
      " [-0.718525  ]\n",
      " [ 0.8280193 ]]\n",
      "Reward : [-0.93621907]\n",
      "--------------------------------------------------\n",
      "Step : 24\n",
      "Current State : [ 0.7057277 -0.7084832  0.2866523]\n",
      "Currrent Action : tensor([[-0.0165]])\n",
      "Next State : [[ 0.7057277]\n",
      " [-0.7084832]\n",
      " [ 0.2866523]]\n",
      "Reward : [-0.71125145]\n",
      "--------------------------------------------------\n",
      "Step : 25\n",
      "Current State : [ 0.70203763 -0.71213984 -0.10389943]\n",
      "Currrent Action : tensor([[0.9387]])\n",
      "Next State : [[ 0.70203763]\n",
      " [-0.71213984]\n",
      " [-0.10389943]]\n",
      "Reward : [-0.62901287]\n",
      "--------------------------------------------------\n",
      "Step : 26\n",
      "Current State : [ 0.67459166 -0.7381911  -0.7568672 ]\n",
      "Currrent Action : tensor([[-0.7924]])\n",
      "Next State : [[ 0.67459166]\n",
      " [-0.7381911 ]\n",
      " [-0.7568672 ]]\n",
      "Reward : [-0.6298296]\n",
      "--------------------------------------------------\n",
      "Step : 27\n",
      "Current State : [ 0.61654   -0.7873236 -1.5214179]\n",
      "Currrent Action : tensor([[-1.4060]])\n",
      "Next State : [[ 0.61654  ]\n",
      " [-0.7873236]\n",
      " [-1.5214179]]\n",
      "Reward : [-0.74880091]\n",
      "--------------------------------------------------\n",
      "Step : 28\n",
      "Current State : [ 0.5266269 -0.8500965 -2.1942549]\n",
      "Currrent Action : tensor([[-0.5490]])\n",
      "Next State : [[ 0.5266269]\n",
      " [-0.8500965]\n",
      " [-2.1942549]]\n",
      "Reward : [-1.05343476]\n",
      "--------------------------------------------------\n",
      "Step : 29\n",
      "Current State : [ 0.39952594 -0.9167219  -2.872562  ]\n",
      "Currrent Action : tensor([[-0.2716]])\n",
      "Next State : [[ 0.39952594]\n",
      " [-0.9167219 ]\n",
      " [-2.872562  ]]\n",
      "Reward : [-1.51414775]\n",
      "--------------------------------------------------\n",
      "Step : 30\n",
      "Current State : [ 0.23340595 -0.9723794  -3.5084147 ]\n",
      "Currrent Action : tensor([[0.3446]])\n",
      "Next State : [[ 0.23340595]\n",
      " [-0.9723794 ]\n",
      " [-3.5084147 ]]\n",
      "Reward : [-2.17040832]\n",
      "--------------------------------------------------\n",
      "Step : 31\n",
      "Current State : [ 0.0267227 -0.9996429 -4.1770606]\n",
      "Currrent Action : tensor([[0.4043]])\n",
      "Next State : [[ 0.0267227]\n",
      " [-0.9996429]\n",
      " [-4.1770606]]\n",
      "Reward : [-3.01386638]\n",
      "--------------------------------------------------\n",
      "Step : 32\n",
      "Current State : [-0.21623749 -0.9763408  -4.8937    ]\n",
      "Currrent Action : tensor([[0.2206]])\n",
      "Next State : [[-0.21623749]\n",
      " [-0.9763408 ]\n",
      " [-4.8937    ]]\n",
      "Reward : [-4.1289858]\n",
      "--------------------------------------------------\n",
      "Step : 33\n",
      "Current State : [-0.48335686 -0.87542343 -5.7305226 ]\n",
      "Currrent Action : tensor([[-0.6971]])\n",
      "Next State : [[-0.48335686]\n",
      " [-0.87542343]\n",
      " [-5.7305226 ]]\n",
      "Reward : [-5.59496196]\n",
      "--------------------------------------------------\n",
      "Step : 34\n",
      "Current State : [-0.7321967 -0.6810933 -6.3411274]\n",
      "Currrent Action : tensor([[0.3064]])\n",
      "Next State : [[-0.7321967]\n",
      " [-0.6810933]\n",
      " [-6.3411274]]\n",
      "Reward : [-7.59077623]\n",
      "--------------------------------------------------\n",
      "Step : 35\n",
      "Current State : [-0.9173191  -0.39815277 -6.795049  ]\n",
      "Currrent Action : tensor([[0.3793]])\n",
      "Next State : [[-0.9173191 ]\n",
      " [-0.39815277]\n",
      " [-6.795049  ]]\n",
      "Reward : [-9.74441454]\n",
      "--------------------------------------------------\n",
      "Step : 36\n",
      "Current State : [-0.9976223  -0.06891797 -6.810592  ]\n",
      "Currrent Action : tensor([[1.8871]])\n",
      "Next State : [[-0.9976223 ]\n",
      " [-0.06891797]\n",
      " [-6.810592  ]]\n",
      "Reward : [-12.08514887]\n",
      "--------------------------------------------------\n",
      "Step : 37\n",
      "Current State : [-0.9650784   0.26196107 -6.6805263 ]\n",
      "Currrent Action : tensor([[1.2117]])\n",
      "Next State : [[-0.9650784 ]\n",
      " [ 0.26196107]\n",
      " [-6.6805263 ]]\n",
      "Reward : [-14.08087829]\n",
      "--------------------------------------------------\n",
      "Step : 38\n",
      "Current State : [-0.8365312  0.5479192 -6.2964234]\n",
      "Currrent Action : tensor([[1.2509]])\n",
      "Next State : [[-0.8365312]\n",
      " [ 0.5479192]\n",
      " [-6.2964234]]\n",
      "Reward : [-12.73898436]\n",
      "--------------------------------------------------\n",
      "Step : 39\n",
      "Current State : [-0.64002645  0.76835287 -5.927776  ]\n",
      "Currrent Action : tensor([[-0.2819]])\n",
      "Next State : [[-0.64002645]\n",
      " [ 0.76835287]\n",
      " [-5.927776  ]]\n",
      "Reward : [-10.5269724]\n",
      "--------------------------------------------------\n",
      "Step : 40\n",
      "Current State : [-0.40484634  0.9143847  -5.554438  ]\n",
      "Currrent Action : tensor([[-1.3528]])\n",
      "Next State : [[-0.40484634]\n",
      " [ 0.9143847 ]\n",
      " [-5.554438  ]]\n",
      "Reward : [-8.64739856]\n",
      "--------------------------------------------------\n",
      "Step : 41\n",
      "Current State : [-0.17575489  0.98443395 -4.802765  ]\n",
      "Currrent Action : tensor([[0.4392]])\n",
      "Next State : [[-0.17575489]\n",
      " [ 0.98443395]\n",
      " [-4.802765  ]]\n",
      "Reward : [-7.03595329]\n",
      "--------------------------------------------------\n",
      "Step : 42\n",
      "Current State : [ 0.01154917  0.9999333  -3.7644393 ]\n",
      "Currrent Action : tensor([[2.3356]])\n",
      "Next State : [[ 0.01154917]\n",
      " [ 0.9999333 ]\n",
      " [-3.7644393 ]]\n",
      "Reward : [-5.36430237]\n",
      "--------------------------------------------------\n",
      "Step : 43\n",
      "Current State : [ 0.16323693  0.98658687 -3.0484257 ]\n",
      "Currrent Action : tensor([[-0.2262]])\n",
      "Next State : [[ 0.16323693]\n",
      " [ 0.98658687]\n",
      " [-3.0484257 ]]\n",
      "Reward : [-3.8484024]\n",
      "--------------------------------------------------\n",
      "Step : 44\n",
      "Current State : [ 0.27857596  0.9604142  -2.3668067 ]\n",
      "Currrent Action : tensor([[-0.3888]])\n",
      "Next State : [[ 0.27857596]\n",
      " [ 0.9604142 ]\n",
      " [-2.3668067 ]]\n",
      "Reward : [-2.9085994]\n",
      "--------------------------------------------------\n",
      "Step : 45\n",
      "Current State : [ 0.36946547  0.92924446 -1.9224538 ]\n",
      "Currrent Action : tensor([[-1.8397]])\n",
      "Next State : [[ 0.36946547]\n",
      " [ 0.92924446]\n",
      " [-1.9224538 ]]\n",
      "Reward : [-2.22375627]\n",
      "--------------------------------------------------\n",
      "Step : 46\n",
      "Current State : [ 0.41757324  0.90864325 -1.046784  ]\n",
      "Currrent Action : tensor([[1.1916]])\n",
      "Next State : [[ 0.41757324]\n",
      " [ 0.90864325]\n",
      " [-1.046784  ]]\n",
      "Reward : [-1.79273125]\n",
      "--------------------------------------------------\n",
      "Step : 47\n",
      "Current State : [ 0.4342862   0.90077496 -0.36945513]\n",
      "Currrent Action : tensor([[-0.0277]])\n",
      "Next State : [[ 0.4342862 ]\n",
      " [ 0.90077496]\n",
      " [-0.36945513]]\n",
      "Reward : [-1.40922977]\n",
      "--------------------------------------------------\n",
      "Step : 48\n",
      "Current State : [0.4166052  0.9090875  0.39075726]\n",
      "Currrent Action : tensor([[0.5642]])\n",
      "Next State : [[0.4166052 ]\n",
      " [0.9090875 ]\n",
      " [0.39075726]]\n",
      "Reward : [-1.27184386]\n",
      "--------------------------------------------------\n",
      "Step : 49\n",
      "Current State : [0.37115124 0.9285724  0.98918664]\n",
      "Currrent Action : tensor([[-0.5559]])\n",
      "Next State : [[0.37115124]\n",
      " [0.9285724 ]\n",
      " [0.98918664]]\n",
      "Reward : [-1.31766111]\n",
      "--------------------------------------------------\n",
      "Step : 50\n",
      "Current State : [0.2925946 0.9562366 1.6661886]\n",
      "Currrent Action : tensor([[-0.1295]])\n",
      "Next State : [[0.2925946]\n",
      " [0.9562366]\n",
      " [1.6661886]]\n",
      "Reward : [-1.51526992]\n",
      "--------------------------------------------------\n",
      "Step : 51\n",
      "Current State : [0.17597234 0.9843951  2.4009125 ]\n",
      "Currrent Action : tensor([[0.1170]])\n",
      "Next State : [[0.17597234]\n",
      " [0.9843951 ]\n",
      " [2.4009125 ]]\n",
      "Reward : [-1.90034445]\n",
      "--------------------------------------------------\n",
      "Step : 52\n",
      "Current State : [0.01716642 0.99985266 3.1945236 ]\n",
      "Currrent Action : tensor([[0.3688]])\n",
      "Next State : [[0.01716642]\n",
      " [0.99985266]\n",
      " [3.1945236 ]]\n",
      "Reward : [-2.5195394]\n",
      "--------------------------------------------------\n",
      "Step : 53\n",
      "Current State : [-0.18305823  0.9831021   4.0252724 ]\n",
      "Currrent Action : tensor([[0.5391]])\n",
      "Next State : [[-0.18305823]\n",
      " [ 0.9831021 ]\n",
      " [ 4.0252724 ]]\n",
      "Reward : [-3.43455196]\n",
      "--------------------------------------------------\n",
      "Step : 54\n",
      "Current State : [-0.40837395  0.91281474  4.731513  ]\n",
      "Currrent Action : tensor([[-0.2072]])\n",
      "Next State : [[-0.40837395]\n",
      " [ 0.91281474]\n",
      " [ 4.731513  ]]\n",
      "Reward : [-4.69997305]\n",
      "--------------------------------------------------\n",
      "Step : 55\n",
      "Current State : [-0.63737065  0.7705574   5.4081826 ]\n",
      "Currrent Action : tensor([[-0.0529]])\n",
      "Next State : [[-0.63737065]\n",
      " [ 0.7705574 ]\n",
      " [ 5.4081826 ]]\n",
      "Reward : [-6.20467025]\n",
      "--------------------------------------------------\n",
      "Step : 56\n",
      "Current State : [-0.8416198   0.54007053  6.1838856 ]\n",
      "Currrent Action : tensor([[1.3186]])\n",
      "Next State : [[-0.8416198 ]\n",
      " [ 0.54007053]\n",
      " [ 6.1838856 ]]\n",
      "Reward : [-8.0426723]\n",
      "--------------------------------------------------\n",
      "Step : 57\n",
      "Current State : [-0.9722601   0.23390216  6.6886334 ]\n",
      "Currrent Action : tensor([[0.6646]])\n",
      "Next State : [[-0.9722601 ]\n",
      " [ 0.23390216]\n",
      " [ 6.6886334 ]]\n",
      "Reward : [-10.43489583]\n",
      "--------------------------------------------------\n",
      "Step : 58\n",
      "Current State : [-0.9947116  -0.10270779  6.7795696 ]\n",
      "Currrent Action : tensor([[-0.5633]])\n",
      "Next State : [[-0.9947116 ]\n",
      " [-0.10270779]\n",
      " [ 6.7795696 ]]\n",
      "Reward : [-12.91604878]\n",
      "--------------------------------------------------\n",
      "Step : 59\n",
      "Current State : [-0.9069537  -0.42123038  6.638245  ]\n",
      "Currrent Action : tensor([[-0.4286]])\n",
      "Next State : [[-0.9069537 ]\n",
      " [-0.42123038]\n",
      " [ 6.638245  ]]\n",
      "Reward : [-13.83015841]\n",
      "--------------------------------------------------\n",
      "Step : 60\n",
      "Current State : [-0.7207466 -0.6931986  6.6223226]\n",
      "Currrent Action : tensor([[2.5779]])\n",
      "Next State : [[-0.7207466]\n",
      " [-0.6931986]\n",
      " [ 6.6223226]]\n",
      "Reward : [-11.73734832]\n",
      "--------------------------------------------------\n",
      "Step : 61\n",
      "Current State : [-0.47585174 -0.8795255   6.1789303 ]\n",
      "Currrent Action : tensor([[0.5100]])\n",
      "Next State : [[-0.47585174]\n",
      " [-0.8795255 ]\n",
      " [ 6.1789303 ]]\n",
      "Reward : [-10.02960747]\n",
      "--------------------------------------------------\n",
      "Step : 62\n",
      "Current State : [-0.21096914 -0.97749275  5.667318  ]\n",
      "Currrent Action : tensor([[0.9869]])\n",
      "Next State : [[-0.21096914]\n",
      " [-0.97749275]\n",
      " [ 5.667318  ]]\n",
      "Reward : [-8.09025875]\n",
      "--------------------------------------------------\n",
      "Step : 63\n",
      "Current State : [ 0.01914244 -0.9998168   4.6341987 ]\n",
      "Currrent Action : tensor([[-2.3169]])\n",
      "Next State : [[ 0.01914244]\n",
      " [-0.9998168 ]\n",
      " [ 4.6341987 ]]\n",
      "Reward : [-6.3962316]\n",
      "--------------------------------------------------\n",
      "Step : 64\n",
      "Current State : [ 0.21243288 -0.97717565  3.8984075 ]\n",
      "Currrent Action : tensor([[0.0938]])\n",
      "Next State : [[ 0.21243288]\n",
      " [-0.97717565]\n",
      " [ 3.8984075 ]]\n",
      "Reward : [-4.55521446]\n",
      "--------------------------------------------------\n",
      "Step : 65\n",
      "Current State : [ 0.36074105 -0.932666    3.0999665 ]\n",
      "Currrent Action : tensor([[-0.4371]])\n",
      "Next State : [[ 0.36074105]\n",
      " [-0.932666  ]\n",
      " [ 3.0999665 ]]\n",
      "Reward : [-3.36067173]\n",
      "--------------------------------------------------\n",
      "Step : 66\n",
      "Current State : [ 0.4648479 -0.8853906  2.28801  ]\n",
      "Currrent Action : tensor([[-0.7497]])\n",
      "Next State : [[ 0.4648479]\n",
      " [-0.8853906]\n",
      " [ 2.28801  ]]\n",
      "Reward : [-2.40570592]\n",
      "--------------------------------------------------\n",
      "Step : 67\n",
      "Current State : [ 0.5400057 -0.8416614  1.739621 ]\n",
      "Currrent Action : tensor([[0.7710]])\n",
      "Next State : [[ 0.5400057]\n",
      " [-0.8416614]\n",
      " [ 1.739621 ]]\n",
      "Reward : [-1.70638759]\n",
      "--------------------------------------------------\n",
      "Step : 68\n",
      "Current State : [ 0.59485364 -0.803834    1.332794  ]\n",
      "Currrent Action : tensor([[1.4961]])\n",
      "Next State : [[ 0.59485364]\n",
      " [-0.803834  ]\n",
      " [ 1.332794  ]]\n",
      "Reward : [-1.30557161]\n",
      "--------------------------------------------------\n",
      "Step : 69\n",
      "Current State : [ 0.6158061  -0.78789777  0.5265017 ]\n",
      "Currrent Action : tensor([[-1.3561]])\n",
      "Next State : [[ 0.6158061 ]\n",
      " [-0.78789777]\n",
      " [ 0.5265017 ]]\n",
      "Reward : [-1.05129255]\n",
      "--------------------------------------------------\n",
      "Step : 70\n",
      "Current State : [ 0.6068006  -0.7948541  -0.22758788]\n",
      "Currrent Action : tensor([[-1.0878]])\n",
      "Next State : [[ 0.6068006 ]\n",
      " [-0.7948541 ]\n",
      " [-0.22758788]]\n",
      "Reward : [-0.85225608]\n",
      "--------------------------------------------------\n",
      "Step : 71\n",
      "Current State : [ 0.5819045  -0.81325716 -0.6192147 ]\n",
      "Currrent Action : tensor([[1.3634]])\n",
      "Next State : [[ 0.5819045 ]\n",
      " [-0.81325716]\n",
      " [-0.6192147 ]]\n",
      "Reward : [-0.85117151]\n",
      "--------------------------------------------------\n",
      "Step : 72\n",
      "Current State : [ 0.5272266 -0.8497247 -1.3147026]\n",
      "Currrent Action : tensor([[-0.5703]])\n",
      "Next State : [[ 0.5272266]\n",
      " [-0.8497247]\n",
      " [-1.3147026]]\n",
      "Reward : [-0.94065087]\n",
      "--------------------------------------------------\n",
      "Step : 73\n",
      "Current State : [ 0.43929675 -0.898342   -2.0103538 ]\n",
      "Currrent Action : tensor([[-0.3891]])\n",
      "Next State : [[ 0.43929675]\n",
      " [-0.898342  ]\n",
      " [-2.0103538 ]]\n",
      "Reward : [-1.20416068]\n",
      "--------------------------------------------------\n",
      "Step : 74\n",
      "Current State : [ 0.31751415 -0.9482535  -2.6341772 ]\n",
      "Currrent Action : tensor([[0.3329]])\n",
      "Next State : [[ 0.31751415]\n",
      " [-0.9482535 ]\n",
      " [-2.6341772 ]]\n",
      "Reward : [-1.64967587]\n",
      "--------------------------------------------------\n",
      "Step : 75\n",
      "Current State : [ 0.14750323 -0.9890616  -3.5012684 ]\n",
      "Currrent Action : tensor([[-1.0393]])\n",
      "Next State : [[ 0.14750323]\n",
      " [-0.9890616 ]\n",
      " [-3.5012684 ]]\n",
      "Reward : [-2.2516983]\n",
      "--------------------------------------------------\n",
      "Step : 76\n",
      "Current State : [-0.07355265 -0.9972913  -4.433251  ]\n",
      "Currrent Action : tensor([[-1.2679]])\n",
      "Next State : [[-0.07355265]\n",
      " [-0.9972913 ]\n",
      " [-4.433251  ]]\n",
      "Reward : [-3.25172146]\n",
      "--------------------------------------------------\n",
      "Step : 77\n",
      "Current State : [-0.314076   -0.94939786 -4.917282  ]\n",
      "Currrent Action : tensor([[1.7596]])\n",
      "Next State : [[-0.314076  ]\n",
      " [-0.94939786]\n",
      " [-4.917282  ]]\n",
      "Reward : [-4.67256959]\n",
      "--------------------------------------------------\n",
      "Step : 78\n",
      "Current State : [-0.5551836 -0.8317278 -5.3820086]\n",
      "Currrent Action : tensor([[1.6488]])\n",
      "Next State : [[-0.5551836]\n",
      " [-0.8317278]\n",
      " [-5.3820086]]\n",
      "Reward : [-5.99384198]\n",
      "--------------------------------------------------\n",
      "Step : 79\n",
      "Current State : [-0.7710996 -0.6367145 -5.8396535]\n",
      "Currrent Action : tensor([[1.1077]])\n",
      "Next State : [[-0.7710996]\n",
      " [-0.6367145]\n",
      " [-5.8396535]]\n",
      "Reward : [-7.56075082]\n",
      "--------------------------------------------------\n",
      "Step : 80\n",
      "Current State : [-0.92846686 -0.37141532 -6.1939406 ]\n",
      "Currrent Action : tensor([[0.8217]])\n",
      "Next State : [[-0.92846686]\n",
      " [-0.37141532]\n",
      " [-6.1939406 ]]\n",
      "Reward : [-9.42000937]\n",
      "--------------------------------------------------\n",
      "Step : 81\n",
      "Current State : [-0.9983867  -0.05678109 -6.474426  ]\n",
      "Currrent Action : tensor([[-0.0128]])\n",
      "Next State : [[-0.9983867 ]\n",
      " [-0.05678109]\n",
      " [-6.474426  ]]\n",
      "Reward : [-11.45994117]\n",
      "--------------------------------------------------\n",
      "Step : 82\n",
      "Current State : [-0.9638534   0.26643318 -6.5300436 ]\n",
      "Currrent Action : tensor([[-0.0869]])\n",
      "Next State : [[-0.9638534 ]\n",
      " [ 0.26643318]\n",
      " [-6.5300436 ]]\n",
      "Reward : [-13.70770021]\n",
      "--------------------------------------------------\n",
      "Step : 83\n",
      "Current State : [-0.83780265  0.5459731  -6.1571937 ]\n",
      "Currrent Action : tensor([[1.1535]])\n",
      "Next State : [[-0.83780265]\n",
      " [ 0.5459731 ]\n",
      " [-6.1571937 ]]\n",
      "Reward : [-12.51329938]\n",
      "--------------------------------------------------\n",
      "Step : 84\n",
      "Current State : [-0.66003126  0.7512381  -5.447714  ]\n",
      "Currrent Action : tensor([[3.1623]])\n",
      "Next State : [[-0.66003126]\n",
      " [ 0.7512381 ]\n",
      " [-5.447714  ]]\n",
      "Reward : [-10.36941703]\n",
      "--------------------------------------------------\n",
      "Step : 85\n",
      "Current State : [-0.471394   0.8819227 -4.599793 ]\n",
      "Currrent Action : tensor([[1.8966]])\n",
      "Next State : [[-0.471394 ]\n",
      " [ 0.8819227]\n",
      " [-4.599793 ]]\n",
      "Reward : [-8.22304637]\n",
      "--------------------------------------------------\n",
      "Step : 86\n",
      "Current State : [-0.29178628  0.95648354 -3.895538  ]\n",
      "Currrent Action : tensor([[0.2854]])\n",
      "Next State : [[-0.29178628]\n",
      " [ 0.95648354]\n",
      " [-3.895538  ]]\n",
      "Reward : [-6.36636225]\n",
      "--------------------------------------------------\n",
      "Step : 87\n",
      "Current State : [-0.13276292  0.9911478  -3.2587557 ]\n",
      "Currrent Action : tensor([[-0.5372]])\n",
      "Next State : [[-0.13276292]\n",
      " [ 0.9911478 ]\n",
      " [-3.2587557 ]]\n",
      "Reward : [-5.00308925]\n",
      "--------------------------------------------------\n",
      "Step : 88\n",
      "Current State : [ 0.00570461  0.9999837  -2.777214  ]\n",
      "Currrent Action : tensor([[-1.7455]])\n",
      "Next State : [[ 0.00570461]\n",
      " [ 0.9999837 ]\n",
      " [-2.777214  ]]\n",
      "Reward : [-3.96844924]\n",
      "--------------------------------------------------\n",
      "Step : 89\n",
      "Current State : [ 0.10088899  0.99489766 -1.9071258 ]\n",
      "Currrent Action : tensor([[0.8007]])\n",
      "Next State : [[ 0.10088899]\n",
      " [ 0.99489766]\n",
      " [-1.9071258 ]]\n",
      "Reward : [-3.22144482]\n",
      "--------------------------------------------------\n",
      "Step : 90\n",
      "Current State : [ 0.14361028  0.98963434 -0.86095256]\n",
      "Currrent Action : tensor([[2.5653]])\n",
      "Next State : [[ 0.14361028]\n",
      " [ 0.98963434]\n",
      " [-0.86095256]]\n",
      "Reward : [-2.52783503]\n",
      "--------------------------------------------------\n",
      "Step : 91\n",
      "Current State : [ 0.14677788  0.9891695  -0.06403047]\n",
      "Currrent Action : tensor([[0.3646]])\n",
      "Next State : [[ 0.14677788]\n",
      " [ 0.9891695 ]\n",
      " [-0.06403047]]\n",
      "Reward : [-2.10969489]\n",
      "--------------------------------------------------\n",
      "Step : 92\n",
      "Current State : [0.11827575 0.9929808  0.57513636]\n",
      "Currrent Action : tensor([[-0.6847]])\n",
      "Next State : [[0.11827575]\n",
      " [0.9929808 ]\n",
      " [0.57513636]]\n",
      "Reward : [-2.02719195]\n",
      "--------------------------------------------------\n",
      "Step : 93\n",
      "Current State : [0.05450804 0.99851334 1.2803639 ]\n",
      "Currrent Action : tensor([[-0.2634]])\n",
      "Next State : [[0.05450804]\n",
      " [0.99851334]\n",
      " [1.2803639 ]]\n",
      "Reward : [-2.14215748]\n",
      "--------------------------------------------------\n",
      "Step : 94\n",
      "Current State : [-0.05038328  0.99872994  2.0987937 ]\n",
      "Currrent Action : tensor([[0.4636]])\n",
      "Next State : [[-0.05038328]\n",
      " [ 0.99872994]\n",
      " [ 2.0987937 ]]\n",
      "Reward : [-2.46319634]\n",
      "--------------------------------------------------\n",
      "Step : 95\n",
      "Current State : [-0.18720654  0.98232055  2.7582603 ]\n",
      "Currrent Action : tensor([[-0.5972]])\n",
      "Next State : [[-0.18720654]\n",
      " [ 0.98232055]\n",
      " [ 2.7582603 ]]\n",
      "Reward : [-3.06914266]\n",
      "--------------------------------------------------\n",
      "Step : 96\n",
      "Current State : [-0.3614956  0.9323738  3.6310768]\n",
      "Currrent Action : tensor([[0.9072]])\n",
      "Next State : [[-0.3614956]\n",
      " [ 0.9323738]\n",
      " [ 3.6310768]]\n",
      "Reward : [-3.85610488]\n",
      "--------------------------------------------------\n",
      "Step : 97\n",
      "Current State : [-0.5491599  0.8357173  4.229743 ]\n",
      "Currrent Action : tensor([[-0.6708]])\n",
      "Next State : [[-0.5491599]\n",
      " [ 0.8357173]\n",
      " [ 4.229743 ]]\n",
      "Reward : [-5.08511337]\n",
      "--------------------------------------------------\n",
      "Step : 98\n",
      "Current State : [-0.735091   0.6779685  4.888845 ]\n",
      "Currrent Action : tensor([[0.2154]])\n",
      "Next State : [[-0.735091 ]\n",
      " [ 0.6779685]\n",
      " [ 4.888845 ]]\n",
      "Reward : [-6.42088988]\n",
      "--------------------------------------------------\n",
      "Step : 99\n",
      "Current State : [-0.8897892   0.45637172  5.4216447 ]\n",
      "Currrent Action : tensor([[0.1622]])\n",
      "Next State : [[-0.8897892 ]\n",
      " [ 0.45637172]\n",
      " [ 5.4216447 ]]\n",
      "Reward : [-8.13378478]\n",
      "learning iteration : 9\n",
      "--------------------------------------------------\n",
      "Step : 0\n",
      "Current State : [0.70357925 0.71061677 0.23510213]\n",
      "Currrent Action : tensor([[1.1076]])\n",
      "Next State : [[0.70357925]\n",
      " [0.71061677]\n",
      " [0.23510213]]\n",
      "Reward : [-0.6284286]\n",
      "--------------------------------------------------\n",
      "Step : 1\n",
      "Current State : [0.6809001  0.73237634 0.6286189 ]\n",
      "Currrent Action : tensor([[-0.9296]])\n",
      "Next State : [[0.6809001 ]\n",
      " [0.73237634]\n",
      " [0.6286189 ]]\n",
      "Reward : [-0.63108331]\n",
      "--------------------------------------------------\n",
      "Step : 2\n",
      "Current State : [0.6433571  0.76556623 1.0023123 ]\n",
      "Currrent Action : tensor([[-1.1706]])\n",
      "Next State : [[0.6433571 ]\n",
      " [0.76556623]\n",
      " [1.0023123 ]]\n",
      "Reward : [-0.71625057]\n",
      "--------------------------------------------------\n",
      "Step : 3\n",
      "Current State : [0.5802031  0.81447184 1.597945  ]\n",
      "Currrent Action : tensor([[0.1431]])\n",
      "Next State : [[0.5802031 ]\n",
      " [0.81447184]\n",
      " [1.597945  ]]\n",
      "Reward : [-0.8607297]\n",
      "--------------------------------------------------\n",
      "Step : 4\n",
      "Current State : [0.4858126 0.874063  2.233709 ]\n",
      "Currrent Action : tensor([[0.1661]])\n",
      "Next State : [[0.4858126]\n",
      " [0.874063 ]\n",
      " [2.233709 ]]\n",
      "Reward : [-1.16132841]\n",
      "--------------------------------------------------\n",
      "Step : 5\n",
      "Current State : [0.36188033 0.9322246  2.7401695 ]\n",
      "Currrent Action : tensor([[-0.9939]])\n",
      "Next State : [[0.36188033]\n",
      " [0.9322246 ]\n",
      " [2.7401695 ]]\n",
      "Reward : [-1.63097366]\n",
      "--------------------------------------------------\n",
      "Step : 6\n",
      "Current State : [0.198047   0.98019254 3.4183817 ]\n",
      "Currrent Action : tensor([[-0.1397]])\n",
      "Next State : [[0.198047  ]\n",
      " [0.98019254]\n",
      " [3.4183817 ]]\n",
      "Reward : [-2.1921019]\n",
      "--------------------------------------------------\n",
      "Step : 7\n",
      "Current State : [-4.3326771e-04  9.9999988e-01  3.9959664e+00]\n",
      "Currrent Action : tensor([[-1.0504]])\n",
      "Next State : [[-4.3326771e-04]\n",
      " [ 9.9999988e-01]\n",
      " [ 3.9959664e+00]]\n",
      "Reward : [-3.05046046]\n",
      "--------------------------------------------------\n",
      "Step : 8\n",
      "Current State : [-0.23288932  0.97250324  4.692288  ]\n",
      "Currrent Action : tensor([[-0.3579]])\n",
      "Next State : [[-0.23288932]\n",
      " [ 0.97250324]\n",
      " [ 4.692288  ]]\n",
      "Reward : [-4.06566519]\n",
      "--------------------------------------------------\n",
      "Step : 9\n",
      "Current State : [-0.4816578  0.8763594  5.349955 ]\n",
      "Currrent Action : tensor([[-0.4781]])\n",
      "Next State : [[-0.4816578]\n",
      " [ 0.8763594]\n",
      " [ 5.349955 ]]\n",
      "Reward : [-5.46305746]\n",
      "--------------------------------------------------\n",
      "Step : 10\n",
      "Current State : [-0.7252584   0.68847674  6.177285  ]\n",
      "Currrent Action : tensor([[1.1337]])\n",
      "Next State : [[-0.7252584 ]\n",
      " [ 0.68847674]\n",
      " [ 6.177285  ]]\n",
      "Reward : [-7.16223342]\n",
      "--------------------------------------------------\n",
      "Step : 11\n",
      "Current State : [-0.9172386  0.3983383  6.993643 ]\n",
      "Currrent Action : tensor([[2.0818]])\n",
      "Next State : [[-0.9172386]\n",
      " [ 0.3983383]\n",
      " [ 6.993643 ]]\n",
      "Reward : [-9.494791]\n",
      "--------------------------------------------------\n",
      "Step : 12\n",
      "Current State : [-0.9992589   0.03849214  7.424057  ]\n",
      "Currrent Action : tensor([[0.8777]])\n",
      "Next State : [[-0.9992589 ]\n",
      " [ 0.03849214]\n",
      " [ 7.424057  ]]\n",
      "Reward : [-12.35508754]\n",
      "--------------------------------------------------\n",
      "Step : 13\n",
      "Current State : [-0.9415789  -0.33679247  7.6401987 ]\n",
      "Currrent Action : tensor([[1.2485]])\n",
      "Next State : [[-0.9415789 ]\n",
      " [-0.33679247]\n",
      " [ 7.6401987 ]]\n",
      "Reward : [-15.14239452]\n",
      "--------------------------------------------------\n",
      "Step : 14\n",
      "Current State : [-0.7519207  -0.65925354  7.526346  ]\n",
      "Currrent Action : tensor([[0.9249]])\n",
      "Next State : [[-0.7519207 ]\n",
      " [-0.65925354]\n",
      " [ 7.526346  ]]\n",
      "Reward : [-13.66739501]\n",
      "--------------------------------------------------\n",
      "Step : 15\n",
      "Current State : [-0.48105717 -0.87668926  6.9822116 ]\n",
      "Currrent Action : tensor([[-0.3313]])\n",
      "Next State : [[-0.48105717]\n",
      " [-0.87668926]\n",
      " [ 6.9822116 ]]\n",
      "Reward : [-11.52965402]\n",
      "--------------------------------------------------\n",
      "Step : 16\n",
      "Current State : [-0.19260004 -0.98127735  6.160981  ]\n",
      "Currrent Action : tensor([[-1.0914]])\n",
      "Next State : [[-0.19260004]\n",
      " [-0.98127735]\n",
      " [ 6.160981  ]]\n",
      "Reward : [-9.17222378]\n",
      "--------------------------------------------------\n",
      "Step : 17\n",
      "Current State : [ 0.07567696 -0.99713236  5.3912096 ]\n",
      "Currrent Action : tensor([[-0.2254]])\n",
      "Next State : [[ 0.07567696]\n",
      " [-0.99713236]\n",
      " [ 5.3912096 ]]\n",
      "Reward : [-6.90965941]\n",
      "--------------------------------------------------\n",
      "Step : 18\n",
      "Current State : [ 0.30785507 -0.95143324  4.743768  ]\n",
      "Currrent Action : tensor([[0.6694]])\n",
      "Next State : [[ 0.30785507]\n",
      " [-0.95143324]\n",
      " [ 4.743768  ]]\n",
      "Reward : [-5.14212762]\n",
      "--------------------------------------------------\n",
      "Step : 19\n",
      "Current State : [ 0.49866608 -0.86679417  4.182428  ]\n",
      "Currrent Action : tensor([[1.0149]])\n",
      "Next State : [[ 0.49866608]\n",
      " [-0.86679417]\n",
      " [ 4.182428  ]]\n",
      "Reward : [-3.83357182]\n",
      "--------------------------------------------------\n",
      "Step : 20\n",
      "Current State : [ 0.6344683  -0.77294886  3.3052251 ]\n",
      "Currrent Action : tensor([[-1.5140]])\n",
      "Next State : [[ 0.6344683 ]\n",
      " [-0.77294886]\n",
      " [ 3.3052251 ]]\n",
      "Reward : [-2.85141236]\n",
      "--------------------------------------------------\n",
      "Step : 21\n",
      "Current State : [ 0.7374598 -0.675391   2.8396213]\n",
      "Currrent Action : tensor([[0.7607]])\n",
      "Next State : [[ 0.7374598]\n",
      " [-0.675391 ]\n",
      " [ 2.8396213]]\n",
      "Reward : [-1.87355963]\n",
      "--------------------------------------------------\n",
      "Step : 22\n",
      "Current State : [ 0.8091081  -0.58765984  2.2666261 ]\n",
      "Currrent Action : tensor([[-0.4430]])\n",
      "Next State : [[ 0.8091081 ]\n",
      " [-0.58765984]\n",
      " [ 2.2666261 ]]\n",
      "Reward : [-1.35635576]\n",
      "--------------------------------------------------\n",
      "Step : 23\n",
      "Current State : [ 0.8575889 -0.5143358  1.7586102]\n",
      "Currrent Action : tensor([[-0.4485]])\n",
      "Next State : [[ 0.8575889]\n",
      " [-0.5143358]\n",
      " [ 1.7586102]]\n",
      "Reward : [-0.90854991]\n",
      "--------------------------------------------------\n",
      "Step : 24\n",
      "Current State : [ 0.8858916 -0.4638923  1.1569831]\n",
      "Currrent Action : tensor([[-1.4392]])\n",
      "Next State : [[ 0.8858916]\n",
      " [-0.4638923]\n",
      " [ 1.1569831]]\n",
      "Reward : [-0.60319389]\n",
      "--------------------------------------------------\n",
      "Step : 25\n",
      "Current State : [ 0.9032213  -0.42917514  0.7760899 ]\n",
      "Currrent Action : tensor([[-0.2198]])\n",
      "Next State : [[ 0.9032213 ]\n",
      " [-0.42917514]\n",
      " [ 0.7760899 ]]\n",
      "Reward : [-0.36660351]\n",
      "--------------------------------------------------\n",
      "Step : 26\n",
      "Current State : [ 0.907296   -0.4204926   0.19182365]\n",
      "Currrent Action : tensor([[-1.7492]])\n",
      "Next State : [[ 0.907296  ]\n",
      " [-0.4204926 ]\n",
      " [ 0.19182365]]\n",
      "Reward : [-0.26005401]\n",
      "--------------------------------------------------\n",
      "Step : 27\n",
      "Current State : [ 0.90721256 -0.4206725  -0.00396657]\n",
      "Currrent Action : tensor([[0.7972]])\n",
      "Next State : [[ 0.90721256]\n",
      " [-0.4206725 ]\n",
      " [-0.00396657]]\n",
      "Reward : [-0.19266088]\n",
      "--------------------------------------------------\n",
      "Step : 28\n",
      "Current State : [ 0.8980411  -0.43991148 -0.4262725 ]\n",
      "Currrent Action : tensor([[-0.7120]])\n",
      "Next State : [[ 0.8980411 ]\n",
      " [-0.43991148]\n",
      " [-0.4262725 ]]\n",
      "Reward : [-0.18902644]\n",
      "--------------------------------------------------\n",
      "Step : 29\n",
      "Current State : [ 0.8809022  -0.47329837 -0.7506243 ]\n",
      "Currrent Action : tensor([[0.0372]])\n",
      "Next State : [[ 0.8809022 ]\n",
      " [-0.47329837]\n",
      " [-0.7506243 ]]\n",
      "Reward : [-0.22565257]\n",
      "--------------------------------------------------\n",
      "Step : 30\n",
      "Current State : [ 0.8611284 -0.5083875 -0.8055981]\n",
      "Currrent Action : tensor([[2.3670]])\n",
      "Next State : [[ 0.8611284]\n",
      " [-0.5083875]\n",
      " [-0.8055981]]\n",
      "Reward : [-0.30342359]\n",
      "--------------------------------------------------\n",
      "Step : 31\n",
      "Current State : [ 0.8266015  -0.56278765 -1.2888623 ]\n",
      "Currrent Action : tensor([[-0.6798]])\n",
      "Next State : [[ 0.8266015 ]\n",
      " [-0.56278765]\n",
      " [-1.2888623 ]]\n",
      "Reward : [-0.34978187]\n",
      "--------------------------------------------------\n",
      "Step : 32\n",
      "Current State : [ 0.7824675  -0.62269145 -1.4884669 ]\n",
      "Currrent Action : tensor([[1.4832]])\n",
      "Next State : [[ 0.7824675 ]\n",
      " [-0.62269145]\n",
      " [-1.4884669 ]]\n",
      "Reward : [-0.52562688]\n",
      "--------------------------------------------------\n",
      "Step : 33\n",
      "Current State : [ 0.71984345 -0.69413644 -1.9008379 ]\n",
      "Currrent Action : tensor([[0.3643]])\n",
      "Next State : [[ 0.71984345]\n",
      " [-0.69413644]\n",
      " [-1.9008379 ]]\n",
      "Reward : [-0.67350897]\n",
      "--------------------------------------------------\n",
      "Step : 34\n",
      "Current State : [ 0.6319772 -0.774987  -2.3894985]\n",
      "Currrent Action : tensor([[0.2129]])\n",
      "Next State : [[ 0.6319772]\n",
      " [-0.774987 ]\n",
      " [-2.3894985]]\n",
      "Reward : [-0.94998973]\n",
      "--------------------------------------------------\n",
      "Step : 35\n",
      "Current State : [ 0.5208374 -0.8536559 -2.7254033]\n",
      "Currrent Action : tensor([[1.6356]])\n",
      "Next State : [[ 0.5208374]\n",
      " [-0.8536559]\n",
      " [-2.7254033]]\n",
      "Reward : [-1.35987258]\n",
      "--------------------------------------------------\n",
      "Step : 36\n",
      "Current State : [ 0.38380137 -0.92341566 -3.0784438 ]\n",
      "Currrent Action : tensor([[1.9147]])\n",
      "Next State : [[ 0.38380137]\n",
      " [-0.92341566]\n",
      " [-3.0784438 ]]\n",
      "Reward : [-1.79290513]\n",
      "--------------------------------------------------\n",
      "Step : 37\n",
      "Current State : [ 0.21167731 -0.9773396  -3.612372  ]\n",
      "Currrent Action : tensor([[1.0576]])\n",
      "Next State : [[ 0.21167731]\n",
      " [-0.9773396 ]\n",
      " [-3.612372  ]]\n",
      "Reward : [-2.33386282]\n",
      "--------------------------------------------------\n",
      "Step : 38\n",
      "Current State : [-6.9109513e-04 -9.9999976e-01 -4.2796388e+00]\n",
      "Currrent Action : tensor([[0.4383]])\n",
      "Next State : [[-6.9109513e-04]\n",
      " [-9.9999976e-01]\n",
      " [-4.2796388e+00]]\n",
      "Reward : [-3.14793634]\n",
      "--------------------------------------------------\n",
      "Step : 39\n",
      "Current State : [-0.2530112 -0.9674634 -5.1020074]\n",
      "Currrent Action : tensor([[-0.4825]])\n",
      "Next State : [[-0.2530112]\n",
      " [-0.9674634]\n",
      " [-5.1020074]]\n",
      "Reward : [-4.30133625]\n",
      "--------------------------------------------------\n",
      "Step : 40\n",
      "Current State : [-0.5236818 -0.8519139 -5.9075117]\n",
      "Currrent Action : tensor([[-0.5327]])\n",
      "Next State : [[-0.5236818]\n",
      " [-0.8519139]\n",
      " [-5.9075117]]\n",
      "Reward : [-5.93975474]\n",
      "--------------------------------------------------\n",
      "Step : 41\n",
      "Current State : [-0.7692188  -0.63898546 -6.5290127 ]\n",
      "Currrent Action : tensor([[0.1162]])\n",
      "Next State : [[-0.7692188 ]\n",
      " [-0.63898546]\n",
      " [-6.5290127 ]]\n",
      "Reward : [-7.99261173]\n",
      "--------------------------------------------------\n",
      "Step : 42\n",
      "Current State : [-0.9422974 -0.334777  -7.036201 ]\n",
      "Currrent Action : tensor([[-0.1863]])\n",
      "Next State : [[-0.9422974]\n",
      " [-0.334777 ]\n",
      " [-7.036201 ]]\n",
      "Reward : [-10.2575668]\n",
      "--------------------------------------------------\n",
      "Step : 43\n",
      "Current State : [-0.99979377  0.02030835 -7.2335663 ]\n",
      "Currrent Action : tensor([[0.3581]])\n",
      "Next State : [[-0.99979377]\n",
      " [ 0.02030835]\n",
      " [-7.2335663 ]]\n",
      "Reward : [-12.79219587]\n",
      "--------------------------------------------------\n",
      "Step : 44\n",
      "Current State : [-0.92911494  0.36979112 -7.169489  ]\n",
      "Currrent Action : tensor([[0.3256]])\n",
      "Next State : [[-0.92911494]\n",
      " [ 0.36979112]\n",
      " [-7.169489  ]]\n",
      "Reward : [-14.97496128]\n",
      "--------------------------------------------------\n",
      "Step : 45\n",
      "Current State : [-0.74842143  0.6632235  -6.9266686 ]\n",
      "Currrent Action : tensor([[-0.2302]])\n",
      "Next State : [[-0.74842143]\n",
      " [ 0.6632235 ]\n",
      " [-6.9266686 ]]\n",
      "Reward : [-12.77332081]\n",
      "--------------------------------------------------\n",
      "Step : 46\n",
      "Current State : [-0.5075062  0.8616481 -6.267812 ]\n",
      "Currrent Action : tensor([[1.0763]])\n",
      "Next State : [[-0.5075062]\n",
      " [ 0.8616481]\n",
      " [-6.267812 ]]\n",
      "Reward : [-10.63838394]\n",
      "--------------------------------------------------\n",
      "Step : 47\n",
      "Current State : [-0.2609819  0.9653437 -5.364978 ]\n",
      "Currrent Action : tensor([[1.7107]])\n",
      "Next State : [[-0.2609819]\n",
      " [ 0.9653437]\n",
      " [-5.364978 ]]\n",
      "Reward : [-8.35443686]\n",
      "--------------------------------------------------\n",
      "Step : 48\n",
      "Current State : [-0.01698986  0.99985564 -4.9409704 ]\n",
      "Currrent Action : tensor([[-2.0450]])\n",
      "Next State : [[-0.01698986]\n",
      " [ 0.99985564]\n",
      " [-4.9409704 ]]\n",
      "Reward : [-6.24892029]\n",
      "--------------------------------------------------\n",
      "Step : 49\n",
      "Current State : [ 0.18411271  0.98290515 -4.043195  ]\n",
      "Currrent Action : tensor([[0.9859]])\n",
      "Next State : [[ 0.18411271]\n",
      " [ 0.98290515]\n",
      " [-4.043195  ]]\n",
      "Reward : [-4.96335825]\n",
      "--------------------------------------------------\n",
      "Step : 50\n",
      "Current State : [ 0.3429489   0.93935406 -3.2977068 ]\n",
      "Currrent Action : tensor([[0.0554]])\n",
      "Next State : [[ 0.3429489 ]\n",
      " [ 0.93935406]\n",
      " [-3.2977068 ]]\n",
      "Reward : [-3.55470842]\n",
      "--------------------------------------------------\n",
      "Step : 51\n",
      "Current State : [ 0.45609453  0.8899313  -2.4709466 ]\n",
      "Currrent Action : tensor([[0.8150]])\n",
      "Next State : [[ 0.45609453]\n",
      " [ 0.8899313 ]\n",
      " [-2.4709466 ]]\n",
      "Reward : [-2.578362]\n",
      "--------------------------------------------------\n",
      "Step : 52\n",
      "Current State : [ 0.546381    0.83753675 -2.088708  ]\n",
      "Currrent Action : tensor([[-1.9014]])\n",
      "Next State : [[ 0.546381  ]\n",
      " [ 0.83753675]\n",
      " [-2.088708  ]]\n",
      "Reward : [-1.818009]\n",
      "--------------------------------------------------\n",
      "Step : 53\n",
      "Current State : [ 0.6119203  0.7909194 -1.6089832]\n",
      "Currrent Action : tensor([[-0.9895]])\n",
      "Next State : [[ 0.6119203]\n",
      " [ 0.7909194]\n",
      " [-1.6089832]]\n",
      "Reward : [-1.42282004]\n",
      "--------------------------------------------------\n",
      "Step : 54\n",
      "Current State : [ 0.6493046   0.76052845 -0.9636701 ]\n",
      "Currrent Action : tensor([[0.3475]])\n",
      "Next State : [[ 0.6493046 ]\n",
      " [ 0.76052845]\n",
      " [-0.9636701 ]]\n",
      "Reward : [-1.09131304]\n",
      "--------------------------------------------------\n",
      "Step : 55\n",
      "Current State : [ 0.6654831  0.7464129 -0.4294228]\n",
      "Currrent Action : tensor([[-0.2410]])\n",
      "Next State : [[ 0.6654831]\n",
      " [ 0.7464129]\n",
      " [-0.4294228]]\n",
      "Reward : [-0.83963876]\n",
      "--------------------------------------------------\n",
      "Step : 56\n",
      "Current State : [0.66525906 0.7466126  0.00600305]\n",
      "Currrent Action : tensor([[-0.8292]])\n",
      "Next State : [[0.66525906]\n",
      " [0.7466126 ]\n",
      " [0.00600305]]\n",
      "Reward : [-0.72919614]\n",
      "--------------------------------------------------\n",
      "Step : 57\n",
      "Current State : [0.6395111  0.7687819  0.67957246]\n",
      "Currrent Action : tensor([[0.7574]])\n",
      "Next State : [[0.6395111 ]\n",
      " [0.7687819 ]\n",
      " [0.67957246]]\n",
      "Reward : [-0.71115133]\n",
      "--------------------------------------------------\n",
      "Step : 58\n",
      "Current State : [0.6020405  0.79846555 0.9561589 ]\n",
      "Currrent Action : tensor([[-2.3224]])\n",
      "Next State : [[0.6020405 ]\n",
      " [0.79846555]\n",
      " [0.9561589 ]]\n",
      "Reward : [-0.81919544]\n",
      "--------------------------------------------------\n",
      "Step : 59\n",
      "Current State : [0.54649925 0.8374596  1.3575183 ]\n",
      "Currrent Action : tensor([[-1.3166]])\n",
      "Next State : [[0.54649925]\n",
      " [0.8374596 ]\n",
      " [1.3575183 ]]\n",
      "Reward : [-0.94830542]\n",
      "--------------------------------------------------\n",
      "Step : 60\n",
      "Current State : [0.46148786 0.88714653 1.9701338 ]\n",
      "Currrent Action : tensor([[-0.1032]])\n",
      "Next State : [[0.46148786]\n",
      " [0.88714653]\n",
      " [1.9701338 ]]\n",
      "Reward : [-1.16958683]\n",
      "--------------------------------------------------\n",
      "Step : 61\n",
      "Current State : [0.33554277 0.94202495 2.7498047 ]\n",
      "Currrent Action : tensor([[0.7621]])\n",
      "Next State : [[0.33554277]\n",
      " [0.94202495]\n",
      " [2.7498047 ]]\n",
      "Reward : [-1.57927665]\n",
      "--------------------------------------------------\n",
      "Step : 62\n",
      "Current State : [0.17550078 0.9844793  3.3153386 ]\n",
      "Currrent Action : tensor([[-0.9399]])\n",
      "Next State : [[0.17550078]\n",
      " [0.9844793 ]\n",
      " [3.3153386 ]]\n",
      "Reward : [-2.26652081]\n",
      "--------------------------------------------------\n",
      "Step : 63\n",
      "Current State : [-0.02264055  0.99974364  3.981138  ]\n",
      "Currrent Action : tensor([[-0.4837]])\n",
      "Next State : [[-0.02264055]\n",
      " [ 0.99974364]\n",
      " [ 3.981138  ]]\n",
      "Reward : [-3.04368195]\n",
      "--------------------------------------------------\n",
      "Step : 64\n",
      "Current State : [-0.24727741  0.9689447   4.544538  ]\n",
      "Currrent Action : tensor([[-1.2427]])\n",
      "Next State : [[-0.24727741]\n",
      " [ 0.9689447 ]\n",
      " [ 4.544538  ]]\n",
      "Reward : [-4.12553759]\n",
      "--------------------------------------------------\n",
      "Step : 65\n",
      "Current State : [-0.48548493  0.87424505  5.1409674 ]\n",
      "Currrent Action : tensor([[-0.8685]])\n",
      "Next State : [[-0.48548493]\n",
      " [ 0.87424505]\n",
      " [ 5.1409674 ]]\n",
      "Reward : [-5.3808607]\n",
      "--------------------------------------------------\n",
      "Step : 66\n",
      "Current State : [-0.7129865  0.7011777  5.736612 ]\n",
      "Currrent Action : tensor([[-0.4003]])\n",
      "Next State : [[-0.7129865]\n",
      " [ 0.7011777]\n",
      " [ 5.736612 ]]\n",
      "Reward : [-6.96001041]\n",
      "--------------------------------------------------\n",
      "Step : 67\n",
      "Current State : [-0.8956607  0.4447381  6.3233232]\n",
      "Currrent Action : tensor([[0.4055]])\n",
      "Next State : [[-0.8956607]\n",
      " [ 0.4447381]\n",
      " [ 6.3233232]]\n",
      "Reward : [-8.88210766]\n",
      "--------------------------------------------------\n",
      "Step : 68\n",
      "Current State : [-0.9925029   0.12222106  6.767088  ]\n",
      "Currrent Action : tensor([[0.7347]])\n",
      "Next State : [[-0.9925029 ]\n",
      " [ 0.12222106]\n",
      " [ 6.767088  ]]\n",
      "Reward : [-11.1851924]\n",
      "--------------------------------------------------\n",
      "Step : 69\n",
      "Current State : [-0.9725883  -0.23253398  7.1441936 ]\n",
      "Currrent Action : tensor([[1.9029]])\n",
      "Next State : [[-0.9725883 ]\n",
      " [-0.23253398]\n",
      " [ 7.1441936 ]]\n",
      "Reward : [-13.69772392]\n",
      "--------------------------------------------------\n",
      "Step : 70\n",
      "Current State : [-0.8345507  -0.55093116  6.9759464 ]\n",
      "Currrent Action : tensor([[0.0410]])\n",
      "Next State : [[-0.8345507 ]\n",
      " [-0.55093116]\n",
      " [ 6.9759464 ]]\n",
      "Reward : [-13.55408002]\n",
      "--------------------------------------------------\n",
      "Step : 71\n",
      "Current State : [-0.6145924 -0.7888448  6.508951 ]\n",
      "Currrent Action : tensor([[-0.3586]])\n",
      "Next State : [[-0.6145924]\n",
      " [-0.7888448]\n",
      " [ 6.508951 ]]\n",
      "Reward : [-11.41045416]\n",
      "--------------------------------------------------\n",
      "Step : 72\n",
      "Current State : [-0.3516129 -0.9361455  6.051514 ]\n",
      "Currrent Action : tensor([[0.8946]])\n",
      "Next State : [[-0.3516129]\n",
      " [-0.9361455]\n",
      " [ 6.051514 ]]\n",
      "Reward : [-9.22224028]\n",
      "--------------------------------------------------\n",
      "Step : 73\n",
      "Current State : [-0.08251403 -0.9965899   5.53371   ]\n",
      "Currrent Action : tensor([[1.2287]])\n",
      "Next State : [[-0.08251403]\n",
      " [-0.9965899 ]\n",
      " [ 5.53371   ]]\n",
      "Reward : [-7.38883849]\n",
      "--------------------------------------------------\n",
      "Step : 74\n",
      "Current State : [ 0.15591258 -0.98777086  4.7831845 ]\n",
      "Currrent Action : tensor([[-0.0206]])\n",
      "Next State : [[ 0.15591258]\n",
      " [-0.98777086]\n",
      " [ 4.7831845 ]]\n",
      "Reward : [-5.79594071]\n",
      "--------------------------------------------------\n",
      "Step : 75\n",
      "Current State : [ 0.3455246 -0.9384097  3.9249303]\n",
      "Currrent Action : tensor([[-0.7828]])\n",
      "Next State : [[ 0.3455246]\n",
      " [-0.9384097]\n",
      " [ 3.9249303]]\n",
      "Reward : [-4.28858723]\n",
      "--------------------------------------------------\n",
      "Step : 76\n",
      "Current State : [ 0.49206266 -0.87055975  3.2331948 ]\n",
      "Currrent Action : tensor([[0.0805]])\n",
      "Next State : [[ 0.49206266]\n",
      " [-0.87055975]\n",
      " [ 3.2331948 ]]\n",
      "Reward : [-3.02403473]\n",
      "--------------------------------------------------\n",
      "Step : 77\n",
      "Current State : [ 0.5946928 -0.803953   2.4485197]\n",
      "Currrent Action : tensor([[-0.8784]])\n",
      "Next State : [[ 0.5946928]\n",
      " [-0.803953 ]\n",
      " [ 2.4485197]]\n",
      "Reward : [-2.16197814]\n",
      "--------------------------------------------------\n",
      "Step : 78\n",
      "Current State : [ 0.67735505 -0.73565626  2.145555  ]\n",
      "Currrent Action : tensor([[2.0917]])\n",
      "Next State : [[ 0.67735505]\n",
      " [-0.73565626]\n",
      " [ 2.145555  ]]\n",
      "Reward : [-1.47571806]\n",
      "--------------------------------------------------\n",
      "Step : 79\n",
      "Current State : [ 0.7241771 -0.689614   1.313583 ]\n",
      "Currrent Action : tensor([[-1.8682]])\n",
      "Next State : [[ 0.7241771]\n",
      " [-0.689614 ]\n",
      " [ 1.313583 ]]\n",
      "Reward : [-1.14715633]\n",
      "--------------------------------------------------\n",
      "Step : 80\n",
      "Current State : [ 0.7547696  -0.65598994  0.90924877]\n",
      "Currrent Action : tensor([[0.7525]])\n",
      "Next State : [[ 0.7547696 ]\n",
      " [-0.65598994]\n",
      " [ 0.90924877]]\n",
      "Reward : [-0.75217019]\n",
      "--------------------------------------------------\n",
      "Step : 81\n",
      "Current State : [ 0.76798445 -0.6404685   0.4077063 ]\n",
      "Currrent Action : tensor([[-0.0637]])\n",
      "Next State : [[ 0.76798445]\n",
      " [-0.6404685 ]\n",
      " [ 0.4077063 ]]\n",
      "Reward : [-0.59460829]\n",
      "--------------------------------------------------\n",
      "Step : 82\n",
      "Current State : [ 0.75591844 -0.65466577 -0.37264508]\n",
      "Currrent Action : tensor([[-2.2878]])\n",
      "Next State : [[ 0.75591844]\n",
      " [-0.65466577]\n",
      " [-0.37264508]]\n",
      "Reward : [-0.50379778]\n",
      "--------------------------------------------------\n",
      "Step : 83\n",
      "Current State : [ 0.72192574 -0.6919705  -1.0094932 ]\n",
      "Currrent Action : tensor([[-0.9723]])\n",
      "Next State : [[ 0.72192574]\n",
      " [-0.6919705 ]\n",
      " [-1.0094932 ]]\n",
      "Reward : [-0.52425721]\n",
      "--------------------------------------------------\n",
      "Step : 84\n",
      "Current State : [ 0.6686612 -0.7435672 -1.4834889]\n",
      "Currrent Action : tensor([[0.2999]])\n",
      "Next State : [[ 0.6686612]\n",
      " [-0.7435672]\n",
      " [-1.4834889]]\n",
      "Reward : [-0.68602224]\n",
      "--------------------------------------------------\n",
      "Step : 85\n",
      "Current State : [ 0.5856854  -0.81053853 -2.133629  ]\n",
      "Currrent Action : tensor([[-0.6164]])\n",
      "Next State : [[ 0.5856854 ]\n",
      " [-0.81053853]\n",
      " [-2.133629  ]]\n",
      "Reward : [-0.92335091]\n",
      "--------------------------------------------------\n",
      "Step : 86\n",
      "Current State : [ 0.47531623 -0.87981504 -2.6080387 ]\n",
      "Currrent Action : tensor([[0.8900]])\n",
      "Next State : [[ 0.47531623]\n",
      " [-0.87981504]\n",
      " [-2.6080387 ]]\n",
      "Reward : [-1.34918851]\n",
      "--------------------------------------------------\n",
      "Step : 87\n",
      "Current State : [ 0.33229584 -0.9431752  -3.1317344 ]\n",
      "Currrent Action : tensor([[0.9078]])\n",
      "Next State : [[ 0.33229584]\n",
      " [-0.9431752 ]\n",
      " [-3.1317344 ]]\n",
      "Reward : [-1.83765263]\n",
      "--------------------------------------------------\n",
      "Step : 88\n",
      "Current State : [ 0.1549443 -0.9879232 -3.6633127]\n",
      "Currrent Action : tensor([[1.1720]])\n",
      "Next State : [[ 0.1549443]\n",
      " [-0.9879232]\n",
      " [-3.6633127]]\n",
      "Reward : [-2.50012064]\n",
      "--------------------------------------------------\n",
      "Step : 89\n",
      "Current State : [-0.05926539 -0.99824226 -4.2974243 ]\n",
      "Currrent Action : tensor([[0.7122]])\n",
      "Next State : [[-0.05926539]\n",
      " [-0.99824226]\n",
      " [-4.2974243 ]]\n",
      "Reward : [-3.3453557]\n",
      "--------------------------------------------------\n",
      "Step : 90\n",
      "Current State : [-0.29227558 -0.9563342  -4.746106  ]\n",
      "Currrent Action : tensor([[2.2642]])\n",
      "Next State : [[-0.29227558]\n",
      " [-0.9563342 ]\n",
      " [-4.746106  ]]\n",
      "Reward : [-4.508]\n",
      "--------------------------------------------------\n",
      "Step : 91\n",
      "Current State : [-0.53696674 -0.84360343 -5.4046392 ]\n",
      "Currrent Action : tensor([[0.3914]])\n",
      "Next State : [[-0.53696674]\n",
      " [-0.84360343]\n",
      " [-5.4046392 ]]\n",
      "Reward : [-5.73989473]\n",
      "--------------------------------------------------\n",
      "Step : 92\n",
      "Current State : [-0.75926495 -0.65078163 -5.9069095 ]\n",
      "Currrent Action : tensor([[0.8695]])\n",
      "Next State : [[-0.75926495]\n",
      " [-0.65078163]\n",
      " [-5.9069095 ]]\n",
      "Reward : [-7.4912465]\n",
      "--------------------------------------------------\n",
      "Step : 93\n",
      "Current State : [-0.9195495  -0.39297408 -6.094996  ]\n",
      "Currrent Action : tensor([[2.8605]])\n",
      "Next State : [[-0.9195495 ]\n",
      " [-0.39297408]\n",
      " [-6.094996  ]]\n",
      "Reward : [-9.41254605]\n",
      "--------------------------------------------------\n",
      "Step : 94\n",
      "Current State : [-0.99520487 -0.09781273 -6.1178856 ]\n",
      "Currrent Action : tensor([[1.8123]])\n",
      "Next State : [[-0.99520487]\n",
      " [-0.09781273]\n",
      " [-6.1178856 ]]\n",
      "Reward : [-11.21334197]\n",
      "--------------------------------------------------\n",
      "Step : 95\n",
      "Current State : [-0.98057824  0.19612837 -5.9075484 ]\n",
      "Currrent Action : tensor([[1.8913]])\n",
      "Next State : [[-0.98057824]\n",
      " [ 0.19612837]\n",
      " [-5.9075484 ]]\n",
      "Reward : [-13.01007197]\n",
      "--------------------------------------------------\n",
      "Step : 96\n",
      "Current State : [-0.8815898   0.47201636 -5.8833704 ]\n",
      "Currrent Action : tensor([[-0.8195]])\n",
      "Next State : [[-0.8815898 ]\n",
      " [ 0.47201636]\n",
      " [-5.8833704 ]]\n",
      "Reward : [-12.15880724]\n",
      "--------------------------------------------------\n",
      "Step : 97\n",
      "Current State : [-0.72588164  0.68781966 -5.3380895 ]\n",
      "Currrent Action : tensor([[1.2751]])\n",
      "Next State : [[-0.72588164]\n",
      " [ 0.68781966]\n",
      " [-5.3380895 ]]\n",
      "Reward : [-10.48561587]\n",
      "--------------------------------------------------\n",
      "Step : 98\n",
      "Current State : [-0.5401281   0.84158283 -4.83452   ]\n",
      "Currrent Action : tensor([[-0.0820]])\n",
      "Next State : [[-0.5401281 ]\n",
      " [ 0.84158283]\n",
      " [-4.83452   ]]\n",
      "Reward : [-8.52874783]\n",
      "--------------------------------------------------\n",
      "Step : 99\n",
      "Current State : [-0.35840514  0.93356615 -4.0806065 ]\n",
      "Currrent Action : tensor([[0.8182]])\n",
      "Next State : [[-0.35840514]\n",
      " [ 0.93356615]\n",
      " [-4.0806065 ]]\n",
      "Reward : [-6.92345999]\n",
      "learning iteration : 10\n",
      "--------------------------------------------------\n",
      "Step : 0\n",
      "Current State : [ 0.48527354 -0.8743624   0.323275  ]\n",
      "Currrent Action : tensor([[2.3364]])\n",
      "Next State : [[ 0.48527354]\n",
      " [-0.8743624 ]\n",
      " [ 0.323275  ]]\n",
      "Reward : [-1.21791484]\n",
      "--------------------------------------------------\n",
      "Step : 1\n",
      "Current State : [ 0.47192374 -0.88163936 -0.30408934]\n",
      "Currrent Action : tensor([[0.1894]])\n",
      "Next State : [[ 0.47192374]\n",
      " [-0.88163936]\n",
      " [-0.30408934]]\n",
      "Reward : [-1.14283866]\n",
      "--------------------------------------------------\n",
      "Step : 2\n",
      "Current State : [ 0.42658648 -0.9044468  -1.0151255 ]\n",
      "Currrent Action : tensor([[-0.3320]])\n",
      "Next State : [[ 0.42658648]\n",
      " [-0.9044468 ]\n",
      " [-1.0151255 ]]\n",
      "Reward : [-1.17429935]\n",
      "--------------------------------------------------\n",
      "Step : 3\n",
      "Current State : [ 0.34758556 -0.93764824 -1.714407  ]\n",
      "Currrent Action : tensor([[-0.1396]])\n",
      "Next State : [[ 0.34758556]\n",
      " [-0.93764824]\n",
      " [-1.714407  ]]\n",
      "Reward : [-1.38015075]\n",
      "--------------------------------------------------\n",
      "Step : 4\n",
      "Current State : [ 0.23515806 -0.97195715 -2.3522742 ]\n",
      "Currrent Action : tensor([[0.4358]])\n",
      "Next State : [[ 0.23515806]\n",
      " [-0.97195715]\n",
      " [-2.3522742 ]]\n",
      "Reward : [-1.77228217]\n",
      "--------------------------------------------------\n",
      "Step : 5\n",
      "Current State : [ 0.0864963 -0.9962522 -3.0155332]\n",
      "Currrent Action : tensor([[0.4381]])\n",
      "Next State : [[ 0.0864963]\n",
      " [-0.9962522]\n",
      " [-3.0155332]]\n",
      "Reward : [-2.33150721]\n",
      "--------------------------------------------------\n",
      "Step : 6\n",
      "Current State : [-0.10280915 -0.9947011  -3.791913  ]\n",
      "Currrent Action : tensor([[-0.1946]])\n",
      "Next State : [[-0.10280915]\n",
      " [-0.9947011 ]\n",
      " [-3.791913  ]]\n",
      "Reward : [-3.1122072]\n",
      "--------------------------------------------------\n",
      "Step : 7\n",
      "Current State : [-0.31682876 -0.94848275 -4.38786   ]\n",
      "Currrent Action : tensor([[1.0005]])\n",
      "Next State : [[-0.31682876]\n",
      " [-0.94848275]\n",
      " [-4.38786   ]]\n",
      "Reward : [-4.24042591]\n",
      "--------------------------------------------------\n",
      "Step : 8\n",
      "Current State : [-0.5462389  -0.83762944 -5.109667  ]\n",
      "Currrent Action : tensor([[-0.0696]])\n",
      "Next State : [[-0.5462389 ]\n",
      " [-0.83762944]\n",
      " [-5.109667  ]]\n",
      "Reward : [-5.50946827]\n",
      "--------------------------------------------------\n",
      "Step : 9\n",
      "Current State : [-0.7516994 -0.6595059 -5.455353 ]\n",
      "Currrent Action : tensor([[1.8836]])\n",
      "Next State : [[-0.7516994]\n",
      " [-0.6595059]\n",
      " [-5.455353 ]]\n",
      "Reward : [-7.23117324]\n",
      "--------------------------------------------------\n",
      "Step : 10\n",
      "Current State : [-0.90574497 -0.42382312 -5.649982  ]\n",
      "Currrent Action : tensor([[2.3920]])\n",
      "Next State : [[-0.90574497]\n",
      " [-0.42382312]\n",
      " [-5.649982  ]]\n",
      "Reward : [-8.84341725]\n",
      "--------------------------------------------------\n",
      "Step : 11\n",
      "Current State : [-0.9903565 -0.1385423 -5.9734554]\n",
      "Currrent Action : tensor([[-0.0374]])\n",
      "Next State : [[-0.9903565]\n",
      " [-0.1385423]\n",
      " [-5.9734554]]\n",
      "Reward : [-10.50347127]\n",
      "--------------------------------------------------\n",
      "Step : 12\n",
      "Current State : [-0.9887892   0.14931825 -5.7773623 ]\n",
      "Currrent Action : tensor([[2.2291]])\n",
      "Next State : [[-0.9887892 ]\n",
      " [ 0.14931825]\n",
      " [-5.7773623 ]]\n",
      "Reward : [-12.58784369]\n",
      "--------------------------------------------------\n",
      "Step : 13\n",
      "Current State : [-0.905727   0.4238615 -5.756515 ]\n",
      "Currrent Action : tensor([[-0.6076]])\n",
      "Next State : [[-0.905727 ]\n",
      " [ 0.4238615]\n",
      " [-5.756515 ]]\n",
      "Reward : [-12.28851284]\n",
      "--------------------------------------------------\n",
      "Step : 14\n",
      "Current State : [-0.7675681  0.6409674 -5.161069 ]\n",
      "Currrent Action : tensor([[1.8503]])\n",
      "Next State : [[-0.7675681]\n",
      " [ 0.6409674]\n",
      " [-5.161069 ]]\n",
      "Reward : [-10.6281816]\n",
      "--------------------------------------------------\n",
      "Step : 15\n",
      "Current State : [-0.5954132   0.80341965 -4.7451706 ]\n",
      "Currrent Action : tensor([[-0.4322]])\n",
      "Next State : [[-0.5954132 ]\n",
      " [ 0.80341965]\n",
      " [-4.7451706 ]]\n",
      "Reward : [-8.64595726]\n",
      "--------------------------------------------------\n",
      "Step : 16\n",
      "Current State : [-0.41583875  0.9094384  -4.1782994 ]\n",
      "Currrent Action : tensor([[-0.2380]])\n",
      "Next State : [[-0.41583875]\n",
      " [ 0.9094384 ]\n",
      " [-4.1782994 ]]\n",
      "Reward : [-7.12952968]\n",
      "--------------------------------------------------\n",
      "Step : 17\n",
      "Current State : [-0.26100346  0.9653379  -3.2960665 ]\n",
      "Currrent Action : tensor([[1.3344]])\n",
      "Next State : [[-0.26100346]\n",
      " [ 0.9653379 ]\n",
      " [-3.2960665 ]]\n",
      "Reward : [-5.74624399]\n",
      "--------------------------------------------------\n",
      "Step : 18\n",
      "Current State : [-0.14248613  0.9897968  -2.4217768 ]\n",
      "Currrent Action : tensor([[1.0019]])\n",
      "Next State : [[-0.14248613]\n",
      " [ 0.9897968 ]\n",
      " [-2.4217768 ]]\n",
      "Reward : [-4.45411276]\n",
      "--------------------------------------------------\n",
      "Step : 19\n",
      "Current State : [-0.0656018  0.9978459 -1.5464754]\n",
      "Currrent Action : tensor([[0.8864]])\n",
      "Next State : [[-0.0656018]\n",
      " [ 0.9978459]\n",
      " [-1.5464754]]\n",
      "Reward : [-3.5242903]\n",
      "--------------------------------------------------\n",
      "Step : 20\n",
      "Current State : [-0.02704528  0.9996342  -0.77200735]\n",
      "Currrent Action : tensor([[0.1739]])\n",
      "Next State : [[-0.02704528]\n",
      " [ 0.9996342 ]\n",
      " [-0.77200735]]\n",
      "Reward : [-2.91714199]\n",
      "--------------------------------------------------\n",
      "Step : 21\n",
      "Current State : [-0.0262379   0.9996557  -0.01615332]\n",
      "Currrent Action : tensor([[0.0409]])\n",
      "Next State : [[-0.0262379 ]\n",
      " [ 0.9996557 ]\n",
      " [-0.01615332]]\n",
      "Reward : [-2.61270954]\n",
      "--------------------------------------------------\n",
      "Step : 22\n",
      "Current State : [-0.06334839  0.9979915   0.7429986 ]\n",
      "Currrent Action : tensor([[0.0627]])\n",
      "Next State : [[-0.06334839]\n",
      " [ 0.9979915 ]\n",
      " [ 0.7429986 ]]\n",
      "Reward : [-2.55055796]\n",
      "--------------------------------------------------\n",
      "Step : 23\n",
      "Current State : [-0.14259085  0.98978174  1.5937536 ]\n",
      "Currrent Action : tensor([[0.6817]])\n",
      "Next State : [[-0.14259085]\n",
      " [ 0.98978174]\n",
      " [ 1.5937536 ]]\n",
      "Reward : [-2.72623716]\n",
      "--------------------------------------------------\n",
      "Step : 24\n",
      "Current State : [-0.2584754   0.96601784  2.3673022 ]\n",
      "Currrent Action : tensor([[0.2081]])\n",
      "Next State : [[-0.2584754 ]\n",
      " [ 0.96601784]\n",
      " [ 2.3673022 ]]\n",
      "Reward : [-3.19141533]\n",
      "--------------------------------------------------\n",
      "Step : 25\n",
      "Current State : [-0.41508645  0.909782    3.331884  ]\n",
      "Currrent Action : tensor([[1.6005]])\n",
      "Next State : [[-0.41508645]\n",
      " [ 0.909782  ]\n",
      " [ 3.331884  ]]\n",
      "Reward : [-3.92007672]\n",
      "--------------------------------------------------\n",
      "Step : 26\n",
      "Current State : [-0.58550656  0.8106677   3.9493392 ]\n",
      "Currrent Action : tensor([[-0.4325]])\n",
      "Next State : [[-0.58550656]\n",
      " [ 0.8106677 ]\n",
      " [ 3.9493392 ]]\n",
      "Reward : [-5.10567004]\n",
      "--------------------------------------------------\n",
      "Step : 27\n",
      "Current State : [-0.7479749   0.66372705  4.390018  ]\n",
      "Currrent Action : tensor([[-1.1155]])\n",
      "Next State : [[-0.7479749 ]\n",
      " [ 0.66372705]\n",
      " [ 4.390018  ]]\n",
      "Reward : [-6.38471081]\n",
      "--------------------------------------------------\n",
      "Step : 28\n",
      "Current State : [-0.8840678   0.46735868  4.7897987 ]\n",
      "Currrent Action : tensor([[-0.6534]])\n",
      "Next State : [[-0.8840678 ]\n",
      " [ 0.46735868]\n",
      " [ 4.7897987 ]]\n",
      "Reward : [-7.76375204]\n",
      "--------------------------------------------------\n",
      "Step : 29\n",
      "Current State : [-0.9722248  0.2340491  5.0012064]\n",
      "Currrent Action : tensor([[-0.9274]])\n",
      "Next State : [[-0.9722248]\n",
      " [ 0.2340491]\n",
      " [ 5.0012064]]\n",
      "Reward : [-9.34565274]\n",
      "--------------------------------------------------\n",
      "Step : 30\n",
      "Current State : [-0.9994162 -0.0341666  5.4082727]\n",
      "Currrent Action : tensor([[1.5435]])\n",
      "Next State : [[-0.9994162]\n",
      " [-0.0341666]\n",
      " [ 5.4082727]]\n",
      "Reward : [-10.94466103]\n",
      "--------------------------------------------------\n",
      "Step : 31\n",
      "Current State : [-0.9518971  -0.30641788  5.545087  ]\n",
      "Currrent Action : tensor([[1.0829]])\n",
      "Next State : [[-0.9518971 ]\n",
      " [-0.30641788]\n",
      " [ 5.545087  ]]\n",
      "Reward : [-12.58216926]\n",
      "--------------------------------------------------\n",
      "Step : 32\n",
      "Current State : [-0.83697927 -0.54723454  5.3525796 ]\n",
      "Currrent Action : tensor([[0.2487]])\n",
      "Next State : [[-0.83697927]\n",
      " [-0.54723454]\n",
      " [ 5.3525796 ]]\n",
      "Reward : [-11.08469503]\n",
      "--------------------------------------------------\n",
      "Step : 33\n",
      "Current State : [-0.6761878 -0.7367293  4.98328  ]\n",
      "Currrent Action : tensor([[0.2742]])\n",
      "Next State : [[-0.6761878]\n",
      " [-0.7367293]\n",
      " [ 4.98328  ]]\n",
      "Reward : [-9.43167718]\n",
      "--------------------------------------------------\n",
      "Step : 34\n",
      "Current State : [-0.5029021 -0.8643434  4.3124537]\n",
      "Currrent Action : tensor([[-0.7885]])\n",
      "Next State : [[-0.5029021]\n",
      " [-0.8643434]\n",
      " [ 4.3124537]]\n",
      "Reward : [-7.83562008]\n",
      "--------------------------------------------------\n",
      "Step : 35\n",
      "Current State : [-0.3450967  -0.93856716  3.4922254 ]\n",
      "Currrent Action : tensor([[-1.1465]])\n",
      "Next State : [[-0.3450967 ]\n",
      " [-0.93856716]\n",
      " [ 3.4922254 ]]\n",
      "Reward : [-6.26159253]\n",
      "--------------------------------------------------\n",
      "Step : 36\n",
      "Current State : [-0.22122958 -0.97522175  2.5853338 ]\n",
      "Currrent Action : tensor([[-1.3531]])\n",
      "Next State : [[-0.22122958]\n",
      " [-0.97522175]\n",
      " [ 2.5853338 ]]\n",
      "Reward : [-4.91985495]\n",
      "--------------------------------------------------\n",
      "Step : 37\n",
      "Current State : [-0.12994052 -0.9915218   1.8553224 ]\n",
      "Currrent Action : tensor([[0.0094]])\n",
      "Next State : [[-0.12994052]\n",
      " [-0.9915218 ]\n",
      " [ 1.8553224 ]]\n",
      "Reward : [-3.88636994]\n",
      "--------------------------------------------------\n",
      "Step : 38\n",
      "Current State : [-0.0787148  -0.99689716  1.0302538 ]\n",
      "Currrent Action : tensor([[-0.5428]])\n",
      "Next State : [[-0.0787148 ]\n",
      " [-0.99689716]\n",
      " [ 1.0302538 ]]\n",
      "Reward : [-3.23827611]\n",
      "--------------------------------------------------\n",
      "Step : 39\n",
      "Current State : [-0.04964685 -0.99876684  0.5825809 ]\n",
      "Currrent Action : tensor([[2.2850]])\n",
      "Next State : [[-0.04964685]\n",
      " [-0.99876684]\n",
      " [ 0.5825809 ]]\n",
      "Reward : [-2.83129816]\n",
      "--------------------------------------------------\n",
      "Step : 40\n",
      "Current State : [-0.05270956 -0.9986099  -0.0613346 ]\n",
      "Currrent Action : tensor([[0.7011]])\n",
      "Next State : [[-0.05270956]\n",
      " [-0.9986099 ]\n",
      " [-0.0613346 ]]\n",
      "Reward : [-2.6603338]\n",
      "--------------------------------------------------\n",
      "Step : 41\n",
      "Current State : [-0.09121717 -0.995831   -0.7722029 ]\n",
      "Currrent Action : tensor([[0.2539]])\n",
      "Next State : [[-0.09121717]\n",
      " [-0.995831  ]\n",
      " [-0.7722029 ]]\n",
      "Reward : [-2.63629138]\n",
      "--------------------------------------------------\n",
      "Step : 42\n",
      "Current State : [-0.16680537 -0.98598987 -1.5248923 ]\n",
      "Currrent Action : tensor([[-0.0388]])\n",
      "Next State : [[-0.16680537]\n",
      " [-0.98598987]\n",
      " [-1.5248923 ]]\n",
      "Reward : [-2.82234216]\n",
      "--------------------------------------------------\n",
      "Step : 43\n",
      "Current State : [-0.28084728 -0.9597525  -2.3417609 ]\n",
      "Currrent Action : tensor([[-0.5158]])\n",
      "Next State : [[-0.28084728]\n",
      " [-0.9597525 ]\n",
      " [-2.3417609 ]]\n",
      "Reward : [-3.25477844]\n",
      "--------------------------------------------------\n",
      "Step : 44\n",
      "Current State : [-0.4184619  -0.90823436 -2.9414866 ]\n",
      "Currrent Action : tensor([[0.8006]])\n",
      "Next State : [[-0.4184619 ]\n",
      " [-0.90823436]\n",
      " [-2.9414866 ]]\n",
      "Reward : [-3.99180591]\n",
      "--------------------------------------------------\n",
      "Step : 45\n",
      "Current State : [-0.57517916 -0.81802744 -3.6214406 ]\n",
      "Currrent Action : tensor([[0.0081]])\n",
      "Next State : [[-0.57517916]\n",
      " [-0.81802744]\n",
      " [-3.6214406 ]]\n",
      "Reward : [-4.8754307]\n",
      "--------------------------------------------------\n",
      "Step : 46\n",
      "Current State : [-0.738082   -0.67471105 -4.348001  ]\n",
      "Currrent Action : tensor([[-0.7536]])\n",
      "Next State : [[-0.738082  ]\n",
      " [-0.67471105]\n",
      " [-4.348001  ]]\n",
      "Reward : [-6.08024513]\n",
      "--------------------------------------------------\n",
      "Step : 47\n",
      "Current State : [-0.87736386 -0.4798257  -4.8023415 ]\n",
      "Currrent Action : tensor([[0.3446]])\n",
      "Next State : [[-0.87736386]\n",
      " [-0.4798257 ]\n",
      " [-4.8023415 ]]\n",
      "Reward : [-7.65552507]\n",
      "--------------------------------------------------\n",
      "Step : 48\n",
      "Current State : [-0.967069   -0.25451437 -4.8622108 ]\n",
      "Currrent Action : tensor([[2.3487]])\n",
      "Next State : [[-0.967069  ]\n",
      " [-0.25451437]\n",
      " [-4.8622108 ]]\n",
      "Reward : [-9.28585096]\n",
      "--------------------------------------------------\n",
      "Step : 49\n",
      "Current State : [-0.9999861   0.00527537 -5.252418  ]\n",
      "Currrent Action : tensor([[-1.3288]])\n",
      "Next State : [[-0.9999861 ]\n",
      " [ 0.00527537]\n",
      " [-5.252418  ]]\n",
      "Reward : [-10.68475668]\n",
      "--------------------------------------------------\n",
      "Step : 50\n",
      "Current State : [-0.9672042   0.25399992 -5.030763  ]\n",
      "Currrent Action : tensor([[1.4513]])\n",
      "Next State : [[-0.9672042 ]\n",
      " [ 0.25399992]\n",
      " [-5.030763  ]]\n",
      "Reward : [-12.5973817]\n",
      "--------------------------------------------------\n",
      "Step : 51\n",
      "Current State : [-0.8852213  0.4651701 -4.540263 ]\n",
      "Currrent Action : tensor([[3.8795]])\n",
      "Next State : [[-0.8852213]\n",
      " [ 0.4651701]\n",
      " [-4.540263 ]]\n",
      "Reward : [-10.85680837]\n",
      "--------------------------------------------------\n",
      "Step : 52\n",
      "Current State : [-0.7705063  0.6374324 -4.1466904]\n",
      "Currrent Action : tensor([[0.2980]])\n",
      "Next State : [[-0.7705063]\n",
      " [ 0.6374324]\n",
      " [-4.1466904]]\n",
      "Reward : [-9.12520764]\n",
      "--------------------------------------------------\n",
      "Step : 53\n",
      "Current State : [-0.6476831   0.76190984 -3.5019107 ]\n",
      "Currrent Action : tensor([[1.1114]])\n",
      "Next State : [[-0.6476831 ]\n",
      " [ 0.76190984]\n",
      " [-3.5019107 ]]\n",
      "Reward : [-7.72535336]\n",
      "--------------------------------------------------\n",
      "Step : 54\n",
      "Current State : [-0.54216844  0.8402698  -2.6304784 ]\n",
      "Currrent Action : tensor([[2.1682]])\n",
      "Next State : [[-0.54216844]\n",
      " [ 0.8402698 ]\n",
      " [-2.6304784 ]]\n",
      "Reward : [-6.40749118]\n",
      "--------------------------------------------------\n",
      "Step : 55\n",
      "Current State : [-0.45118535  0.89243025 -2.0984502 ]\n",
      "Currrent Action : tensor([[-0.6545]])\n",
      "Next State : [[-0.45118535]\n",
      " [ 0.89243025]\n",
      " [-2.0984502 ]]\n",
      "Reward : [-5.28829964]\n",
      "--------------------------------------------------\n",
      "Step : 56\n",
      "Current State : [-0.379391   0.9252364 -1.5791018]\n",
      "Currrent Action : tensor([[-0.9998]])\n",
      "Next State : [[-0.379391 ]\n",
      " [ 0.9252364]\n",
      " [-1.5791018]]\n",
      "Reward : [-4.59841907]\n",
      "--------------------------------------------------\n",
      "Step : 57\n",
      "Current State : [-0.34215337  0.9396441  -0.7986078 ]\n",
      "Currrent Action : tensor([[0.5771]])\n",
      "Next State : [[-0.34215337]\n",
      " [ 0.9396441 ]\n",
      " [-0.7986078 ]]\n",
      "Reward : [-4.09103193]\n",
      "--------------------------------------------------\n",
      "Step : 58\n",
      "Current State : [-0.33423808  0.9424887  -0.1682182 ]\n",
      "Currrent Action : tensor([[-0.4956]])\n",
      "Next State : [[-0.33423808]\n",
      " [ 0.9424887 ]\n",
      " [-0.1682182 ]]\n",
      "Reward : [-3.75043826]\n",
      "--------------------------------------------------\n",
      "Step : 59\n",
      "Current State : [-0.3665979  0.9303795  0.6910592]\n",
      "Currrent Action : tensor([[1.0161]])\n",
      "Next State : [[-0.3665979]\n",
      " [ 0.9303795]\n",
      " [ 0.6910592]]\n",
      "Reward : [-3.6580501]\n",
      "--------------------------------------------------\n",
      "Step : 60\n",
      "Current State : [-0.4437616   0.89614487  1.6888438 ]\n",
      "Currrent Action : tensor([[2.0955]])\n",
      "Next State : [[-0.4437616 ]\n",
      " [ 0.89614487]\n",
      " [ 1.6888438 ]]\n",
      "Reward : [-3.83924054]\n",
      "--------------------------------------------------\n",
      "Step : 61\n",
      "Current State : [-0.54048944  0.8413508   2.2245367 ]\n",
      "Currrent Action : tensor([[-0.9094]])\n",
      "Next State : [[-0.54048944]\n",
      " [ 0.8413508 ]\n",
      " [ 2.2245367 ]]\n",
      "Reward : [-4.40933483]\n",
      "--------------------------------------------------\n",
      "Step : 62\n",
      "Current State : [-0.6570388  0.7538568  2.9173048]\n",
      "Currrent Action : tensor([[0.4117]])\n",
      "Next State : [[-0.6570388]\n",
      " [ 0.7538568]\n",
      " [ 2.9173048]]\n",
      "Reward : [-5.08239739]\n",
      "--------------------------------------------------\n",
      "Step : 63\n",
      "Current State : [-0.77647066  0.63015336  3.4432335 ]\n",
      "Currrent Action : tensor([[-0.2631]])\n",
      "Next State : [[-0.77647066]\n",
      " [ 0.63015336]\n",
      " [ 3.4432335 ]]\n",
      "Reward : [-6.0846169]\n",
      "--------------------------------------------------\n",
      "Step : 64\n",
      "Current State : [-0.88738656  0.46102616  4.0519896 ]\n",
      "Currrent Action : tensor([[0.9076]])\n",
      "Next State : [[-0.88738656]\n",
      " [ 0.46102616]\n",
      " [ 4.0519896 ]]\n",
      "Reward : [-7.23723177]\n",
      "--------------------------------------------------\n",
      "Step : 65\n",
      "Current State : [-0.96861273  0.24857469  4.5588555 ]\n",
      "Currrent Action : tensor([[1.0740]])\n",
      "Next State : [[-0.96861273]\n",
      " [ 0.24857469]\n",
      " [ 4.5588555 ]]\n",
      "Reward : [-8.73160947]\n",
      "--------------------------------------------------\n",
      "Step : 66\n",
      "Current State : [-0.99995446  0.00954382  4.83329   ]\n",
      "Currrent Action : tensor([[0.5867]])\n",
      "Next State : [[-0.99995446]\n",
      " [ 0.00954382]\n",
      " [ 4.83329   ]]\n",
      "Reward : [-10.43298105]\n",
      "--------------------------------------------------\n",
      "Step : 67\n",
      "Current State : [-0.97227937 -0.23382221  4.91102   ]\n",
      "Currrent Action : tensor([[0.4705]])\n",
      "Next State : [[-0.97227937]\n",
      " [-0.23382221]\n",
      " [ 4.91102   ]]\n",
      "Reward : [-12.14601981]\n",
      "--------------------------------------------------\n",
      "Step : 68\n",
      "Current State : [-0.89423555 -0.4475967   4.561379  ]\n",
      "Currrent Action : tensor([[-1.1618]])\n",
      "Next State : [[-0.89423555]\n",
      " [-0.4475967 ]\n",
      " [ 4.561379  ]]\n",
      "Reward : [-10.85558918]\n",
      "--------------------------------------------------\n",
      "Step : 69\n",
      "Current State : [-0.7710168  -0.63681483  4.5256815 ]\n",
      "Currrent Action : tensor([[2.1700]])\n",
      "Next State : [[-0.7710168 ]\n",
      " [-0.63681483]\n",
      " [ 4.5256815 ]]\n",
      "Reward : [-9.25371353]\n",
      "--------------------------------------------------\n",
      "Step : 70\n",
      "Current State : [-0.6388732 -0.769312   3.7480705]\n",
      "Currrent Action : tensor([[-2.7913]])\n",
      "Next State : [[-0.6388732]\n",
      " [-0.769312 ]\n",
      " [ 3.7480705]]\n",
      "Reward : [-8.06072066]\n",
      "--------------------------------------------------\n",
      "Step : 71\n",
      "Current State : [-0.5090804  -0.86071897  3.1783338 ]\n",
      "Currrent Action : tensor([[0.0483]])\n",
      "Next State : [[-0.5090804 ]\n",
      " [-0.86071897]\n",
      " [ 3.1783338 ]]\n",
      "Reward : [-6.52972766]\n",
      "--------------------------------------------------\n",
      "Step : 72\n",
      "Current State : [-0.39132282 -0.92025346  2.640949  ]\n",
      "Currrent Action : tensor([[0.7210]])\n",
      "Next State : [[-0.39132282]\n",
      " [-0.92025346]\n",
      " [ 2.640949  ]]\n",
      "Reward : [-5.44135658]\n",
      "--------------------------------------------------\n",
      "Step : 73\n",
      "Current State : [-0.2893268 -0.9572304  2.1709027]\n",
      "Currrent Action : tensor([[1.4676]])\n",
      "Next State : [[-0.2893268]\n",
      " [-0.9572304]\n",
      " [ 2.1709027]]\n",
      "Reward : [-4.59181113]\n",
      "--------------------------------------------------\n",
      "Step : 74\n",
      "Current State : [-0.22321121 -0.9747701   1.3683182 ]\n",
      "Currrent Action : tensor([[-0.5644]])\n",
      "Next State : [[-0.22321121]\n",
      " [-0.9747701 ]\n",
      " [ 1.3683182 ]]\n",
      "Reward : [-3.94728878]\n",
      "--------------------------------------------------\n",
      "Step : 75\n",
      "Current State : [-0.1997195  -0.9798531   0.48071846]\n",
      "Currrent Action : tensor([[-1.0435]])\n",
      "Next State : [[-0.1997195 ]\n",
      " [-0.9798531 ]\n",
      " [ 0.48071846]]\n",
      "Reward : [-3.41358913]\n",
      "--------------------------------------------------\n",
      "Step : 76\n",
      "Current State : [-0.21409139 -0.9768136  -0.29379824]\n",
      "Currrent Action : tensor([[-0.2642]])\n",
      "Next State : [[-0.21409139]\n",
      " [-0.9768136 ]\n",
      " [-0.29379824]]\n",
      "Reward : [-3.16269492]\n",
      "--------------------------------------------------\n",
      "Step : 77\n",
      "Current State : [-0.25921947 -0.96581846 -0.9290481 ]\n",
      "Currrent Action : tensor([[0.6491]])\n",
      "Next State : [[-0.25921947]\n",
      " [-0.96581846]\n",
      " [-0.9290481 ]]\n",
      "Reward : [-3.2008421]\n",
      "--------------------------------------------------\n",
      "Step : 78\n",
      "Current State : [-0.3396092  -0.94056666 -1.6857482 ]\n",
      "Currrent Action : tensor([[-0.2156]])\n",
      "Next State : [[-0.3396092 ]\n",
      " [-0.94056666]\n",
      " [-1.6857482 ]]\n",
      "Reward : [-3.44628623]\n",
      "--------------------------------------------------\n",
      "Step : 79\n",
      "Current State : [-0.44447768 -0.8957899  -2.281794  ]\n",
      "Currrent Action : tensor([[0.7292]])\n",
      "Next State : [[-0.44447768]\n",
      " [-0.8957899 ]\n",
      " [-2.281794  ]]\n",
      "Reward : [-3.96073691]\n",
      "--------------------------------------------------\n",
      "Step : 80\n",
      "Current State : [-0.57011527 -0.82156473 -2.9211013 ]\n",
      "Currrent Action : tensor([[0.2169]])\n",
      "Next State : [[-0.57011527]\n",
      " [-0.82156473]\n",
      " [-2.9211013 ]]\n",
      "Reward : [-4.64724027]\n",
      "--------------------------------------------------\n",
      "Step : 81\n",
      "Current State : [-0.69648105 -0.7175752  -3.2767127 ]\n",
      "Currrent Action : tensor([[1.7371]])\n",
      "Next State : [[-0.69648105]\n",
      " [-0.7175752 ]\n",
      " [-3.2767127 ]]\n",
      "Reward : [-5.59755645]\n",
      "--------------------------------------------------\n",
      "Step : 82\n",
      "Current State : [-0.8236917 -0.5670379 -3.9481864]\n",
      "Currrent Action : tensor([[-0.8886]])\n",
      "Next State : [[-0.8236917]\n",
      " [-0.5670379]\n",
      " [-3.9481864]]\n",
      "Reward : [-6.5560573]\n",
      "--------------------------------------------------\n",
      "Step : 83\n",
      "Current State : [-0.92472607 -0.38063335 -4.2484827 ]\n",
      "Currrent Action : tensor([[0.8332]])\n",
      "Next State : [[-0.92472607]\n",
      " [-0.38063335]\n",
      " [-4.2484827 ]]\n",
      "Reward : [-8.00444552]\n",
      "--------------------------------------------------\n",
      "Step : 84\n",
      "Current State : [-0.9840608 -0.1778323 -4.233958 ]\n",
      "Currrent Action : tensor([[2.3539]])\n",
      "Next State : [[-0.9840608]\n",
      " [-0.1778323]\n",
      " [-4.233958 ]]\n",
      "Reward : [-9.37757552]\n",
      "--------------------------------------------------\n",
      "Step : 85\n",
      "Current State : [-0.9992676   0.03826643 -4.3411794 ]\n",
      "Currrent Action : tensor([[0.1744]])\n",
      "Next State : [[-0.9992676 ]\n",
      " [ 0.03826643]\n",
      " [-4.3411794 ]]\n",
      "Reward : [-10.57091023]\n",
      "--------------------------------------------------\n",
      "Step : 86\n",
      "Current State : [-0.9677368   0.25196332 -4.3286543 ]\n",
      "Currrent Action : tensor([[-0.1078]])\n",
      "Next State : [[-0.9677368 ]\n",
      " [ 0.25196332]\n",
      " [-4.3286543 ]]\n",
      "Reward : [-11.51517118]\n",
      "--------------------------------------------------\n",
      "Step : 87\n",
      "Current State : [-0.8985931   0.43878287 -3.9907053 ]\n",
      "Currrent Action : tensor([[0.9932]])\n",
      "Next State : [[-0.8985931 ]\n",
      " [ 0.43878287]\n",
      " [-3.9907053 ]]\n",
      "Reward : [-10.20881139]\n",
      "--------------------------------------------------\n",
      "Step : 88\n",
      "Current State : [-0.8125256   0.58292556 -3.361618  ]\n",
      "Currrent Action : tensor([[2.0168]])\n",
      "Next State : [[-0.8125256 ]\n",
      " [ 0.58292556]\n",
      " [-3.361618  ]]\n",
      "Reward : [-8.81841697]\n",
      "--------------------------------------------------\n",
      "Step : 89\n",
      "Current State : [-0.721677   0.6922299 -2.844995 ]\n",
      "Currrent Action : tensor([[0.5295]])\n",
      "Next State : [[-0.721677 ]\n",
      " [ 0.6922299]\n",
      " [-2.844995 ]]\n",
      "Reward : [-7.47703926]\n",
      "--------------------------------------------------\n",
      "Step : 90\n",
      "Current State : [-0.64120126  0.7673727  -2.203185  ]\n",
      "Currrent Action : tensor([[0.8176]])\n",
      "Next State : [[-0.64120126]\n",
      " [ 0.7673727 ]\n",
      " [-2.203185  ]]\n",
      "Reward : [-6.4602839]\n",
      "--------------------------------------------------\n",
      "Step : 91\n",
      "Current State : [-0.5815426   0.81351596 -1.5087808 ]\n",
      "Currrent Action : tensor([[0.7925]])\n",
      "Next State : [[-0.5815426 ]\n",
      " [ 0.81351596]\n",
      " [-1.5087808 ]]\n",
      "Reward : [-5.62468017]\n",
      "--------------------------------------------------\n",
      "Step : 92\n",
      "Current State : [-0.54197735  0.8403931  -0.95670855]\n",
      "Currrent Action : tensor([[-0.3871]])\n",
      "Next State : [[-0.54197735]\n",
      " [ 0.8403931 ]\n",
      " [-0.95670855]]\n",
      "Reward : [-5.03011318]\n",
      "--------------------------------------------------\n",
      "Step : 93\n",
      "Current State : [-0.5270698   0.849822   -0.35278678]\n",
      "Currrent Action : tensor([[-0.1758]])\n",
      "Next State : [[-0.5270698 ]\n",
      " [ 0.849822  ]\n",
      " [-0.35278678]]\n",
      "Reward : [-4.68651464]\n",
      "--------------------------------------------------\n",
      "Step : 94\n",
      "Current State : [-0.531049    0.8473411   0.09378497]\n",
      "Currrent Action : tensor([[-1.2720]])\n",
      "Next State : [[-0.531049  ]\n",
      " [ 0.8473411 ]\n",
      " [ 0.09378497]]\n",
      "Reward : [-4.53370666]\n",
      "--------------------------------------------------\n",
      "Step : 95\n",
      "Current State : [-0.5629162   0.82651395  0.7614368 ]\n",
      "Currrent Action : tensor([[0.2143]])\n",
      "Next State : [[-0.5629162 ]\n",
      " [ 0.82651395]\n",
      " [ 0.7614368 ]]\n",
      "Reward : [-4.54052856]\n",
      "--------------------------------------------------\n",
      "Step : 96\n",
      "Current State : [-0.62775546  0.7784106   1.6151278 ]\n",
      "Currrent Action : tensor([[1.5587]])\n",
      "Next State : [[-0.62775546]\n",
      " [ 0.7784106 ]\n",
      " [ 1.6151278 ]]\n",
      "Reward : [-4.76369505]\n",
      "--------------------------------------------------\n",
      "Step : 97\n",
      "Current State : [-0.7091328  0.7050749  2.1920238]\n",
      "Currrent Action : tensor([[-0.0461]])\n",
      "Next State : [[-0.7091328]\n",
      " [ 0.7050749]\n",
      " [ 2.1920238]]\n",
      "Reward : [-5.32094815]\n",
      "--------------------------------------------------\n",
      "Step : 98\n",
      "Current State : [-0.80046195  0.59938353  2.795961  ]\n",
      "Currrent Action : tensor([[0.5009]])\n",
      "Next State : [[-0.80046195]\n",
      " [ 0.59938353]\n",
      " [ 2.795961  ]]\n",
      "Reward : [-6.04592994]\n",
      "--------------------------------------------------\n",
      "Step : 99\n",
      "Current State : [-0.88476324  0.46604082  3.158403  ]\n",
      "Currrent Action : tensor([[-0.5806]])\n",
      "Next State : [[-0.88476324]\n",
      " [ 0.46604082]\n",
      " [ 3.158403  ]]\n",
      "Reward : [-7.02638764]\n",
      "learning iteration : 11\n",
      "--------------------------------------------------\n",
      "Step : 0\n",
      "Current State : [ 0.9984615  -0.05544945  0.55056536]\n",
      "Currrent Action : tensor([[-1.2061]])\n",
      "Next State : [[ 0.9984615 ]\n",
      " [-0.05544945]\n",
      " [ 0.55056536]]\n",
      "Reward : [-0.07133585]\n",
      "--------------------------------------------------\n",
      "Step : 1\n",
      "Current State : [ 0.9993899  -0.03492725  0.41087097]\n",
      "Currrent Action : tensor([[-0.6540]])\n",
      "Next State : [[ 0.9993899 ]\n",
      " [-0.03492725]\n",
      " [ 0.41087097]]\n",
      "Reward : [-0.0338178]\n",
      "--------------------------------------------------\n",
      "Step : 2\n",
      "Current State : [ 0.9997228  -0.02354475  0.22774865]\n",
      "Currrent Action : tensor([[-1.0462]])\n",
      "Next State : [[ 0.9997228 ]\n",
      " [-0.02354475]\n",
      " [ 0.22774865]]\n",
      "Reward : [-0.0191964]\n",
      "--------------------------------------------------\n",
      "Step : 3\n",
      "Current State : [ 0.99989307 -0.01462529  0.17842218]\n",
      "Currrent Action : tensor([[-0.2111]])\n",
      "Next State : [[ 0.99989307]\n",
      " [-0.01462529]\n",
      " [ 0.17842218]]\n",
      "Reward : [-0.00578597]\n",
      "--------------------------------------------------\n",
      "Step : 4\n",
      "Current State : [ 0.99986285 -0.01655993 -0.03869747]\n",
      "Currrent Action : tensor([[-1.3743]])\n",
      "Next State : [[ 0.99986285]\n",
      " [-0.01655993]\n",
      " [-0.03869747]]\n",
      "Reward : [-0.00528617]\n",
      "--------------------------------------------------\n",
      "Step : 5\n",
      "Current State : [ 0.9997722  -0.0213433  -0.09568472]\n",
      "Currrent Action : tensor([[-0.2971]])\n",
      "Next State : [[ 0.9997722 ]\n",
      " [-0.0213433 ]\n",
      " [-0.09568472]]\n",
      "Reward : [-0.00051228]\n",
      "--------------------------------------------------\n",
      "Step : 6\n",
      "Current State : [ 0.9992531  -0.03864328 -0.34615952]\n",
      "Currrent Action : tensor([[-1.5631]])\n",
      "Next State : [[ 0.9992531 ]\n",
      " [-0.03864328]\n",
      " [-0.34615952]]\n",
      "Reward : [-0.00381449]\n",
      "--------------------------------------------------\n",
      "Step : 7\n",
      "Current State : [ 0.998889   -0.04712466 -0.16978438]\n",
      "Currrent Action : tensor([[1.3691]])\n",
      "Next State : [[ 0.998889  ]\n",
      " [-0.04712466]\n",
      " [-0.16978438]]\n",
      "Reward : [-0.01535099]\n",
      "--------------------------------------------------\n",
      "Step : 8\n",
      "Current State : [ 0.99835116 -0.0574016  -0.2058209 ]\n",
      "Currrent Action : tensor([[-0.0046]])\n",
      "Next State : [[ 0.99835116]\n",
      " [-0.0574016 ]\n",
      " [-0.2058209 ]]\n",
      "Reward : [-0.00510507]\n",
      "--------------------------------------------------\n",
      "Step : 9\n",
      "Current State : [ 0.9977682  -0.06677248 -0.18778078]\n",
      "Currrent Action : tensor([[0.4073]])\n",
      "Next State : [[ 0.9977682 ]\n",
      " [-0.06677248]\n",
      " [-0.18778078]]\n",
      "Reward : [-0.00770067]\n",
      "--------------------------------------------------\n",
      "Step : 10\n",
      "Current State : [ 0.9979709  -0.06367211  0.06213985]\n",
      "Currrent Action : tensor([[2.6531]])\n",
      "Next State : [[ 0.9979709 ]\n",
      " [-0.06367211]\n",
      " [ 0.06213985]]\n",
      "Reward : [-0.01199137]\n",
      "--------------------------------------------------\n",
      "Step : 11\n",
      "Current State : [ 0.9976816  -0.0680543  -0.08783461]\n",
      "Currrent Action : tensor([[-0.6815]])\n",
      "Next State : [[ 0.9976816 ]\n",
      " [-0.0680543 ]\n",
      " [-0.08783461]]\n",
      "Reward : [-0.00491016]\n",
      "--------------------------------------------------\n",
      "Step : 12\n",
      "Current State : [ 0.99661386 -0.08222391 -0.28419796]\n",
      "Currrent Action : tensor([[-0.9688]])\n",
      "Next State : [[ 0.99661386]\n",
      " [-0.08222391]\n",
      " [-0.28419796]]\n",
      "Reward : [-0.00634865]\n",
      "--------------------------------------------------\n",
      "Step : 13\n",
      "Current State : [ 0.99516815 -0.09818555 -0.32054302]\n",
      "Currrent Action : tensor([[0.1688]])\n",
      "Next State : [[ 0.99516815]\n",
      " [-0.09818555]\n",
      " [-0.32054302]]\n",
      "Reward : [-0.01488141]\n",
      "--------------------------------------------------\n",
      "Step : 14\n",
      "Current State : [ 0.9928194  -0.11962245 -0.43131214]\n",
      "Currrent Action : tensor([[-0.2475]])\n",
      "Next State : [[ 0.9928194 ]\n",
      " [-0.11962245]\n",
      " [-0.43131214]]\n",
      "Reward : [-0.0200076]\n",
      "--------------------------------------------------\n",
      "Step : 15\n",
      "Current State : [ 0.99042034 -0.1380851  -0.37236258]\n",
      "Currrent Action : tensor([[0.9911]])\n",
      "Next State : [[ 0.99042034]\n",
      " [-0.1380851 ]\n",
      " [-0.37236258]]\n",
      "Reward : [-0.03396363]\n",
      "--------------------------------------------------\n",
      "Step : 16\n",
      "Current State : [ 0.98825437 -0.15281801 -0.29782847]\n",
      "Currrent Action : tensor([[1.1873]])\n",
      "Next State : [[ 0.98825437]\n",
      " [-0.15281801]\n",
      " [-0.29782847]]\n",
      "Reward : [-0.03446505]\n",
      "--------------------------------------------------\n",
      "Step : 17\n",
      "Current State : [ 0.98596656 -0.16694286 -0.28618097]\n",
      "Currrent Action : tensor([[0.8417]])\n",
      "Next State : [[ 0.98596656]\n",
      " [-0.16694286]\n",
      " [-0.28618097]]\n",
      "Reward : [-0.03311614]\n",
      "--------------------------------------------------\n",
      "Step : 18\n",
      "Current State : [ 0.9821392  -0.1881555  -0.43111134]\n",
      "Currrent Action : tensor([[-0.1315]])\n",
      "Next State : [[ 0.9821392 ]\n",
      " [-0.1881555 ]\n",
      " [-0.43111134]]\n",
      "Reward : [-0.03633999]\n",
      "--------------------------------------------------\n",
      "Step : 19\n",
      "Current State : [ 0.9794873  -0.20150594 -0.27222797]\n",
      "Currrent Action : tensor([[2.2472]])\n",
      "Next State : [[ 0.9794873 ]\n",
      " [-0.20150594]\n",
      " [-0.27222797]]\n",
      "Reward : [-0.05841404]\n",
      "--------------------------------------------------\n",
      "Step : 20\n",
      "Current State : [ 0.9764951  -0.21553956 -0.28698367]\n",
      "Currrent Action : tensor([[0.9092]])\n",
      "Next State : [[ 0.9764951 ]\n",
      " [-0.21553956]\n",
      " [-0.28698367]]\n",
      "Reward : [-0.04940382]\n",
      "--------------------------------------------------\n",
      "Step : 21\n",
      "Current State : [ 0.9700695  -0.24282737 -0.560701  ]\n",
      "Currrent Action : tensor([[-0.7471]])\n",
      "Next State : [[ 0.9700695 ]\n",
      " [-0.24282737]\n",
      " [-0.560701  ]]\n",
      "Reward : [-0.05598921]\n",
      "--------------------------------------------------\n",
      "Step : 22\n",
      "Current State : [ 0.96194774 -0.27323347 -0.6294682 ]\n",
      "Currrent Action : tensor([[0.7557]])\n",
      "Next State : [[ 0.96194774]\n",
      " [-0.27323347]\n",
      " [-0.6294682 ]]\n",
      "Reward : [-0.09217161]\n",
      "--------------------------------------------------\n",
      "Step : 23\n",
      "Current State : [ 0.94531816 -0.32614967 -1.1094968 ]\n",
      "Currrent Action : tensor([[-1.8340]])\n",
      "Next State : [[ 0.94531816]\n",
      " [-0.32614967]\n",
      " [-1.1094968 ]]\n",
      "Reward : [-0.11957878]\n",
      "--------------------------------------------------\n",
      "Step : 24\n",
      "Current State : [ 0.91526324 -0.40285635 -1.6481575 ]\n",
      "Currrent Action : tensor([[-1.9603]])\n",
      "Next State : [[ 0.91526324]\n",
      " [-0.40285635]\n",
      " [-1.6481575 ]]\n",
      "Reward : [-0.2373164]\n",
      "--------------------------------------------------\n",
      "Step : 25\n",
      "Current State : [ 0.87785363 -0.478929   -1.6959759 ]\n",
      "Currrent Action : tensor([[1.6955]])\n",
      "Next State : [[ 0.87785363]\n",
      " [-0.478929  ]\n",
      " [-1.6959759 ]]\n",
      "Reward : [-0.4464396]\n",
      "--------------------------------------------------\n",
      "Step : 26\n",
      "Current State : [ 0.8268719 -0.5623903 -1.9567888]\n",
      "Currrent Action : tensor([[0.6559]])\n",
      "Next State : [[ 0.8268719]\n",
      " [-0.5623903]\n",
      " [-1.9567888]]\n",
      "Reward : [-0.53749825]\n",
      "--------------------------------------------------\n",
      "Step : 27\n",
      "Current State : [ 0.762616  -0.6468515 -2.1234972]\n",
      "Currrent Action : tensor([[1.7006]])\n",
      "Next State : [[ 0.762616 ]\n",
      " [-0.6468515]\n",
      " [-2.1234972]]\n",
      "Reward : [-0.74253006]\n",
      "--------------------------------------------------\n",
      "Step : 28\n",
      "Current State : [ 0.67215884 -0.740407   -2.6045406 ]\n",
      "Currrent Action : tensor([[0.0273]])\n",
      "Next State : [[ 0.67215884]\n",
      " [-0.740407  ]\n",
      " [-2.6045406 ]]\n",
      "Reward : [-0.94576473]\n",
      "--------------------------------------------------\n",
      "Step : 29\n",
      "Current State : [ 0.5424216 -0.8401064 -3.2760758]\n",
      "Currrent Action : tensor([[-0.7749]])\n",
      "Next State : [[ 0.5424216]\n",
      " [-0.8401064]\n",
      " [-3.2760758]]\n",
      "Reward : [-1.37397872]\n",
      "--------------------------------------------------\n",
      "Step : 30\n",
      "Current State : [ 0.36549777 -0.93081224 -3.9829876 ]\n",
      "Currrent Action : tensor([[-0.5122]])\n",
      "Next State : [[ 0.36549777]\n",
      " [-0.93081224]\n",
      " [-3.9829876 ]]\n",
      "Reward : [-2.06849496]\n",
      "--------------------------------------------------\n",
      "Step : 31\n",
      "Current State : [ 0.13280022 -0.9911428  -4.819476  ]\n",
      "Currrent Action : tensor([[-0.9225]])\n",
      "Next State : [[ 0.13280022]\n",
      " [-0.9911428 ]\n",
      " [-4.819476  ]]\n",
      "Reward : [-3.01919056]\n",
      "--------------------------------------------------\n",
      "Step : 32\n",
      "Current State : [-0.14599602 -0.9892852  -5.594268  ]\n",
      "Currrent Action : tensor([[-0.2096]])\n",
      "Next State : [[-0.14599602]\n",
      " [-0.9892852 ]\n",
      " [-5.594268  ]]\n",
      "Reward : [-4.3894803]\n",
      "--------------------------------------------------\n",
      "Step : 33\n",
      "Current State : [-0.43346235 -0.9011717  -6.036232  ]\n",
      "Currrent Action : tensor([[2.5090]])\n",
      "Next State : [[-0.43346235]\n",
      " [-0.9011717 ]\n",
      " [-6.036232  ]]\n",
      "Reward : [-6.08275774]\n",
      "--------------------------------------------------\n",
      "Step : 34\n",
      "Current State : [-0.6999091 -0.7142319 -6.5387783]\n",
      "Currrent Action : tensor([[1.1555]])\n",
      "Next State : [[-0.6999091]\n",
      " [-0.7142319]\n",
      " [-6.5387783]]\n",
      "Reward : [-7.72182112]\n",
      "--------------------------------------------------\n",
      "Step : 35\n",
      "Current State : [-0.90084326 -0.43414444 -6.9287558 ]\n",
      "Currrent Action : tensor([[0.9713]])\n",
      "Next State : [[-0.90084326]\n",
      " [-0.43414444]\n",
      " [-6.9287558 ]]\n",
      "Reward : [-9.78053393]\n",
      "--------------------------------------------------\n",
      "Step : 36\n",
      "Current State : [-0.99674416 -0.08062938 -7.3674264 ]\n",
      "Currrent Action : tensor([[-0.7537]])\n",
      "Next State : [[-0.99674416]\n",
      " [-0.08062938]\n",
      " [-7.3674264 ]]\n",
      "Reward : [-12.05091342]\n",
      "--------------------------------------------------\n",
      "Step : 37\n",
      "Current State : [-0.95948344  0.28176504 -7.327004  ]\n",
      "Currrent Action : tensor([[0.6726]])\n",
      "Next State : [[-0.95948344]\n",
      " [ 0.28176504]\n",
      " [-7.327004  ]]\n",
      "Reward : [-14.79730958]\n",
      "--------------------------------------------------\n",
      "Step : 38\n",
      "Current State : [-0.80480766  0.59353566 -6.9962354 ]\n",
      "Currrent Action : tensor([[0.7963]])\n",
      "Next State : [[-0.80480766]\n",
      " [ 0.59353566]\n",
      " [-6.9962354 ]]\n",
      "Reward : [-13.52563729]\n",
      "--------------------------------------------------\n",
      "Step : 39\n",
      "Current State : [-0.5761074  0.817374  -6.4278603]\n",
      "Currrent Action : tensor([[0.8215]])\n",
      "Next State : [[-0.5761074]\n",
      " [ 0.817374 ]\n",
      " [-6.4278603]]\n",
      "Reward : [-11.1761819]\n",
      "--------------------------------------------------\n",
      "Step : 40\n",
      "Current State : [-0.31266978  0.9498619  -5.919118  ]\n",
      "Currrent Action : tensor([[-0.6953]])\n",
      "Next State : [[-0.31266978]\n",
      " [ 0.9498619 ]\n",
      " [-5.919118  ]]\n",
      "Reward : [-8.90537532]\n",
      "--------------------------------------------------\n",
      "Step : 41\n",
      "Current State : [-0.0575679  0.9983416 -5.2080536]\n",
      "Currrent Action : tensor([[-0.0089]])\n",
      "Next State : [[-0.0575679]\n",
      " [ 0.9983416]\n",
      " [-5.2080536]]\n",
      "Reward : [-7.0711567]\n",
      "--------------------------------------------------\n",
      "Step : 42\n",
      "Current State : [ 0.15674374  0.9876393  -4.29985   ]\n",
      "Currrent Action : tensor([[1.0630]])\n",
      "Next State : [[ 0.15674374]\n",
      " [ 0.9876393 ]\n",
      " [-4.29985   ]]\n",
      "Reward : [-5.36518611]\n",
      "--------------------------------------------------\n",
      "Step : 43\n",
      "Current State : [ 0.33170137  0.9433844  -3.6142745 ]\n",
      "Currrent Action : tensor([[-0.3677]])\n",
      "Next State : [[ 0.33170137]\n",
      " [ 0.9433844 ]\n",
      " [-3.6142745 ]]\n",
      "Reward : [-3.84671592]\n",
      "--------------------------------------------------\n",
      "Step : 44\n",
      "Current State : [ 0.4643673  0.8856427 -2.8962717]\n",
      "Currrent Action : tensor([[0.0698]])\n",
      "Next State : [[ 0.4643673]\n",
      " [ 0.8856427]\n",
      " [-2.8962717]]\n",
      "Reward : [-2.82582713]\n",
      "--------------------------------------------------\n",
      "Step : 45\n",
      "Current State : [ 0.5611713   0.82769966 -2.2576034 ]\n",
      "Currrent Action : tensor([[-0.1704]])\n",
      "Next State : [[ 0.5611713 ]\n",
      " [ 0.82769966]\n",
      " [-2.2576034 ]]\n",
      "Reward : [-2.02234277]\n",
      "--------------------------------------------------\n",
      "Step : 46\n",
      "Current State : [ 0.62994784  0.7766375  -1.7137153 ]\n",
      "Currrent Action : tensor([[-0.5126]])\n",
      "Next State : [[ 0.62994784]\n",
      " [ 0.7766375 ]\n",
      " [-1.7137153 ]]\n",
      "Reward : [-1.46055742]\n",
      "--------------------------------------------------\n",
      "Step : 47\n",
      "Current State : [ 0.6744902  0.7382838 -1.1757594]\n",
      "Currrent Action : tensor([[-0.2968]])\n",
      "Next State : [[ 0.6744902]\n",
      " [ 0.7382838]\n",
      " [-1.1757594]]\n",
      "Reward : [-1.08464296]\n",
      "--------------------------------------------------\n",
      "Step : 48\n",
      "Current State : [ 0.6888605   0.7248939  -0.39283866]\n",
      "Currrent Action : tensor([[1.5281]])\n",
      "Next State : [[ 0.6888605 ]\n",
      " [ 0.7248939 ]\n",
      " [-0.39283866]]\n",
      "Reward : [-0.83034332]\n",
      "--------------------------------------------------\n",
      "Step : 49\n",
      "Current State : [0.6819323  0.73141533 0.19029476]\n",
      "Currrent Action : tensor([[0.2631]])\n",
      "Next State : [[0.6819323 ]\n",
      " [0.73141533]\n",
      " [0.19029476]]\n",
      "Reward : [-0.67302847]\n",
      "--------------------------------------------------\n",
      "Step : 50\n",
      "Current State : [0.6548392  0.75576824 0.7286268 ]\n",
      "Currrent Action : tensor([[-0.0682]])\n",
      "Next State : [[0.6548392 ]\n",
      " [0.75576824]\n",
      " [0.7286268 ]]\n",
      "Reward : [-0.67667406]\n",
      "--------------------------------------------------\n",
      "Step : 51\n",
      "Current State : [0.6029576  0.79777324 1.3353311 ]\n",
      "Currrent Action : tensor([[0.2659]])\n",
      "Next State : [[0.6029576 ]\n",
      " [0.79777324]\n",
      " [1.3353311 ]]\n",
      "Reward : [-0.78731201]\n",
      "--------------------------------------------------\n",
      "Step : 52\n",
      "Current State : [0.5322642 0.8465783 1.7186091]\n",
      "Currrent Action : tensor([[-1.4337]])\n",
      "Next State : [[0.5322642]\n",
      " [0.8465783]\n",
      " [1.7186091]]\n",
      "Reward : [-1.03339047]\n",
      "--------------------------------------------------\n",
      "Step : 53\n",
      "Current State : [0.42707497 0.90421623 2.4003482 ]\n",
      "Currrent Action : tensor([[0.3120]])\n",
      "Next State : [[0.42707497]\n",
      " [0.90421623]\n",
      " [2.4003482 ]]\n",
      "Reward : [-1.31459677]\n",
      "--------------------------------------------------\n",
      "Step : 54\n",
      "Current State : [0.2930577  0.95609474 2.8766403 ]\n",
      "Currrent Action : tensor([[-1.3458]])\n",
      "Next State : [[0.2930577 ]\n",
      " [0.95609474]\n",
      " [2.8766403 ]]\n",
      "Reward : [-1.85384095]\n",
      "--------------------------------------------------\n",
      "Step : 55\n",
      "Current State : [0.12757333 0.99182916 3.3900294 ]\n",
      "Currrent Action : tensor([[-1.3579]])\n",
      "Next State : [[0.12757333]\n",
      " [0.99182916]\n",
      " [3.3900294 ]]\n",
      "Reward : [-2.45082836]\n",
      "--------------------------------------------------\n",
      "Step : 56\n",
      "Current State : [-0.08116081  0.996701    4.183442  ]\n",
      "Currrent Action : tensor([[0.3303]])\n",
      "Next State : [[-0.08116081]\n",
      " [ 0.996701  ]\n",
      " [ 4.183442  ]]\n",
      "Reward : [-3.23122558]\n",
      "--------------------------------------------------\n",
      "Step : 57\n",
      "Current State : [-0.32702452  0.94501585  5.038061  ]\n",
      "Currrent Action : tensor([[0.7140]])\n",
      "Next State : [[-0.32702452]\n",
      " [ 0.94501585]\n",
      " [ 5.038061  ]]\n",
      "Reward : [-4.47988629]\n",
      "--------------------------------------------------\n",
      "Step : 58\n",
      "Current State : [-0.58282006  0.8126012   5.780832  ]\n",
      "Currrent Action : tensor([[0.2267]])\n",
      "Next State : [[-0.58282006]\n",
      " [ 0.8126012 ]\n",
      " [ 5.780832  ]]\n",
      "Reward : [-6.16328141]\n",
      "--------------------------------------------------\n",
      "Step : 59\n",
      "Current State : [-0.81421673  0.580561    6.5836797 ]\n",
      "Currrent Action : tensor([[1.2893]])\n",
      "Next State : [[-0.81421673]\n",
      " [ 0.580561  ]\n",
      " [ 6.5836797 ]]\n",
      "Reward : [-8.15267398]\n",
      "--------------------------------------------------\n",
      "Step : 60\n",
      "Current State : [-0.96805    0.2507573  7.3191004]\n",
      "Currrent Action : tensor([[3.5116]])\n",
      "Next State : [[-0.96805  ]\n",
      " [ 0.2507573]\n",
      " [ 7.3191004]]\n",
      "Reward : [-10.69985139]\n",
      "--------------------------------------------------\n",
      "Step : 61\n",
      "Current State : [-0.99123746 -0.1320921   7.7188354 ]\n",
      "Currrent Action : tensor([[1.4111]])\n",
      "Next State : [[-0.99123746]\n",
      " [-0.1320921 ]\n",
      " [ 7.7188354 ]]\n",
      "Reward : [-13.70021035]\n",
      "--------------------------------------------------\n",
      "Step : 62\n",
      "Current State : [-0.86859614 -0.4955207   7.7190986 ]\n",
      "Currrent Action : tensor([[0.6622]])\n",
      "Next State : [[-0.86859614]\n",
      " [-0.4955207 ]\n",
      " [ 7.7190986 ]]\n",
      "Reward : [-15.01324386]\n",
      "--------------------------------------------------\n",
      "Step : 63\n",
      "Current State : [-0.6275357  -0.77858776  7.479561  ]\n",
      "Currrent Action : tensor([[0.8807]])\n",
      "Next State : [[-0.6275357 ]\n",
      " [-0.77858776]\n",
      " [ 7.479561  ]]\n",
      "Reward : [-12.84018426]\n",
      "--------------------------------------------------\n",
      "Step : 64\n",
      "Current State : [-0.32554165 -0.94552773  6.9359913 ]\n",
      "Currrent Action : tensor([[0.2691]])\n",
      "Next State : [[-0.32554165]\n",
      " [-0.94552773]\n",
      " [ 6.9359913 ]]\n",
      "Reward : [-10.6532677]\n",
      "--------------------------------------------------\n",
      "Step : 65\n",
      "Current State : [-0.02306562 -0.9997339   6.1703367 ]\n",
      "Currrent Action : tensor([[-0.3767]])\n",
      "Next State : [[-0.02306562]\n",
      " [-0.9997339 ]\n",
      " [ 6.1703367 ]]\n",
      "Reward : [-8.42999239]\n",
      "--------------------------------------------------\n",
      "Step : 66\n",
      "Current State : [ 0.23925756 -0.97095615  5.293376  ]\n",
      "Currrent Action : tensor([[-0.8477]])\n",
      "Next State : [[ 0.23925756]\n",
      " [-0.97095615]\n",
      " [ 5.293376  ]]\n",
      "Reward : [-6.34842671]\n",
      "--------------------------------------------------\n",
      "Step : 67\n",
      "Current State : [ 0.44846693 -0.8937994   4.468965  ]\n",
      "Currrent Action : tensor([[-0.6413]])\n",
      "Next State : [[ 0.44846693]\n",
      " [-0.8937994 ]\n",
      " [ 4.468965  ]]\n",
      "Reward : [-4.56915402]\n",
      "--------------------------------------------------\n",
      "Step : 68\n",
      "Current State : [ 0.6116571 -0.791123   3.862083 ]\n",
      "Currrent Action : tensor([[0.4231]])\n",
      "Next State : [[ 0.6116571]\n",
      " [-0.791123 ]\n",
      " [ 3.862083 ]]\n",
      "Reward : [-3.2200201]\n",
      "--------------------------------------------------\n",
      "Step : 69\n",
      "Current State : [ 0.7375508  -0.67529166  3.4256551 ]\n",
      "Currrent Action : tensor([[1.0461]])\n",
      "Next State : [[ 0.7375508 ]\n",
      " [-0.67529166]\n",
      " [ 3.4256551 ]]\n",
      "Reward : [-2.32557974]\n",
      "--------------------------------------------------\n",
      "Step : 70\n",
      "Current State : [ 0.8299837 -0.5577876  2.9928443]\n",
      "Currrent Action : tensor([[0.4911]])\n",
      "Next State : [[ 0.8299837]\n",
      " [-0.5577876]\n",
      " [ 2.9928443]]\n",
      "Reward : [-1.72336716]\n",
      "--------------------------------------------------\n",
      "Step : 71\n",
      "Current State : [ 0.8933746 -0.4493126  2.5144417]\n",
      "Currrent Action : tensor([[-0.4004]])\n",
      "Next State : [[ 0.8933746]\n",
      " [-0.4493126]\n",
      " [ 2.5144417]]\n",
      "Reward : [-1.246002]\n",
      "--------------------------------------------------\n",
      "Step : 72\n",
      "Current State : [ 0.935367   -0.35367858  2.0898952 ]\n",
      "Currrent Action : tensor([[-0.5837]])\n",
      "Next State : [[ 0.935367  ]\n",
      " [-0.35367858]\n",
      " [ 2.0898952 ]]\n",
      "Reward : [-0.84973455]\n",
      "--------------------------------------------------\n",
      "Step : 73\n",
      "Current State : [ 0.9633158  -0.26837048  1.7959986 ]\n",
      "Currrent Action : tensor([[-0.1909]])\n",
      "Next State : [[ 0.9633158 ]\n",
      " [-0.26837048]\n",
      " [ 1.7959986 ]]\n",
      "Reward : [-0.56748562]\n",
      "--------------------------------------------------\n",
      "Step : 74\n",
      "Current State : [ 0.9786591  -0.20549063  1.2947206 ]\n",
      "Currrent Action : tensor([[-2.4543]])\n",
      "Next State : [[ 0.9786591 ]\n",
      " [-0.20549063]\n",
      " [ 1.2947206 ]]\n",
      "Reward : [-0.40038253]\n",
      "--------------------------------------------------\n",
      "Step : 75\n",
      "Current State : [ 0.98930156 -0.1458849   1.2111524 ]\n",
      "Currrent Action : tensor([[0.4703]])\n",
      "Next State : [[ 0.98930156]\n",
      " [-0.1458849 ]\n",
      " [ 1.2111524 ]]\n",
      "Reward : [-0.21068589]\n",
      "--------------------------------------------------\n",
      "Step : 76\n",
      "Current State : [ 0.99495417 -0.10033043  0.91815716]\n",
      "Currrent Action : tensor([[-1.2239]])\n",
      "Next State : [[ 0.99495417]\n",
      " [-0.10033043]\n",
      " [ 0.91815716]]\n",
      "Reward : [-0.16962203]\n",
      "--------------------------------------------------\n",
      "Step : 77\n",
      "Current State : [ 0.9984386  -0.05586076  0.8921936 ]\n",
      "Currrent Action : tensor([[0.3286]])\n",
      "Next State : [[ 0.9984386 ]\n",
      " [-0.05586076]\n",
      " [ 0.8921936 ]]\n",
      "Reward : [-0.09450936]\n",
      "--------------------------------------------------\n",
      "Step : 78\n",
      "Current State : [ 0.9998466 -0.0175161  0.767457 ]\n",
      "Currrent Action : tensor([[-0.5523]])\n",
      "Next State : [[ 0.9998466]\n",
      " [-0.0175161]\n",
      " [ 0.767457 ]]\n",
      "Reward : [-0.08302962]\n",
      "--------------------------------------------------\n",
      "Step : 79\n",
      "Current State : [0.9997805 0.0209514 0.7693986]\n",
      "Currrent Action : tensor([[0.1005]])\n",
      "Next State : [[0.9997805]\n",
      " [0.0209514]\n",
      " [0.7693986]]\n",
      "Reward : [-0.05921598]\n",
      "--------------------------------------------------\n",
      "Step : 80\n",
      "Current State : [0.9984939  0.05486239 0.67874026]\n",
      "Currrent Action : tensor([[-0.7091]])\n",
      "Next State : [[0.9984939 ]\n",
      " [0.05486239]\n",
      " [0.67874026]]\n",
      "Reward : [-0.06013934]\n",
      "--------------------------------------------------\n",
      "Step : 81\n",
      "Current State : [0.9955448  0.09428976 0.7908019 ]\n",
      "Currrent Action : tensor([[0.4728]])\n",
      "Next State : [[0.9955448 ]\n",
      " [0.09428976]\n",
      " [0.7908019 ]]\n",
      "Reward : [-0.04930525]\n",
      "--------------------------------------------------\n",
      "Step : 82\n",
      "Current State : [0.99065685 0.13637827 0.84749126]\n",
      "Currrent Action : tensor([[-0.0935]])\n",
      "Next State : [[0.99065685]\n",
      " [0.13637827]\n",
      " [0.84749126]]\n",
      "Reward : [-0.07146254]\n",
      "--------------------------------------------------\n",
      "Step : 83\n",
      "Current State : [0.9827127  0.18513715 0.9881365 ]\n",
      "Currrent Action : tensor([[0.2557]])\n",
      "Next State : [[0.9827127 ]\n",
      " [0.18513715]\n",
      " [0.9881365 ]]\n",
      "Reward : [-0.09060505]\n",
      "--------------------------------------------------\n",
      "Step : 84\n",
      "Current State : [0.9729478  0.23102525 0.93839765]\n",
      "Currrent Action : tensor([[-1.2573]])\n",
      "Next State : [[0.9729478 ]\n",
      " [0.23102525]\n",
      " [0.93839765]]\n",
      "Reward : [-0.13389681]\n",
      "--------------------------------------------------\n",
      "Step : 85\n",
      "Current State : [0.96022224 0.2792368  0.9973579 ]\n",
      "Currrent Action : tensor([[-0.7621]])\n",
      "Next State : [[0.96022224]\n",
      " [0.2792368 ]\n",
      " [0.9973579 ]]\n",
      "Reward : [-0.14298995]\n",
      "--------------------------------------------------\n",
      "Step : 86\n",
      "Current State : [0.9413824 0.3373413 1.2218406]\n",
      "Currrent Action : tensor([[0.1004]])\n",
      "Next State : [[0.9413824]\n",
      " [0.3373413]\n",
      " [1.2218406]]\n",
      "Reward : [-0.1795709]\n",
      "--------------------------------------------------\n",
      "Step : 87\n",
      "Current State : [0.9077809  0.41944474 1.7748467 ]\n",
      "Currrent Action : tensor([[2.0082]])\n",
      "Next State : [[0.9077809 ]\n",
      " [0.41944474]\n",
      " [1.7748467 ]]\n",
      "Reward : [-0.27168823]\n",
      "--------------------------------------------------\n",
      "Step : 88\n",
      "Current State : [0.8583443  0.51307416 2.1185775 ]\n",
      "Currrent Action : tensor([[0.1943]])\n",
      "Next State : [[0.8583443 ]\n",
      " [0.51307416]\n",
      " [2.1185775 ]]\n",
      "Reward : [-0.50239071]\n",
      "--------------------------------------------------\n",
      "Step : 89\n",
      "Current State : [0.7967299  0.60433555 2.2033832 ]\n",
      "Currrent Action : tensor([[-2.0195]])\n",
      "Next State : [[0.7967299 ]\n",
      " [0.60433555]\n",
      " [2.2033832 ]]\n",
      "Reward : [-0.74310205]\n",
      "--------------------------------------------------\n",
      "Step : 90\n",
      "Current State : [0.7103174  0.70388156 2.6383145 ]\n",
      "Currrent Action : tensor([[-0.1221]])\n",
      "Next State : [[0.7103174 ]\n",
      " [0.70388156]\n",
      " [2.6383145 ]]\n",
      "Reward : [-0.90661686]\n",
      "--------------------------------------------------\n",
      "Step : 91\n",
      "Current State : [0.5889332 0.8081817 3.2042148]\n",
      "Currrent Action : tensor([[0.2533]])\n",
      "Next State : [[0.5889332]\n",
      " [0.8081817]\n",
      " [3.2042148]]\n",
      "Reward : [-1.30585707]\n",
      "--------------------------------------------------\n",
      "Step : 92\n",
      "Current State : [0.4295056 0.9030642 3.7158606]\n",
      "Currrent Action : tensor([[-0.6299]])\n",
      "Next State : [[0.4295056]\n",
      " [0.9030642]\n",
      " [3.7158606]]\n",
      "Reward : [-1.91268638]\n",
      "--------------------------------------------------\n",
      "Step : 93\n",
      "Current State : [0.21816652 0.97591156 4.480198  ]\n",
      "Currrent Action : tensor([[0.5803]])\n",
      "Next State : [[0.21816652]\n",
      " [0.97591156]\n",
      " [4.480198  ]]\n",
      "Reward : [-2.65089216]\n",
      "--------------------------------------------------\n",
      "Step : 94\n",
      "Current State : [-0.04246917  0.99909776  5.2483454 ]\n",
      "Currrent Action : tensor([[0.2414]])\n",
      "Next State : [[-0.04246917]\n",
      " [ 0.99909776]\n",
      " [ 5.2483454 ]]\n",
      "Reward : [-3.83210099]\n",
      "--------------------------------------------------\n",
      "Step : 95\n",
      "Current State : [-0.34385487  0.9390228   6.1707416 ]\n",
      "Currrent Action : tensor([[1.1538]])\n",
      "Next State : [[-0.34385487]\n",
      " [ 0.9390228 ]\n",
      " [ 6.1707416 ]]\n",
      "Reward : [-5.35851126]\n",
      "--------------------------------------------------\n",
      "Step : 96\n",
      "Current State : [-0.64961684  0.7602618   7.1212263 ]\n",
      "Currrent Action : tensor([[1.6414]])\n",
      "Next State : [[-0.64961684]\n",
      " [ 0.7602618 ]\n",
      " [ 7.1212263 ]]\n",
      "Reward : [-7.503874]\n",
      "--------------------------------------------------\n",
      "Step : 97\n",
      "Current State : [-0.8942047  0.4476583  7.9914227]\n",
      "Currrent Action : tensor([[2.8051]])\n",
      "Next State : [[-0.8942047]\n",
      " [ 0.4476583]\n",
      " [ 7.9914227]]\n",
      "Reward : [-10.26390828]\n",
      "--------------------------------------------------\n",
      "Step : 98\n",
      "Current State : [-0.9979434   0.06410088  8.        ]\n",
      "Currrent Action : tensor([[1.7006]])\n",
      "Next State : [[-0.9979434 ]\n",
      " [ 0.06410088]\n",
      " [ 8.        ]]\n",
      "Reward : [-13.55790196]\n",
      "--------------------------------------------------\n",
      "Step : 99\n",
      "Current State : [-0.9441288  -0.32957664  8.        ]\n",
      "Currrent Action : tensor([[1.6537]])\n",
      "Next State : [[-0.9441288 ]\n",
      " [-0.32957664]\n",
      " [ 8.        ]]\n",
      "Reward : [-15.87341965]\n",
      "learning iteration : 12\n",
      "--------------------------------------------------\n",
      "Step : 0\n",
      "Current State : [-0.20821123 -0.9780839   0.13255914]\n",
      "Currrent Action : tensor([[0.3265]])\n",
      "Next State : [[-0.20821123]\n",
      " [-0.9780839 ]\n",
      " [ 0.13255914]]\n",
      "Reward : [-3.2606851]\n",
      "--------------------------------------------------\n",
      "Step : 1\n",
      "Current State : [-0.23449033 -0.97211844 -0.53897005]\n",
      "Currrent Action : tensor([[0.4136]])\n",
      "Next State : [[-0.23449033]\n",
      " [-0.97211844]\n",
      " [-0.53897005]]\n",
      "Reward : [-3.17225829]\n",
      "--------------------------------------------------\n",
      "Step : 2\n",
      "Current State : [-0.28125072 -0.9596343  -0.9680589 ]\n",
      "Currrent Action : tensor([[2.2552]])\n",
      "Next State : [[-0.28125072]\n",
      " [-0.9596343 ]\n",
      " [-0.9680589 ]]\n",
      "Reward : [-3.30007105]\n",
      "--------------------------------------------------\n",
      "Step : 3\n",
      "Current State : [-0.36673048 -0.93032724 -1.8079    ]\n",
      "Currrent Action : tensor([[-0.8008]])\n",
      "Next State : [[-0.36673048]\n",
      " [-0.93032724]\n",
      " [-1.8079    ]]\n",
      "Reward : [-3.53869579]\n",
      "--------------------------------------------------\n",
      "Step : 4\n",
      "Current State : [-0.46689337 -0.88431364 -2.2056453 ]\n",
      "Currrent Action : tensor([[3.4326]])\n",
      "Next State : [[-0.46689337]\n",
      " [-0.88431364]\n",
      " [-2.2056453 ]]\n",
      "Reward : [-4.11888923]\n",
      "--------------------------------------------------\n",
      "Step : 5\n",
      "Current State : [-0.58470225 -0.81124794 -2.7747705 ]\n",
      "Currrent Action : tensor([[0.6274]])\n",
      "Next State : [[-0.58470225]\n",
      " [-0.81124794]\n",
      " [-2.7747705 ]]\n",
      "Reward : [-4.7163642]\n",
      "--------------------------------------------------\n",
      "Step : 6\n",
      "Current State : [-0.7209413 -0.6929961 -3.612934 ]\n",
      "Currrent Action : tensor([[-1.5315]])\n",
      "Next State : [[-0.7209413]\n",
      " [-0.6929961]\n",
      " [-3.612934 ]]\n",
      "Reward : [-5.59166372]\n",
      "--------------------------------------------------\n",
      "Step : 7\n",
      "Current State : [-0.84308296 -0.53778356 -3.9566145 ]\n",
      "Currrent Action : tensor([[1.1738]])\n",
      "Next State : [[-0.84308296]\n",
      " [-0.53778356]\n",
      " [-3.9566145 ]]\n",
      "Reward : [-6.95187415]\n",
      "--------------------------------------------------\n",
      "Step : 8\n",
      "Current State : [-0.93665355 -0.3502572  -4.1992044 ]\n",
      "Currrent Action : tensor([[1.0717]])\n",
      "Next State : [[-0.93665355]\n",
      " [-0.3502572 ]\n",
      " [-4.1992044 ]]\n",
      "Reward : [-8.19100656]\n",
      "--------------------------------------------------\n",
      "Step : 9\n",
      "Current State : [-0.9888083  -0.14919175 -4.161897  ]\n",
      "Currrent Action : tensor([[2.6191]])\n",
      "Next State : [[-0.9888083 ]\n",
      " [-0.14919175]\n",
      " [-4.161897  ]]\n",
      "Reward : [-9.51657886]\n",
      "--------------------------------------------------\n",
      "Step : 10\n",
      "Current State : [-0.99796516  0.06376179 -4.271118  ]\n",
      "Currrent Action : tensor([[0.0178]])\n",
      "Next State : [[-0.99796516]\n",
      " [ 0.06376179]\n",
      " [-4.271118  ]]\n",
      "Reward : [-10.68325667]\n",
      "--------------------------------------------------\n",
      "Step : 11\n",
      "Current State : [-0.96325064  0.2686041  -4.1627703 ]\n",
      "Currrent Action : tensor([[0.4035]])\n",
      "Next State : [[-0.96325064]\n",
      " [ 0.2686041 ]\n",
      " [-4.1627703 ]]\n",
      "Reward : [-11.2971841]\n",
      "--------------------------------------------------\n",
      "Step : 12\n",
      "Current State : [-0.8951077   0.44584996 -3.8036003 ]\n",
      "Currrent Action : tensor([[1.0514]])\n",
      "Next State : [[-0.8951077 ]\n",
      " [ 0.44584996]\n",
      " [-3.8036003 ]]\n",
      "Reward : [-9.9688569]\n",
      "--------------------------------------------------\n",
      "Step : 13\n",
      "Current State : [-0.8113878  0.5845082 -3.243004 ]\n",
      "Currrent Action : tensor([[1.5081]])\n",
      "Next State : [[-0.8113878]\n",
      " [ 0.5845082]\n",
      " [-3.243004 ]]\n",
      "Reward : [-8.62856613]\n",
      "--------------------------------------------------\n",
      "Step : 14\n",
      "Current State : [-0.7305292  0.6828815 -2.54852  ]\n",
      "Currrent Action : tensor([[1.7074]])\n",
      "Next State : [[-0.7305292]\n",
      " [ 0.6828815]\n",
      " [-2.54852  ]]\n",
      "Reward : [-7.39151675]\n",
      "--------------------------------------------------\n",
      "Step : 15\n",
      "Current State : [-0.6665005   0.74550456 -1.7918354 ]\n",
      "Currrent Action : tensor([[1.6302]])\n",
      "Next State : [[-0.6665005 ]\n",
      " [ 0.74550456]\n",
      " [-1.7918354 ]]\n",
      "Reward : [-6.36374074]\n",
      "--------------------------------------------------\n",
      "Step : 16\n",
      "Current State : [-0.6212527  0.7836103 -1.1832887]\n",
      "Currrent Action : tensor([[0.3295]])\n",
      "Next State : [[-0.6212527]\n",
      " [ 0.7836103]\n",
      " [-1.1832887]]\n",
      "Reward : [-5.612561]\n",
      "--------------------------------------------------\n",
      "Step : 17\n",
      "Current State : [-0.5946538   0.8039819  -0.67010736]\n",
      "Currrent Action : tensor([[-0.4968]])\n",
      "Next State : [[-0.5946538 ]\n",
      " [ 0.8039819 ]\n",
      " [-0.67010736]]\n",
      "Reward : [-5.16295752]\n",
      "--------------------------------------------------\n",
      "Step : 18\n",
      "Current State : [-0.59072053  0.80687624 -0.09766877]\n",
      "Currrent Action : tensor([[-0.2037]])\n",
      "Next State : [[-0.59072053]\n",
      " [ 0.80687624]\n",
      " [-0.09766877]]\n",
      "Reward : [-4.91858171]\n",
      "--------------------------------------------------\n",
      "Step : 19\n",
      "Current State : [-0.6139507  0.7893444  0.5820897]\n",
      "Currrent Action : tensor([[0.4973]])\n",
      "Next State : [[-0.6139507]\n",
      " [ 0.7893444]\n",
      " [ 0.5820897]]\n",
      "Reward : [-4.8532993]\n",
      "--------------------------------------------------\n",
      "Step : 20\n",
      "Current State : [-0.6649742   0.74686635  1.3280653 ]\n",
      "Currrent Action : tensor([[1.0264]])\n",
      "Next State : [[-0.6649742 ]\n",
      " [ 0.74686635]\n",
      " [ 1.3280653 ]]\n",
      "Reward : [-5.01610124]\n",
      "--------------------------------------------------\n",
      "Step : 21\n",
      "Current State : [-0.7316677  0.6816615  1.8661207]\n",
      "Currrent Action : tensor([[-0.1473]])\n",
      "Next State : [[-0.7316677]\n",
      " [ 0.6816615]\n",
      " [ 1.8661207]]\n",
      "Reward : [-5.4583762]\n",
      "--------------------------------------------------\n",
      "Step : 22\n",
      "Current State : [-0.810994    0.58505446  2.5016763 ]\n",
      "Currrent Action : tensor([[0.8287]])\n",
      "Next State : [[-0.810994  ]\n",
      " [ 0.58505446]\n",
      " [ 2.5016763 ]]\n",
      "Reward : [-6.06849442]\n",
      "--------------------------------------------------\n",
      "Step : 23\n",
      "Current State : [-0.8926193  0.4508112  3.1454659]\n",
      "Currrent Action : tensor([[1.3667]])\n",
      "Next State : [[-0.8926193]\n",
      " [ 0.4508112]\n",
      " [ 3.1454659]]\n",
      "Reward : [-6.96121053]\n",
      "--------------------------------------------------\n",
      "Step : 24\n",
      "Current State : [-0.95760196  0.28809464  3.5087478 ]\n",
      "Currrent Action : tensor([[0.1678]])\n",
      "Next State : [[-0.95760196]\n",
      " [ 0.28809464]\n",
      " [ 3.5087478 ]]\n",
      "Reward : [-8.13926508]\n",
      "--------------------------------------------------\n",
      "Step : 25\n",
      "Current State : [-0.9939472   0.10985892  3.6431084 ]\n",
      "Currrent Action : tensor([[-0.5447]])\n",
      "Next State : [[-0.9939472 ]\n",
      " [ 0.10985892]\n",
      " [ 3.6431084 ]]\n",
      "Reward : [-9.35025817]\n",
      "--------------------------------------------------\n",
      "Step : 26\n",
      "Current State : [-0.9970351  -0.07694814  3.7421079 ]\n",
      "Currrent Action : tensor([[0.1107]])\n",
      "Next State : [[-0.9970351 ]\n",
      " [-0.07694814]\n",
      " [ 3.7421079 ]]\n",
      "Reward : [-10.51729828]\n",
      "--------------------------------------------------\n",
      "Step : 27\n",
      "Current State : [-0.96277654 -0.27029857  3.933577  ]\n",
      "Currrent Action : tensor([[1.6612]])\n",
      "Next State : [[-0.96277654]\n",
      " [-0.27029857]\n",
      " [ 3.933577  ]]\n",
      "Reward : [-10.79467609]\n",
      "--------------------------------------------------\n",
      "Step : 28\n",
      "Current State : [-0.894583   -0.44690183  3.7919188 ]\n",
      "Currrent Action : tensor([[0.4071]])\n",
      "Next State : [[-0.894583  ]\n",
      " [-0.44690183]\n",
      " [ 3.7919188 ]]\n",
      "Reward : [-9.7722588]\n",
      "--------------------------------------------------\n",
      "Step : 29\n",
      "Current State : [-0.8043249 -0.5941897  3.459175 ]\n",
      "Currrent Action : tensor([[0.0162]])\n",
      "Next State : [[-0.8043249]\n",
      " [-0.5941897]\n",
      " [ 3.459175 ]]\n",
      "Reward : [-8.6111216]\n",
      "--------------------------------------------------\n",
      "Step : 30\n",
      "Current State : [-0.71027267 -0.7039266   2.8930597 ]\n",
      "Currrent Action : tensor([[-0.8032]])\n",
      "Next State : [[-0.71027267]\n",
      " [-0.7039266 ]\n",
      " [ 2.8930597 ]]\n",
      "Reward : [-7.47393691]\n",
      "--------------------------------------------------\n",
      "Step : 31\n",
      "Current State : [-0.6186893  -0.78563577  2.456244  ]\n",
      "Currrent Action : tensor([[0.6075]])\n",
      "Next State : [[-0.6186893 ]\n",
      " [-0.78563577]\n",
      " [ 2.456244  ]]\n",
      "Reward : [-6.4101673]\n",
      "--------------------------------------------------\n",
      "Step : 32\n",
      "Current State : [-0.54383814 -0.8391901   1.8413856 ]\n",
      "Currrent Action : tensor([[-0.1709]])\n",
      "Next State : [[-0.54383814]\n",
      " [-0.8391901 ]\n",
      " [ 1.8413856 ]]\n",
      "Reward : [-5.61140314]\n",
      "--------------------------------------------------\n",
      "Step : 33\n",
      "Current State : [-0.49616358 -0.8682291   1.1165907 ]\n",
      "Currrent Action : tensor([[-0.6360]])\n",
      "Next State : [[-0.49616358]\n",
      " [-0.8682291 ]\n",
      " [ 1.1165907 ]]\n",
      "Reward : [-4.94393374]\n",
      "--------------------------------------------------\n",
      "Step : 34\n",
      "Current State : [-0.4777998  -0.87846875  0.42052156]\n",
      "Currrent Action : tensor([[-0.2993]])\n",
      "Next State : [[-0.4777998 ]\n",
      " [-0.87846875]\n",
      " [ 0.42052156]]\n",
      "Reward : [-4.49274509]\n",
      "--------------------------------------------------\n",
      "Step : 35\n",
      "Current State : [-0.4823262 -0.8759917 -0.1031978]\n",
      "Currrent Action : tensor([[0.9009]])\n",
      "Next State : [[-0.4823262]\n",
      " [-0.8759917]\n",
      " [-0.1031978]]\n",
      "Reward : [-4.29902775]\n",
      "--------------------------------------------------\n",
      "Step : 36\n",
      "Current State : [-0.5122709 -0.8588239 -0.690373 ]\n",
      "Currrent Action : tensor([[0.4655]])\n",
      "Next State : [[-0.5122709]\n",
      " [-0.8588239]\n",
      " [-0.690373 ]]\n",
      "Reward : [-4.30319163]\n",
      "--------------------------------------------------\n",
      "Step : 37\n",
      "Current State : [-0.55686504 -0.830603   -1.0555953 ]\n",
      "Currrent Action : tensor([[1.8593]])\n",
      "Next State : [[-0.55686504]\n",
      " [-0.830603  ]\n",
      " [-1.0555953 ]]\n",
      "Reward : [-4.49741063]\n",
      "--------------------------------------------------\n",
      "Step : 38\n",
      "Current State : [-0.6216698  -0.78327936 -1.6053214 ]\n",
      "Currrent Action : tensor([[0.4882]])\n",
      "Next State : [[-0.6216698 ]\n",
      " [-0.78327936]\n",
      " [-1.6053214 ]]\n",
      "Reward : [-4.78332956]\n",
      "--------------------------------------------------\n",
      "Step : 39\n",
      "Current State : [-0.6940121 -0.7199633 -1.9234818]\n",
      "Currrent Action : tensor([[1.7953]])\n",
      "Next State : [[-0.6940121]\n",
      " [-0.7199633]\n",
      " [-1.9234818]]\n",
      "Reward : [-5.28600929]\n",
      "--------------------------------------------------\n",
      "Step : 40\n",
      "Current State : [-0.77595377 -0.6307898  -2.423576  ]\n",
      "Currrent Action : tensor([[0.2659]])\n",
      "Next State : [[-0.77595377]\n",
      " [-0.6307898 ]\n",
      " [-2.423576  ]]\n",
      "Reward : [-5.83555978]\n",
      "--------------------------------------------------\n",
      "Step : 41\n",
      "Current State : [-0.8536385 -0.5208659 -2.6941125]\n",
      "Currrent Action : tensor([[1.3504]])\n",
      "Next State : [[-0.8536385]\n",
      " [-0.5208659]\n",
      " [-2.6941125]]\n",
      "Reward : [-6.63598493]\n",
      "--------------------------------------------------\n",
      "Step : 42\n",
      "Current State : [-0.9247669 -0.3805341 -3.1498241]\n",
      "Currrent Action : tensor([[-0.4337]])\n",
      "Next State : [[-0.9247669]\n",
      " [-0.3805341]\n",
      " [-3.1498241]]\n",
      "Reward : [-7.45343548]\n",
      "--------------------------------------------------\n",
      "Step : 43\n",
      "Current State : [-0.9742204  -0.22559835 -3.2563326 ]\n",
      "Currrent Action : tensor([[1.1926]])\n",
      "Next State : [[-0.9742204 ]\n",
      " [-0.22559835]\n",
      " [-3.2563326 ]]\n",
      "Reward : [-8.56276661]\n",
      "--------------------------------------------------\n",
      "Step : 44\n",
      "Current State : [-0.9985556  -0.05372836 -3.4760585 ]\n",
      "Currrent Action : tensor([[-0.3368]])\n",
      "Next State : [[-0.9985556 ]\n",
      " [-0.05372836]\n",
      " [-3.4760585 ]]\n",
      "Reward : [-9.55208655]\n",
      "--------------------------------------------------\n",
      "Step : 45\n",
      "Current State : [-0.9924583   0.12258275 -3.5329218 ]\n",
      "Currrent Action : tensor([[-0.1104]])\n",
      "Next State : [[-0.9924583 ]\n",
      " [ 0.12258275]\n",
      " [-3.5329218 ]]\n",
      "Reward : [-10.74305643]\n",
      "--------------------------------------------------\n",
      "Step : 46\n",
      "Current State : [-0.9576156  0.2880492 -3.3859446]\n",
      "Currrent Action : tensor([[0.3669]])\n",
      "Next State : [[-0.9576156]\n",
      " [ 0.2880492]\n",
      " [-3.3859446]]\n",
      "Reward : [-10.36084291]\n",
      "--------------------------------------------------\n",
      "Step : 47\n",
      "Current State : [-0.8985422  0.4388871 -3.243413 ]\n",
      "Currrent Action : tensor([[-0.4900]])\n",
      "Next State : [[-0.8985422]\n",
      " [ 0.4388871]\n",
      " [-3.243413 ]]\n",
      "Reward : [-9.26580296]\n",
      "--------------------------------------------------\n",
      "Step : 48\n",
      "Current State : [-0.8304761  0.5570542 -2.7294953]\n",
      "Currrent Action : tensor([[1.2317]])\n",
      "Next State : [[-0.8304761]\n",
      " [ 0.5570542]\n",
      " [-2.7294953]]\n",
      "Reward : [-8.27471066]\n",
      "--------------------------------------------------\n",
      "Step : 49\n",
      "Current State : [-0.7539254  0.65696   -2.5188997]\n",
      "Currrent Action : tensor([[-1.3813]])\n",
      "Next State : [[-0.7539254]\n",
      " [ 0.65696  ]\n",
      " [-2.5188997]]\n",
      "Reward : [-7.25328967]\n",
      "--------------------------------------------------\n",
      "Step : 50\n",
      "Current State : [-0.6852684  0.7282906 -1.9808943]\n",
      "Currrent Action : tensor([[0.3019]])\n",
      "Next State : [[-0.6852684]\n",
      " [ 0.7282906]\n",
      " [-1.9808943]]\n",
      "Reward : [-6.5142957]\n",
      "--------------------------------------------------\n",
      "Step : 51\n",
      "Current State : [-0.63291633  0.77422017 -1.3931576 ]\n",
      "Currrent Action : tensor([[0.2768]])\n",
      "Next State : [[-0.63291633]\n",
      " [ 0.77422017]\n",
      " [-1.3931576 ]]\n",
      "Reward : [-5.80166983]\n",
      "--------------------------------------------------\n",
      "Step : 52\n",
      "Current State : [-0.6001115   0.7999164  -0.83347553]\n",
      "Currrent Action : tensor([[-0.1399]])\n",
      "Next State : [[-0.6001115 ]\n",
      " [ 0.7999164 ]\n",
      " [-0.83347553]]\n",
      "Reward : [-5.2841434]\n",
      "--------------------------------------------------\n",
      "Step : 53\n",
      "Current State : [-0.59129536  0.8064551  -0.21952727]\n",
      "Currrent Action : tensor([[0.0934]])\n",
      "Next State : [[-0.59129536]\n",
      " [ 0.8064551 ]\n",
      " [-0.21952727]]\n",
      "Reward : [-4.97320729]\n",
      "--------------------------------------------------\n",
      "Step : 54\n",
      "Current State : [-0.60864806  0.79344034  0.43382892]\n",
      "Currrent Action : tensor([[0.3234]])\n",
      "Next State : [[-0.60864806]\n",
      " [ 0.79344034]\n",
      " [ 0.43382892]]\n",
      "Reward : [-4.8601618]\n",
      "--------------------------------------------------\n",
      "Step : 55\n",
      "Current State : [-0.64595765  0.76337326  0.9584302 ]\n",
      "Currrent Action : tensor([[-0.4699]])\n",
      "Next State : [[-0.64595765]\n",
      " [ 0.76337326]\n",
      " [ 0.9584302 ]]\n",
      "Reward : [-4.9703425]\n",
      "--------------------------------------------------\n",
      "Step : 56\n",
      "Current State : [-0.70325935  0.7109334   1.5538936 ]\n",
      "Currrent Action : tensor([[0.1529]])\n",
      "Next State : [[-0.70325935]\n",
      " [ 0.7109334 ]\n",
      " [ 1.5538936 ]]\n",
      "Reward : [-5.25874493]\n",
      "--------------------------------------------------\n",
      "Step : 57\n",
      "Current State : [-0.77303994  0.63435733  2.072953  ]\n",
      "Currrent Action : tensor([[-0.0943]])\n",
      "Next State : [[-0.77303994]\n",
      " [ 0.63435733]\n",
      " [ 2.072953  ]]\n",
      "Reward : [-5.76757798]\n",
      "--------------------------------------------------\n",
      "Step : 58\n",
      "Current State : [-0.85117966  0.5248745   2.6921854 ]\n",
      "Currrent Action : tensor([[0.9564]])\n",
      "Next State : [[-0.85117966]\n",
      " [ 0.5248745 ]\n",
      " [ 2.6921854 ]]\n",
      "Reward : [-6.45478479]\n",
      "--------------------------------------------------\n",
      "Step : 59\n",
      "Current State : [-0.922076    0.38700888  3.1036434 ]\n",
      "Currrent Action : tensor([[0.1187]])\n",
      "Next State : [[-0.922076  ]\n",
      " [ 0.38700888]\n",
      " [ 3.1036434 ]]\n",
      "Reward : [-7.42785073]\n",
      "--------------------------------------------------\n",
      "Step : 60\n",
      "Current State : [-0.9730976   0.23039329  3.2980733 ]\n",
      "Currrent Action : tensor([[-0.6388]])\n",
      "Next State : [[-0.9730976 ]\n",
      " [ 0.23039329]\n",
      " [ 3.2980733 ]]\n",
      "Reward : [-8.49434128]\n",
      "--------------------------------------------------\n",
      "Step : 61\n",
      "Current State : [-0.9985128   0.05451728  3.5587502 ]\n",
      "Currrent Action : tensor([[0.5859]])\n",
      "Next State : [[-0.9985128 ]\n",
      " [ 0.05451728]\n",
      " [ 3.5587502 ]]\n",
      "Reward : [-9.55099772]\n",
      "--------------------------------------------------\n",
      "Step : 62\n",
      "Current State : [-0.99165326 -0.12893312  3.676747  ]\n",
      "Currrent Action : tensor([[0.5141]])\n",
      "Next State : [[-0.99165326]\n",
      " [-0.12893312]\n",
      " [ 3.676747  ]]\n",
      "Reward : [-10.79660201]\n",
      "--------------------------------------------------\n",
      "Step : 63\n",
      "Current State : [-0.9528271  -0.30351362  3.5817006 ]\n",
      "Currrent Action : tensor([[0.0110]])\n",
      "Next State : [[-0.9528271 ]\n",
      " [-0.30351362]\n",
      " [ 3.5817006 ]]\n",
      "Reward : [-10.425796]\n",
      "--------------------------------------------------\n",
      "Step : 64\n",
      "Current State : [-0.8908927 -0.4542139  3.262231 ]\n",
      "Currrent Action : tensor([[-0.6122]])\n",
      "Next State : [[-0.8908927]\n",
      " [-0.4542139]\n",
      " [ 3.262231 ]]\n",
      "Reward : [-9.31033766]\n",
      "--------------------------------------------------\n",
      "Step : 65\n",
      "Current State : [-0.8116527 -0.5841403  3.046617 ]\n",
      "Currrent Action : tensor([[0.8336]])\n",
      "Next State : [[-0.8116527]\n",
      " [-0.5841403]\n",
      " [ 3.046617 ]]\n",
      "Reward : [-8.19436032]\n",
      "--------------------------------------------------\n",
      "Step : 66\n",
      "Current State : [-0.7258686 -0.6878334  2.693591 ]\n",
      "Currrent Action : tensor([[0.5672]])\n",
      "Next State : [[-0.7258686]\n",
      " [-0.6878334]\n",
      " [ 2.693591 ]]\n",
      "Reward : [-7.267686]\n",
      "--------------------------------------------------\n",
      "Step : 67\n",
      "Current State : [-0.64934987 -0.7604898   2.111341  ]\n",
      "Currrent Action : tensor([[-0.4425]])\n",
      "Next State : [[-0.64934987]\n",
      " [-0.7604898 ]\n",
      " [ 2.111341  ]]\n",
      "Reward : [-6.40486953]\n",
      "--------------------------------------------------\n",
      "Step : 68\n",
      "Current State : [-0.58328366 -0.81226856  1.6792771 ]\n",
      "Currrent Action : tensor([[0.9220]])\n",
      "Next State : [[-0.58328366]\n",
      " [-0.81226856]\n",
      " [ 1.6792771 ]]\n",
      "Reward : [-5.63374894]\n",
      "--------------------------------------------------\n",
      "Step : 69\n",
      "Current State : [-0.5414622 -0.8407251  1.011802 ]\n",
      "Currrent Action : tensor([[-0.3885]])\n",
      "Next State : [[-0.5414622]\n",
      " [-0.8407251]\n",
      " [ 1.011802 ]]\n",
      "Reward : [-5.09386112]\n",
      "--------------------------------------------------\n",
      "Step : 70\n",
      "Current State : [-0.52215856 -0.8528484   0.4559058 ]\n",
      "Currrent Action : tensor([[0.4977]])\n",
      "Next State : [[-0.52215856]\n",
      " [-0.8528484 ]\n",
      " [ 0.4559058 ]]\n",
      "Reward : [-4.69494937]\n",
      "--------------------------------------------------\n",
      "Step : 71\n",
      "Current State : [-0.52537584 -0.85087025 -0.0755356 ]\n",
      "Currrent Action : tensor([[0.7213]])\n",
      "Next State : [[-0.52537584]\n",
      " [-0.85087025]\n",
      " [-0.0755356 ]]\n",
      "Reward : [-4.51645298]\n",
      "--------------------------------------------------\n",
      "Step : 72\n",
      "Current State : [-0.5587847  -0.82931274 -0.79525834]\n",
      "Currrent Action : tensor([[-0.5438]])\n",
      "Next State : [[-0.5587847 ]\n",
      " [-0.82931274]\n",
      " [-0.79525834]]\n",
      "Reward : [-4.51204312]\n",
      "--------------------------------------------------\n",
      "Step : 73\n",
      "Current State : [-0.618024  -0.7861593 -1.4661396]\n",
      "Currrent Action : tensor([[-0.3260]])\n",
      "Next State : [[-0.618024 ]\n",
      " [-0.7861593]\n",
      " [-1.4661396]]\n",
      "Reward : [-4.74501692]\n",
      "--------------------------------------------------\n",
      "Step : 74\n",
      "Current State : [-0.70195866 -0.7122177  -2.2383432 ]\n",
      "Currrent Action : tensor([[-1.2172]])\n",
      "Next State : [[-0.70195866]\n",
      " [-0.7122177 ]\n",
      " [-2.2383432 ]]\n",
      "Reward : [-5.2207101]\n",
      "--------------------------------------------------\n",
      "Step : 75\n",
      "Current State : [-0.7875766 -0.6162168 -2.5744498]\n",
      "Currrent Action : tensor([[1.3204]])\n",
      "Next State : [[-0.7875766]\n",
      " [-0.6162168]\n",
      " [-2.5744498]]\n",
      "Reward : [-6.0202813]\n",
      "--------------------------------------------------\n",
      "Step : 76\n",
      "Current State : [-0.87029386 -0.49253285 -2.9786456 ]\n",
      "Currrent Action : tensor([[0.3864]])\n",
      "Next State : [[-0.87029386]\n",
      " [-0.49253285]\n",
      " [-2.9786456 ]]\n",
      "Reward : [-6.80174069]\n",
      "--------------------------------------------------\n",
      "Step : 77\n",
      "Current State : [-0.94097656 -0.3384717  -3.394109  ]\n",
      "Currrent Action : tensor([[-0.3071]])\n",
      "Next State : [[-0.94097656]\n",
      " [-0.3384717 ]\n",
      " [-3.394109  ]]\n",
      "Reward : [-7.78632824]\n",
      "--------------------------------------------------\n",
      "Step : 78\n",
      "Current State : [-0.9849278 -0.172966  -3.42904  ]\n",
      "Currrent Action : tensor([[1.4595]])\n",
      "Next State : [[-0.9849278]\n",
      " [-0.172966 ]\n",
      " [-3.42904  ]]\n",
      "Reward : [-8.97342359]\n",
      "--------------------------------------------------\n",
      "Step : 79\n",
      "Current State : [-0.9999406  -0.01090182 -3.2587645 ]\n",
      "Currrent Action : tensor([[2.5156]])\n",
      "Next State : [[-0.9999406 ]\n",
      " [-0.01090182]\n",
      " [-3.2587645 ]]\n",
      "Reward : [-9.98738579]\n",
      "--------------------------------------------------\n",
      "Step : 80\n",
      "Current State : [-0.98844653  0.15156987 -3.2611668 ]\n",
      "Currrent Action : tensor([[0.0385]])\n",
      "Next State : [[-0.98844653]\n",
      " [ 0.15156987]\n",
      " [-3.2611668 ]]\n",
      "Reward : [-10.86317983]\n",
      "--------------------------------------------------\n",
      "Step : 81\n",
      "Current State : [-0.95633155  0.29228398 -2.889159  ]\n",
      "Currrent Action : tensor([[1.7222]])\n",
      "Next State : [[-0.95633155]\n",
      " [ 0.29228398]\n",
      " [-2.889159  ]]\n",
      "Reward : [-10.00321659]\n",
      "--------------------------------------------------\n",
      "Step : 82\n",
      "Current State : [-0.90996456  0.41468605 -2.6196704 ]\n",
      "Currrent Action : tensor([[0.3352]])\n",
      "Next State : [[-0.90996456]\n",
      " [ 0.41468605]\n",
      " [-2.6196704 ]]\n",
      "Reward : [-8.92873843]\n",
      "--------------------------------------------------\n",
      "Step : 83\n",
      "Current State : [-0.85281503  0.5222131  -2.4369237 ]\n",
      "Currrent Action : tensor([[-0.8551]])\n",
      "Next State : [[-0.85281503]\n",
      " [ 0.5222131 ]\n",
      " [-2.4369237 ]]\n",
      "Reward : [-8.05276671]\n",
      "--------------------------------------------------\n",
      "Step : 84\n",
      "Current State : [-0.79273736  0.6095633  -2.1213129 ]\n",
      "Currrent Action : tensor([[-0.5070]])\n",
      "Next State : [[-0.79273736]\n",
      " [ 0.6095633 ]\n",
      " [-2.1213129 ]]\n",
      "Reward : [-7.31335165]\n",
      "--------------------------------------------------\n",
      "Step : 85\n",
      "Current State : [-0.7385723  0.6741743 -1.6867307]\n",
      "Currrent Action : tensor([[-0.1506]])\n",
      "Next State : [[-0.7385723]\n",
      " [ 0.6741743]\n",
      " [-1.6867307]]\n",
      "Reward : [-6.63062843]\n",
      "--------------------------------------------------\n",
      "Step : 86\n",
      "Current State : [-0.6990693  0.7150539 -1.137102 ]\n",
      "Currrent Action : tensor([[0.2933]])\n",
      "Next State : [[-0.6990693]\n",
      " [ 0.7150539]\n",
      " [-1.137102 ]]\n",
      "Reward : [-6.05297837]\n",
      "--------------------------------------------------\n",
      "Step : 87\n",
      "Current State : [-0.67783755  0.73521173 -0.58555436]\n",
      "Currrent Action : tensor([[0.1017]])\n",
      "Next State : [[-0.67783755]\n",
      " [ 0.73521173]\n",
      " [-0.58555436]]\n",
      "Reward : [-5.62782615]\n",
      "--------------------------------------------------\n",
      "Step : 88\n",
      "Current State : [-0.6767413   0.7362209  -0.02980021]\n",
      "Currrent Action : tensor([[0.0290]])\n",
      "Next State : [[-0.6767413 ]\n",
      " [ 0.7362209 ]\n",
      " [-0.02980021]]\n",
      "Reward : [-5.39635499]\n",
      "--------------------------------------------------\n",
      "Step : 89\n",
      "Current State : [-0.69677943  0.71728545  0.55140734]\n",
      "Currrent Action : tensor([[0.1936]])\n",
      "Next State : [[-0.69677943]\n",
      " [ 0.71728545]\n",
      " [ 0.55140734]]\n",
      "Reward : [-5.35529469]\n",
      "--------------------------------------------------\n",
      "Step : 90\n",
      "Current State : [-0.7247062  0.689058   0.7942036]\n",
      "Currrent Action : tensor([[-1.9678]])\n",
      "Next State : [[-0.7247062]\n",
      " [ 0.689058 ]\n",
      " [ 0.7942036]]\n",
      "Reward : [-5.51780818]\n",
      "--------------------------------------------------\n",
      "Step : 91\n",
      "Current State : [-0.76584494  0.6430253   1.234929  ]\n",
      "Currrent Action : tensor([[-0.5071]])\n",
      "Next State : [[-0.76584494]\n",
      " [ 0.6430253 ]\n",
      " [ 1.234929  ]]\n",
      "Reward : [-5.73441918]\n",
      "--------------------------------------------------\n",
      "Step : 92\n",
      "Current State : [-0.8192339  0.5734595  1.7543894]\n",
      "Currrent Action : tensor([[0.2479]])\n",
      "Next State : [[-0.8192339]\n",
      " [ 0.5734595]\n",
      " [ 1.7543894]]\n",
      "Reward : [-6.12155167]\n",
      "--------------------------------------------------\n",
      "Step : 93\n",
      "Current State : [-0.8826362  0.4700568  2.4273474]\n",
      "Currrent Action : tensor([[1.6191]])\n",
      "Next State : [[-0.8826362]\n",
      " [ 0.4700568]\n",
      " [ 2.4273474]]\n",
      "Reward : [-6.71571336]\n",
      "--------------------------------------------------\n",
      "Step : 94\n",
      "Current State : [-0.94119364  0.33786756  2.894097  ]\n",
      "Currrent Action : tensor([[0.7614]])\n",
      "Next State : [[-0.94119364]\n",
      " [ 0.33786756]\n",
      " [ 2.894097  ]]\n",
      "Reward : [-7.62414501]\n",
      "--------------------------------------------------\n",
      "Step : 95\n",
      "Current State : [-0.9832038   0.18251099  3.2222111 ]\n",
      "Currrent Action : tensor([[0.4981]])\n",
      "Next State : [[-0.9832038 ]\n",
      " [ 0.18251099]\n",
      " [ 3.2222111 ]]\n",
      "Reward : [-8.66071443]\n",
      "--------------------------------------------------\n",
      "Step : 96\n",
      "Current State : [-0.9999489   0.01010713  3.4686487 ]\n",
      "Currrent Action : tensor([[0.7304]])\n",
      "Next State : [[-0.9999489 ]\n",
      " [ 0.01010713]\n",
      " [ 3.4686487 ]]\n",
      "Reward : [-9.788875]\n",
      "--------------------------------------------------\n",
      "Step : 97\n",
      "Current State : [-0.9850725  -0.17214017  3.6621833 ]\n",
      "Currrent Action : tensor([[1.2397]])\n",
      "Next State : [[-0.9850725 ]\n",
      " [-0.17214017]\n",
      " [ 3.6621833 ]]\n",
      "Reward : [-11.01088972]\n",
      "--------------------------------------------------\n",
      "Step : 98\n",
      "Current State : [-0.93594205 -0.35215402  3.7373936 ]\n",
      "Currrent Action : tensor([[1.3621]])\n",
      "Next State : [[-0.93594205]\n",
      " [-0.35215402]\n",
      " [ 3.7373936 ]]\n",
      "Reward : [-10.15554521]\n",
      "--------------------------------------------------\n",
      "Step : 99\n",
      "Current State : [-0.8632426 -0.5047892  3.385324 ]\n",
      "Currrent Action : tensor([[-0.5864]])\n",
      "Next State : [[-0.8632426]\n",
      " [-0.5047892]\n",
      " [ 3.385324 ]]\n",
      "Reward : [-9.13512724]\n",
      "learning iteration : 13\n",
      "--------------------------------------------------\n",
      "Step : 0\n",
      "Current State : [-0.68867165 -0.72507334 -0.31935555]\n",
      "Currrent Action : tensor([[-0.6031]])\n",
      "Next State : [[-0.68867165]\n",
      " [-0.72507334]\n",
      " [-0.31935555]]\n",
      "Reward : [-5.36763825]\n",
      "--------------------------------------------------\n",
      "Step : 1\n",
      "Current State : [-0.7243095  -0.68947494 -1.00754   ]\n",
      "Currrent Action : tensor([[-0.9625]])\n",
      "Next State : [[-0.7243095 ]\n",
      " [-0.68947494]\n",
      " [-1.00754   ]]\n",
      "Reward : [-5.44213061]\n",
      "--------------------------------------------------\n",
      "Step : 2\n",
      "Current State : [-0.7664533 -0.6423    -1.2653714]\n",
      "Currrent Action : tensor([[1.7285]])\n",
      "Next State : [[-0.7664533]\n",
      " [-0.6423   ]\n",
      " [-1.2653714]]\n",
      "Reward : [-5.77284694]\n",
      "--------------------------------------------------\n",
      "Step : 3\n",
      "Current State : [-0.8189631  -0.57384616 -1.7260151 ]\n",
      "Currrent Action : tensor([[0.1405]])\n",
      "Next State : [[-0.8189631 ]\n",
      " [-0.57384616]\n",
      " [-1.7260151 ]]\n",
      "Reward : [-6.13374793]\n",
      "--------------------------------------------------\n",
      "Step : 4\n",
      "Current State : [-0.875144  -0.4838626 -2.122631 ]\n",
      "Currrent Action : tensor([[0.2251]])\n",
      "Next State : [[-0.875144 ]\n",
      " [-0.4838626]\n",
      " [-2.122631 ]]\n",
      "Reward : [-6.70087789]\n",
      "--------------------------------------------------\n",
      "Step : 5\n",
      "Current State : [-0.92558867 -0.37853092 -2.3370886 ]\n",
      "Currrent Action : tensor([[0.9896]])\n",
      "Next State : [[-0.92558867]\n",
      " [-0.37853092]\n",
      " [-2.3370886 ]]\n",
      "Reward : [-7.402824]\n",
      "--------------------------------------------------\n",
      "Step : 6\n",
      "Current State : [-0.96525836 -0.26129737 -2.4768512 ]\n",
      "Currrent Action : tensor([[0.9609]])\n",
      "Next State : [[-0.96525836]\n",
      " [-0.26129737]\n",
      " [-2.4768512 ]]\n",
      "Reward : [-8.12824531]\n",
      "--------------------------------------------------\n",
      "Step : 7\n",
      "Current State : [-0.9894009  -0.14520961 -2.3728242 ]\n",
      "Currrent Action : tensor([[2.2010]])\n",
      "Next State : [[-0.9894009 ]\n",
      " [-0.14520961]\n",
      " [-2.3728242 ]]\n",
      "Reward : [-8.89591217]\n",
      "--------------------------------------------------\n",
      "Step : 8\n",
      "Current State : [-0.99976546 -0.0216571  -2.4813209 ]\n",
      "Currrent Action : tensor([[0.0027]])\n",
      "Next State : [[-0.99976546]\n",
      " [-0.0216571 ]\n",
      " [-2.4813209 ]]\n",
      "Reward : [-9.53825353]\n",
      "--------------------------------------------------\n",
      "Step : 9\n",
      "Current State : [-0.9961112  0.088105  -2.1975636]\n",
      "Currrent Action : tensor([[2.6417]])\n",
      "Next State : [[-0.9961112]\n",
      " [ 0.088105 ]\n",
      " [-2.1975636]]\n",
      "Reward : [-10.35368259]\n",
      "--------------------------------------------------\n",
      "Step : 10\n",
      "Current State : [-0.98372793  0.17966455 -1.8485212 ]\n",
      "Currrent Action : tensor([[1.8864]])\n",
      "Next State : [[-0.98372793]\n",
      " [ 0.17966455]\n",
      " [-1.8485212 ]]\n",
      "Reward : [-9.80957547]\n",
      "--------------------------------------------------\n",
      "Step : 11\n",
      "Current State : [-0.9654809   0.26047373 -1.6573474 ]\n",
      "Currrent Action : tensor([[0.3762]])\n",
      "Next State : [[-0.9654809 ]\n",
      " [ 0.26047373]\n",
      " [-1.6573474 ]]\n",
      "Reward : [-9.10905291]\n",
      "--------------------------------------------------\n",
      "Step : 12\n",
      "Current State : [-0.942051   0.3354697 -1.5718191]\n",
      "Currrent Action : tensor([[-0.7322]])\n",
      "Next State : [[-0.942051 ]\n",
      " [ 0.3354697]\n",
      " [-1.5718191]]\n",
      "Reward : [-8.55855965]\n",
      "--------------------------------------------------\n",
      "Step : 13\n",
      "Current State : [-0.9173433   0.39809704 -1.3467541 ]\n",
      "Currrent Action : tensor([[-0.1769]])\n",
      "Next State : [[-0.9173433 ]\n",
      " [ 0.39809704]\n",
      " [-1.3467541 ]]\n",
      "Reward : [-8.0842308]\n",
      "--------------------------------------------------\n",
      "Step : 14\n",
      "Current State : [-0.8890501  0.4578099 -1.3217744]\n",
      "Currrent Action : tensor([[-1.8240]])\n",
      "Next State : [[-0.8890501]\n",
      " [ 0.4578099]\n",
      " [-1.3217744]]\n",
      "Reward : [-7.64935149]\n",
      "--------------------------------------------------\n",
      "Step : 15\n",
      "Current State : [-0.87301236  0.48769808 -0.6784169 ]\n",
      "Currrent Action : tensor([[2.2072]])\n",
      "Next State : [[-0.87301236]\n",
      " [ 0.48769808]\n",
      " [-0.6784169 ]]\n",
      "Reward : [-7.28659774]\n",
      "--------------------------------------------------\n",
      "Step : 16\n",
      "Current State : [-0.8623818   0.5062584  -0.42779154]\n",
      "Currrent Action : tensor([[-0.7677]])\n",
      "Next State : [[-0.8623818 ]\n",
      " [ 0.5062584 ]\n",
      " [-0.42779154]]\n",
      "Reward : [-6.97478369]\n",
      "--------------------------------------------------\n",
      "Step : 17\n",
      "Current State : [-0.865597   0.5007413  0.1277136]\n",
      "Currrent Action : tensor([[1.1721]])\n",
      "Next State : [[-0.865597 ]\n",
      " [ 0.5007413]\n",
      " [ 0.1277136]]\n",
      "Reward : [-6.8357005]\n",
      "--------------------------------------------------\n",
      "Step : 18\n",
      "Current State : [-0.87956035  0.47578737  0.57191855]\n",
      "Currrent Action : tensor([[0.4577]])\n",
      "Next State : [[-0.87956035]\n",
      " [ 0.47578737]\n",
      " [ 0.57191855]]\n",
      "Reward : [-6.85125034]\n",
      "--------------------------------------------------\n",
      "Step : 19\n",
      "Current State : [-0.8997534   0.43639868  0.88533604]\n",
      "Currrent Action : tensor([[-0.2895]])\n",
      "Next State : [[-0.8997534 ]\n",
      " [ 0.43639868]\n",
      " [ 0.88533604]]\n",
      "Reward : [-7.03269938]\n",
      "--------------------------------------------------\n",
      "Step : 20\n",
      "Current State : [-0.9283186   0.37178555  1.41321   ]\n",
      "Currrent Action : tensor([[1.3372]])\n",
      "Next State : [[-0.9283186 ]\n",
      " [ 0.37178555]\n",
      " [ 1.41321   ]]\n",
      "Reward : [-7.31627239]\n",
      "--------------------------------------------------\n",
      "Step : 21\n",
      "Current State : [-0.95817786  0.28617328  1.8140199 ]\n",
      "Currrent Action : tensor([[0.8131]])\n",
      "Next State : [[-0.95817786]\n",
      " [ 0.28617328]\n",
      " [ 1.8140199 ]]\n",
      "Reward : [-7.82162633]\n",
      "--------------------------------------------------\n",
      "Step : 22\n",
      "Current State : [-0.9831483   0.18281002  2.1277366 ]\n",
      "Currrent Action : tensor([[0.6606]])\n",
      "Next State : [[-0.9831483 ]\n",
      " [ 0.18281002]\n",
      " [ 2.1277366 ]]\n",
      "Reward : [-8.45976812]\n",
      "--------------------------------------------------\n",
      "Step : 23\n",
      "Current State : [-0.99810195  0.06158372  2.4444232 ]\n",
      "Currrent Action : tensor([[1.1972]])\n",
      "Next State : [[-0.99810195]\n",
      " [ 0.06158372]\n",
      " [ 2.4444232 ]]\n",
      "Reward : [-9.20243741]\n",
      "--------------------------------------------------\n",
      "Step : 24\n",
      "Current State : [-0.99792206 -0.06443287  2.5220048 ]\n",
      "Currrent Action : tensor([[0.2093]])\n",
      "Next State : [[-0.99792206]\n",
      " [-0.06443287]\n",
      " [ 2.5220048 ]]\n",
      "Reward : [-10.08377916]\n",
      "--------------------------------------------------\n",
      "Step : 25\n",
      "Current State : [-0.98163456 -0.19077103  2.5494003 ]\n",
      "Currrent Action : tensor([[0.5048]])\n",
      "Next State : [[-0.98163456]\n",
      " [-0.19077103]\n",
      " [ 2.5494003 ]]\n",
      "Reward : [-10.10494317]\n",
      "--------------------------------------------------\n",
      "Step : 26\n",
      "Current State : [-0.95373285 -0.30065534  2.268643  ]\n",
      "Currrent Action : tensor([[-0.9179]])\n",
      "Next State : [[-0.95373285]\n",
      " [-0.30065534]\n",
      " [ 2.268643  ]]\n",
      "Reward : [-9.35119291]\n",
      "--------------------------------------------------\n",
      "Step : 27\n",
      "Current State : [-0.9174923  -0.39775366  2.0737498 ]\n",
      "Currrent Action : tensor([[0.2040]])\n",
      "Next State : [[-0.9174923 ]\n",
      " [-0.39775366]\n",
      " [ 2.0737498 ]]\n",
      "Reward : [-8.55881963]\n",
      "--------------------------------------------------\n",
      "Step : 28\n",
      "Current State : [-0.8805693  -0.47391748  1.693342  ]\n",
      "Currrent Action : tensor([[-0.5473]])\n",
      "Next State : [[-0.8805693 ]\n",
      " [-0.47391748]\n",
      " [ 1.693342  ]]\n",
      "Reward : [-7.89703879]\n",
      "--------------------------------------------------\n",
      "Step : 29\n",
      "Current State : [-0.8464141 -0.5325253  1.3569404]\n",
      "Currrent Action : tensor([[0.1269]])\n",
      "Next State : [[-0.8464141]\n",
      " [-0.5325253]\n",
      " [ 1.3569404]]\n",
      "Reward : [-7.29791079]\n",
      "--------------------------------------------------\n",
      "Step : 30\n",
      "Current State : [-0.8179781 -0.5752494  1.0265541]\n",
      "Currrent Action : tensor([[0.4601]])\n",
      "Next State : [[-0.8179781]\n",
      " [-0.5752494]\n",
      " [ 1.0265541]]\n",
      "Reward : [-6.84079897]\n",
      "--------------------------------------------------\n",
      "Step : 31\n",
      "Current State : [-0.7963139  -0.6048836   0.73421365]\n",
      "Currrent Action : tensor([[0.9273]])\n",
      "Next State : [[-0.7963139 ]\n",
      " [-0.6048836 ]\n",
      " [ 0.73421365]]\n",
      "Reward : [-6.50048225]\n",
      "--------------------------------------------------\n",
      "Step : 32\n",
      "Current State : [-0.78809744 -0.61555046  0.2692916 ]\n",
      "Currrent Action : tensor([[-0.0751]])\n",
      "Next State : [[-0.78809744]\n",
      " [-0.61555046]\n",
      " [ 0.2692916 ]]\n",
      "Reward : [-6.26384189]\n",
      "--------------------------------------------------\n",
      "Step : 33\n",
      "Current State : [-0.7986675  -0.6017726  -0.34731132]\n",
      "Currrent Action : tensor([[-1.0329]])\n",
      "Next State : [[-0.7986675 ]\n",
      " [-0.6017726 ]\n",
      " [-0.34731132]]\n",
      "Reward : [-6.15132259]\n",
      "--------------------------------------------------\n",
      "Step : 34\n",
      "Current State : [-0.8203542  -0.57185566 -0.73905134]\n",
      "Currrent Action : tensor([[0.3973]])\n",
      "Next State : [[-0.8203542 ]\n",
      " [-0.57185566]\n",
      " [-0.73905134]]\n",
      "Reward : [-6.24160714]\n",
      "--------------------------------------------------\n",
      "Step : 35\n",
      "Current State : [-0.8581841 -0.513342  -1.3938313]\n",
      "Currrent Action : tensor([[-1.5059]])\n",
      "Next State : [[-0.8581841]\n",
      " [-0.513342 ]\n",
      " [-1.3938313]]\n",
      "Reward : [-6.4720977]\n",
      "--------------------------------------------------\n",
      "Step : 36\n",
      "Current State : [-0.89562243 -0.44481507 -1.5621362 ]\n",
      "Currrent Action : tensor([[1.4447]])\n",
      "Next State : [[-0.89562243]\n",
      " [-0.44481507]\n",
      " [-1.5621362 ]]\n",
      "Reward : [-6.96946402]\n",
      "--------------------------------------------------\n",
      "Step : 37\n",
      "Current State : [-0.93138605 -0.36403292 -1.7674698 ]\n",
      "Currrent Action : tensor([[0.8552]])\n",
      "Next State : [[-0.93138605]\n",
      " [-0.36403292]\n",
      " [-1.7674698 ]]\n",
      "Reward : [-7.43050809]\n",
      "--------------------------------------------------\n",
      "Step : 38\n",
      "Current State : [-0.9612451 -0.2756951 -1.8656305]\n",
      "Currrent Action : tensor([[1.1658]])\n",
      "Next State : [[-0.9612451]\n",
      " [-0.2756951]\n",
      " [-1.8656305]]\n",
      "Reward : [-7.981106]\n",
      "--------------------------------------------------\n",
      "Step : 39\n",
      "Current State : [-0.98522264 -0.17127867 -2.1437075 ]\n",
      "Currrent Action : tensor([[-0.4754]])\n",
      "Next State : [[-0.98522264]\n",
      " [-0.17127867]\n",
      " [-2.1437075 ]]\n",
      "Reward : [-8.54092996]\n",
      "--------------------------------------------------\n",
      "Step : 40\n",
      "Current State : [-0.9984249  -0.05610509 -2.3198562 ]\n",
      "Currrent Action : tensor([[-0.3179]])\n",
      "Next State : [[-0.9984249 ]\n",
      " [-0.05610509]\n",
      " [-2.3198562 ]]\n",
      "Reward : [-9.27737326]\n",
      "--------------------------------------------------\n",
      "Step : 41\n",
      "Current State : [-0.9988386   0.04818108 -2.0866861 ]\n",
      "Currrent Action : tensor([[1.8350]])\n",
      "Next State : [[-0.9988386 ]\n",
      " [ 0.04818108]\n",
      " [-2.0866861 ]]\n",
      "Reward : [-10.06159205]\n",
      "--------------------------------------------------\n",
      "Step : 42\n",
      "Current State : [-0.98903835  0.14765875 -2.0000181 ]\n",
      "Currrent Action : tensor([[0.3369]])\n",
      "Next State : [[-0.98903835]\n",
      " [ 0.14765875]\n",
      " [-2.0000181 ]]\n",
      "Reward : [-10.00461913]\n",
      "--------------------------------------------------\n",
      "Step : 43\n",
      "Current State : [-0.9683596   0.24955893 -2.0804818 ]\n",
      "Currrent Action : tensor([[-1.2747]])\n",
      "Next State : [[-0.9683596 ]\n",
      " [ 0.24955893]\n",
      " [-2.0804818 ]]\n",
      "Reward : [-9.36202786]\n",
      "--------------------------------------------------\n",
      "Step : 44\n",
      "Current State : [-0.9392972  0.3431047 -1.9599098]\n",
      "Currrent Action : tensor([[-0.4440]])\n",
      "Next State : [[-0.9392972]\n",
      " [ 0.3431047]\n",
      " [-1.9599098]]\n",
      "Reward : [-8.78148449]\n",
      "--------------------------------------------------\n",
      "Step : 45\n",
      "Current State : [-0.91031706  0.41391164 -1.5305332 ]\n",
      "Currrent Action : tensor([[1.1470]])\n",
      "Next State : [[-0.91031706]\n",
      " [ 0.41391164]\n",
      " [-1.5305332 ]]\n",
      "Reward : [-8.17720023]\n",
      "--------------------------------------------------\n",
      "Step : 46\n",
      "Current State : [-0.8848137   0.46594492 -1.1591072 ]\n",
      "Currrent Action : tensor([[0.4066]])\n",
      "Next State : [[-0.8848137 ]\n",
      " [ 0.46594492]\n",
      " [-1.1591072 ]]\n",
      "Reward : [-7.60480604]\n",
      "--------------------------------------------------\n",
      "Step : 47\n",
      "Current State : [-0.86030567  0.50977856 -1.0045025 ]\n",
      "Currrent Action : tensor([[-1.2990]])\n",
      "Next State : [[-0.86030567]\n",
      " [ 0.50977856]\n",
      " [-1.0045025 ]]\n",
      "Reward : [-7.19510704]\n",
      "--------------------------------------------------\n",
      "Step : 48\n",
      "Current State : [-0.850081    0.5266519  -0.39459574]\n",
      "Currrent Action : tensor([[1.5172]])\n",
      "Next State : [[-0.850081  ]\n",
      " [ 0.5266519 ]\n",
      " [-0.39459574]]\n",
      "Reward : [-6.89790816]\n",
      "--------------------------------------------------\n",
      "Step : 49\n",
      "Current State : [-8.5008729e-01  5.2664179e-01  2.3748554e-04]\n",
      "Currrent Action : tensor([[-0.0010]])\n",
      "Next State : [[-8.5008729e-01]\n",
      " [ 5.2664179e-01]\n",
      " [ 2.3748554e-04]]\n",
      "Reward : [-6.70780581]\n",
      "--------------------------------------------------\n",
      "Step : 50\n",
      "Current State : [-0.86329377  0.50470173  0.5121773 ]\n",
      "Currrent Action : tensor([[0.7797]])\n",
      "Next State : [[-0.86329377]\n",
      " [ 0.50470173]\n",
      " [ 0.5121773 ]]\n",
      "Reward : [-6.69290464]\n",
      "--------------------------------------------------\n",
      "Step : 51\n",
      "Current State : [-0.881496   0.4721915  0.7452233]\n",
      "Currrent Action : tensor([[-0.9699]])\n",
      "Next State : [[-0.881496 ]\n",
      " [ 0.4721915]\n",
      " [ 0.7452233]]\n",
      "Reward : [-6.85262324]\n",
      "--------------------------------------------------\n",
      "Step : 52\n",
      "Current State : [-0.90505576  0.4252929   1.0497954 ]\n",
      "Currrent Action : tensor([[-0.3305]])\n",
      "Next State : [[-0.90505576]\n",
      " [ 0.4252929 ]\n",
      " [ 1.0497954 ]]\n",
      "Reward : [-7.0771772]\n",
      "--------------------------------------------------\n",
      "Step : 53\n",
      "Current State : [-0.935797    0.35253942  1.5800424 ]\n",
      "Currrent Action : tensor([[1.4085]])\n",
      "Next State : [[-0.935797  ]\n",
      " [ 0.35253942]\n",
      " [ 1.5800424 ]]\n",
      "Reward : [-7.41465495]\n",
      "--------------------------------------------------\n",
      "Step : 54\n",
      "Current State : [-0.96497196  0.26235312  1.8964686 ]\n",
      "Currrent Action : tensor([[0.3468]])\n",
      "Next State : [[-0.96497196]\n",
      " [ 0.26235312]\n",
      " [ 1.8964686 ]]\n",
      "Reward : [-7.98545498]\n",
      "--------------------------------------------------\n",
      "Step : 55\n",
      "Current State : [-0.9862527   0.16524412  1.9890882 ]\n",
      "Currrent Action : tensor([[-0.6943]])\n",
      "Next State : [[-0.9862527 ]\n",
      " [ 0.16524412]\n",
      " [ 1.9890882 ]]\n",
      "Reward : [-8.63228079]\n",
      "--------------------------------------------------\n",
      "Step : 56\n",
      "Current State : [-0.9982273   0.05951719  2.129063  ]\n",
      "Currrent Action : tensor([[0.1069]])\n",
      "Next State : [[-0.9982273 ]\n",
      " [ 0.05951719]\n",
      " [ 2.129063  ]]\n",
      "Reward : [-9.24977731]\n",
      "--------------------------------------------------\n",
      "Step : 57\n",
      "Current State : [-0.99857    -0.05345935  2.2607446 ]\n",
      "Currrent Action : tensor([[0.5803]])\n",
      "Next State : [[-0.99857   ]\n",
      " [-0.05345935]\n",
      " [ 2.2607446 ]]\n",
      "Reward : [-9.95259989]\n",
      "--------------------------------------------------\n",
      "Step : 58\n",
      "Current State : [-0.98461163 -0.17475693  2.443481  ]\n",
      "Currrent Action : tensor([[1.4855]])\n",
      "Next State : [[-0.98461163]\n",
      " [-0.17475693]\n",
      " [ 2.443481  ]]\n",
      "Reward : [-10.04971326]\n",
      "--------------------------------------------------\n",
      "Step : 59\n",
      "Current State : [-0.9599731 -0.2800923  2.1646273]\n",
      "Currrent Action : tensor([[-0.9852]])\n",
      "Next State : [[-0.9599731]\n",
      " [-0.2800923]\n",
      " [ 2.1646273]]\n",
      "Reward : [-9.39479371]\n",
      "--------------------------------------------------\n",
      "Step : 60\n",
      "Current State : [-0.9308844  -0.36531386  1.8015939 ]\n",
      "Currrent Action : tensor([[-1.0198]])\n",
      "Next State : [[-0.9308844 ]\n",
      " [-0.36531386]\n",
      " [ 1.8015939 ]]\n",
      "Reward : [-8.63606399]\n",
      "--------------------------------------------------\n",
      "Step : 61\n",
      "Current State : [-0.8936644  -0.44873595  1.8276085 ]\n",
      "Currrent Action : tensor([[2.0391]])\n",
      "Next State : [[-0.8936644 ]\n",
      " [-0.44873595]\n",
      " [ 1.8276085 ]]\n",
      "Reward : [-7.98830947]\n",
      "--------------------------------------------------\n",
      "Step : 62\n",
      "Current State : [-0.8599017 -0.5104596  1.4073768]\n",
      "Currrent Action : tensor([[-0.5579]])\n",
      "Next State : [[-0.8599017]\n",
      " [-0.5104596]\n",
      " [ 1.4073768]]\n",
      "Reward : [-7.49659923]\n",
      "--------------------------------------------------\n",
      "Step : 63\n",
      "Current State : [-0.8310711  -0.55616623  1.0809283 ]\n",
      "Currrent Action : tensor([[0.3760]])\n",
      "Next State : [[-0.8310711 ]\n",
      " [-0.55616623]\n",
      " [ 1.0809283 ]]\n",
      "Reward : [-6.98878869]\n",
      "--------------------------------------------------\n",
      "Step : 64\n",
      "Current State : [-0.81007266 -0.5863295   0.7350943 ]\n",
      "Currrent Action : tensor([[0.4753]])\n",
      "Next State : [[-0.81007266]\n",
      " [-0.5863295 ]\n",
      " [ 0.7350943 ]]\n",
      "Reward : [-6.62888767]\n",
      "--------------------------------------------------\n",
      "Step : 65\n",
      "Current State : [-0.79893625 -0.6014157   0.37503174]\n",
      "Currrent Action : tensor([[0.5312]])\n",
      "Next State : [[-0.79893625]\n",
      " [-0.6014157 ]\n",
      " [ 0.37503174]]\n",
      "Reward : [-6.37990732]\n",
      "--------------------------------------------------\n",
      "Step : 66\n",
      "Current State : [-0.7960775  -0.6051947   0.09477013]\n",
      "Currrent Action : tensor([[1.1387]])\n",
      "Next State : [[-0.7960775 ]\n",
      " [-0.6051947 ]\n",
      " [ 0.09477013]]\n",
      "Reward : [-6.24697863]\n",
      "--------------------------------------------------\n",
      "Step : 67\n",
      "Current State : [-0.80525416 -0.5929298  -0.306362  ]\n",
      "Currrent Action : tensor([[0.3518]])\n",
      "Next State : [[-0.80525416]\n",
      " [-0.5929298 ]\n",
      " [-0.306362  ]]\n",
      "Reward : [-6.20900384]\n",
      "--------------------------------------------------\n",
      "Step : 68\n",
      "Current State : [-0.8304094 -0.5571537 -0.8747614]\n",
      "Currrent Action : tensor([[-0.8247]])\n",
      "Next State : [[-0.8304094]\n",
      " [-0.5571537]\n",
      " [-0.8747614]]\n",
      "Reward : [-6.29461509]\n",
      "--------------------------------------------------\n",
      "Step : 69\n",
      "Current State : [-0.8720979 -0.4893314 -1.5926267]\n",
      "Currrent Action : tensor([[-2.1616]])\n",
      "Next State : [[-0.8720979]\n",
      " [-0.4893314]\n",
      " [-1.5926267]]\n",
      "Reward : [-6.58627696]\n",
      "--------------------------------------------------\n",
      "Step : 70\n",
      "Current State : [-0.9097834  -0.41508323 -1.6657728 ]\n",
      "Currrent Action : tensor([[1.9590]])\n",
      "Next State : [[-0.9097834 ]\n",
      " [-0.41508323]\n",
      " [-1.6657728 ]]\n",
      "Reward : [-7.17580256]\n",
      "--------------------------------------------------\n",
      "Step : 71\n",
      "Current State : [-0.94873446 -0.31607416 -2.1289127 ]\n",
      "Currrent Action : tensor([[-1.0122]])\n",
      "Next State : [[-0.94873446]\n",
      " [-0.31607416]\n",
      " [-2.1289127 ]]\n",
      "Reward : [-7.64190342]\n",
      "--------------------------------------------------\n",
      "Step : 72\n",
      "Current State : [-0.9790234  -0.20374788 -2.3280811 ]\n",
      "Currrent Action : tensor([[0.2526]])\n",
      "Next State : [[-0.9790234 ]\n",
      " [-0.20374788]\n",
      " [-2.3280811 ]]\n",
      "Reward : [-8.40571336]\n",
      "--------------------------------------------------\n",
      "Step : 73\n",
      "Current State : [-0.9953879  -0.09593207 -2.1820955 ]\n",
      "Currrent Action : tensor([[1.9920]])\n",
      "Next State : [[-0.9953879 ]\n",
      " [-0.09593207]\n",
      " [-2.1820955 ]]\n",
      "Reward : [-9.16845637]\n",
      "--------------------------------------------------\n",
      "Step : 74\n",
      "Current State : [-0.9997978   0.02010689 -2.3237612 ]\n",
      "Currrent Action : tensor([[-0.4648]])\n",
      "Next State : [[-0.9997978 ]\n",
      " [ 0.02010689]\n",
      " [-2.3237612 ]]\n",
      "Reward : [-9.75151852]\n",
      "--------------------------------------------------\n",
      "Step : 75\n",
      "Current State : [-0.99192214  0.12684816 -2.1416516 ]\n",
      "Currrent Action : tensor([[1.1135]])\n",
      "Next State : [[-0.99192214]\n",
      " [ 0.12684816]\n",
      " [-2.1416516 ]]\n",
      "Reward : [-10.28489153]\n",
      "--------------------------------------------------\n",
      "Step : 76\n",
      "Current State : [-0.97633654  0.21625662 -1.815758  ]\n",
      "Currrent Action : tensor([[1.5384]])\n",
      "Next State : [[-0.97633654]\n",
      " [ 0.21625662]\n",
      " [-1.815758  ]]\n",
      "Reward : [-9.54765217]\n",
      "--------------------------------------------------\n",
      "Step : 77\n",
      "Current State : [-0.9520342   0.30599165 -1.8600228 ]\n",
      "Currrent Action : tensor([[-1.3764]])\n",
      "Next State : [[-0.9520342 ]\n",
      " [ 0.30599165]\n",
      " [-1.8600228 ]]\n",
      "Reward : [-8.87911052]\n",
      "--------------------------------------------------\n",
      "Step : 78\n",
      "Current State : [-0.92300993  0.3847761  -1.6797072 ]\n",
      "Currrent Action : tensor([[-0.3279]])\n",
      "Next State : [[-0.92300993]\n",
      " [ 0.3847761 ]\n",
      " [-1.6797072 ]]\n",
      "Reward : [-8.3584447]\n",
      "--------------------------------------------------\n",
      "Step : 79\n",
      "Current State : [-0.892527    0.45099404 -1.4582711 ]\n",
      "Currrent Action : tensor([[-0.4476]])\n",
      "Next State : [[-0.892527  ]\n",
      " [ 0.45099404]\n",
      " [-1.4582711 ]]\n",
      "Reward : [-7.82630423]\n",
      "--------------------------------------------------\n",
      "Step : 80\n",
      "Current State : [-0.8681826  0.4962449 -1.0277879]\n",
      "Currrent Action : tensor([[0.6149]])\n",
      "Next State : [[-0.8681826]\n",
      " [ 0.4962449]\n",
      " [-1.0277879]]\n",
      "Reward : [-7.36177948]\n",
      "--------------------------------------------------\n",
      "Step : 81\n",
      "Current State : [-0.8576728   0.51419586 -0.41603255]\n",
      "Currrent Action : tensor([[1.5971]])\n",
      "Next State : [[-0.8576728 ]\n",
      " [ 0.51419586]\n",
      " [-0.41603255]]\n",
      "Reward : [-6.98477135]\n",
      "--------------------------------------------------\n",
      "Step : 82\n",
      "Current State : [-0.86285865  0.50544536  0.20343591]\n",
      "Currrent Action : tensor([[1.5588]])\n",
      "Next State : [[-0.86285865]\n",
      " [ 0.50544536]\n",
      " [ 0.20343591]]\n",
      "Reward : [-6.78765935]\n",
      "--------------------------------------------------\n",
      "Step : 83\n",
      "Current State : [-0.8750382   0.48405382  0.49232954]\n",
      "Currrent Action : tensor([[-0.6013]])\n",
      "Next State : [[-0.8750382 ]\n",
      " [ 0.48405382]\n",
      " [ 0.49232954]]\n",
      "Reward : [-6.82544908]\n",
      "--------------------------------------------------\n",
      "Step : 84\n",
      "Current State : [-0.89515656  0.44575185  0.865351  ]\n",
      "Currrent Action : tensor([[0.0665]])\n",
      "Next State : [[-0.89515656]\n",
      " [ 0.44575185]\n",
      " [ 0.865351  ]]\n",
      "Reward : [-6.97437962]\n",
      "--------------------------------------------------\n",
      "Step : 85\n",
      "Current State : [-0.9211994   0.38909087  1.2473899 ]\n",
      "Currrent Action : tensor([[0.3182]])\n",
      "Next State : [[-0.9211994 ]\n",
      " [ 0.38909087]\n",
      " [ 1.2473899 ]]\n",
      "Reward : [-7.25512635]\n",
      "--------------------------------------------------\n",
      "Step : 86\n",
      "Current State : [-0.9452802   0.32625964  1.3460101 ]\n",
      "Currrent Action : tensor([[-1.2880]])\n",
      "Next State : [[-0.9452802 ]\n",
      " [ 0.32625964]\n",
      " [ 1.3460101 ]]\n",
      "Reward : [-7.67553685]\n",
      "--------------------------------------------------\n",
      "Step : 87\n",
      "Current State : [-0.97122747  0.23815371  1.8375913 ]\n",
      "Currrent Action : tensor([[1.6459]])\n",
      "Next State : [[-0.97122747]\n",
      " [ 0.23815371]\n",
      " [ 1.8375913 ]]\n",
      "Reward : [-8.07576144]\n",
      "--------------------------------------------------\n",
      "Step : 88\n",
      "Current State : [-0.9922407   0.12433151  2.3162065 ]\n",
      "Currrent Action : tensor([[2.1405]])\n",
      "Next State : [[-0.9922407 ]\n",
      " [ 0.12433151]\n",
      " [ 2.3162065 ]]\n",
      "Reward : [-8.75821921]\n",
      "--------------------------------------------------\n",
      "Step : 89\n",
      "Current State : [-0.9999929  -0.00376169  2.5683157 ]\n",
      "Currrent Action : tensor([[1.0591]])\n",
      "Next State : [[-0.9999929 ]\n",
      " [-0.00376169]\n",
      " [ 2.5683157 ]]\n",
      "Reward : [-9.6395212]\n",
      "--------------------------------------------------\n",
      "Step : 90\n",
      "Current State : [-0.9911688  -0.13260645  2.58473   ]\n",
      "Currrent Action : tensor([[0.1282]])\n",
      "Next State : [[-0.9911688 ]\n",
      " [-0.13260645]\n",
      " [ 2.58473   ]]\n",
      "Reward : [-10.50562415]\n",
      "--------------------------------------------------\n",
      "Step : 91\n",
      "Current State : [-0.9702399  -0.24214578  2.2315726 ]\n",
      "Currrent Action : tensor([[-1.6913]])\n",
      "Next State : [[-0.9702399 ]\n",
      " [-0.24214578]\n",
      " [ 2.2315726 ]]\n",
      "Reward : [-9.72258412]\n",
      "--------------------------------------------------\n",
      "Step : 92\n",
      "Current State : [-0.9384752 -0.3453466  2.1606252]\n",
      "Currrent Action : tensor([[0.7377]])\n",
      "Next State : [[-0.9384752]\n",
      " [-0.3453466]\n",
      " [ 2.1606252]]\n",
      "Reward : [-8.89123661]\n",
      "--------------------------------------------------\n",
      "Step : 93\n",
      "Current State : [-0.89620185 -0.44364655  2.1411078 ]\n",
      "Currrent Action : tensor([[1.5966]])\n",
      "Next State : [[-0.89620185]\n",
      " [-0.44364655]\n",
      " [ 2.1411078 ]]\n",
      "Reward : [-8.24781421]\n",
      "--------------------------------------------------\n",
      "Step : 94\n",
      "Current State : [-0.84876955 -0.52876294  1.9495796 ]\n",
      "Currrent Action : tensor([[0.9414]])\n",
      "Next State : [[-0.84876955]\n",
      " [-0.52876294]\n",
      " [ 1.9495796 ]]\n",
      "Reward : [-7.65206452]\n",
      "--------------------------------------------------\n",
      "Step : 95\n",
      "Current State : [-0.8011269 -0.5984945  1.6895627]\n",
      "Currrent Action : tensor([[0.9104]])\n",
      "Next State : [[-0.8011269]\n",
      " [-0.5984945]\n",
      " [ 1.6895627]]\n",
      "Reward : [-7.06029764]\n",
      "--------------------------------------------------\n",
      "Step : 96\n",
      "Current State : [-0.761787   -0.64782757  1.2621722 ]\n",
      "Currrent Action : tensor([[0.1432]])\n",
      "Next State : [[-0.761787  ]\n",
      " [-0.64782757]\n",
      " [ 1.2621722 ]]\n",
      "Reward : [-6.53534302]\n",
      "--------------------------------------------------\n",
      "Step : 97\n",
      "Current State : [-0.7325154 -0.6807505  0.8811491]\n",
      "Currrent Action : tensor([[0.6990]])\n",
      "Next State : [[-0.7325154]\n",
      " [-0.6807505]\n",
      " [ 0.8811491]]\n",
      "Reward : [-6.09809993]\n",
      "--------------------------------------------------\n",
      "Step : 98\n",
      "Current State : [-0.71987426 -0.69410443  0.36776945]\n",
      "Currrent Action : tensor([[-0.0188]])\n",
      "Next State : [[-0.71987426]\n",
      " [-0.69410443]\n",
      " [ 0.36776945]]\n",
      "Reward : [-5.80316325]\n",
      "--------------------------------------------------\n",
      "Step : 99\n",
      "Current State : [-0.733022   -0.680205   -0.38265818]\n",
      "Currrent Action : tensor([[-1.5323]])\n",
      "Next State : [[-0.733022  ]\n",
      " [-0.680205  ]\n",
      " [-0.38265818]]\n",
      "Reward : [-5.65373203]\n",
      "learning iteration : 14\n",
      "--------------------------------------------------\n",
      "Step : 0\n",
      "Current State : [ 0.8722244   0.48910597 -0.18183392]\n",
      "Currrent Action : tensor([[1.9187]])\n",
      "Next State : [[ 0.8722244 ]\n",
      " [ 0.48910597]\n",
      " [-0.18183392]]\n",
      "Reward : [-0.34520712]\n",
      "--------------------------------------------------\n",
      "Step : 1\n",
      "Current State : [0.8622731  0.50644356 0.39981657]\n",
      "Currrent Action : tensor([[1.4321]])\n",
      "Next State : [[0.8622731 ]\n",
      " [0.50644356]\n",
      " [0.39981657]]\n",
      "Reward : [-0.26654427]\n",
      "--------------------------------------------------\n",
      "Step : 2\n",
      "Current State : [0.849134   0.52817744 0.5079492 ]\n",
      "Currrent Action : tensor([[-1.8113]])\n",
      "Next State : [[0.849134  ]\n",
      " [0.52817744]\n",
      " [0.5079492 ]]\n",
      "Reward : [-0.30128598]\n",
      "--------------------------------------------------\n",
      "Step : 3\n",
      "Current State : [0.8257841  0.5639864  0.85505104]\n",
      "Currrent Action : tensor([[-0.3269]])\n",
      "Next State : [[0.8257841 ]\n",
      " [0.5639864 ]\n",
      " [0.85505104]]\n",
      "Reward : [-0.33554775]\n",
      "--------------------------------------------------\n",
      "Step : 4\n",
      "Current State : [0.7938597 0.6081009 1.0892172]\n",
      "Currrent Action : tensor([[-1.2588]])\n",
      "Next State : [[0.7938597]\n",
      " [0.6081009]\n",
      " [1.0892172]]\n",
      "Reward : [-0.43374287]\n",
      "--------------------------------------------------\n",
      "Step : 5\n",
      "Current State : [0.75004494 0.66138685 1.3800019 ]\n",
      "Currrent Action : tensor([[-1.1019]])\n",
      "Next State : [[0.75004494]\n",
      " [0.66138685]\n",
      " [1.3800019 ]]\n",
      "Reward : [-0.54713312]\n",
      "--------------------------------------------------\n",
      "Step : 6\n",
      "Current State : [0.6860957 0.7275113 1.8404266]\n",
      "Currrent Action : tensor([[-0.2374]])\n",
      "Next State : [[0.6860957]\n",
      " [0.7275113]\n",
      " [1.8404266]]\n",
      "Reward : [-0.71274343]\n",
      "--------------------------------------------------\n",
      "Step : 7\n",
      "Current State : [0.59950024 0.80037457 2.2646444 ]\n",
      "Currrent Action : tensor([[-0.8094]])\n",
      "Next State : [[0.59950024]\n",
      " [0.80037457]\n",
      " [2.2646444 ]]\n",
      "Reward : [-1.00308806]\n",
      "--------------------------------------------------\n",
      "Step : 8\n",
      "Current State : [0.4714699  0.88188213 3.038394  ]\n",
      "Currrent Action : tensor([[1.1565]])\n",
      "Next State : [[0.4714699 ]\n",
      " [0.88188213]\n",
      " [3.038394  ]]\n",
      "Reward : [-1.37523393]\n",
      "--------------------------------------------------\n",
      "Step : 9\n",
      "Current State : [0.30181736 0.95336574 3.6871698 ]\n",
      "Currrent Action : tensor([[-0.0842]])\n",
      "Next State : [[0.30181736]\n",
      " [0.95336574]\n",
      " [3.6871698 ]]\n",
      "Reward : [-2.08924423]\n",
      "--------------------------------------------------\n",
      "Step : 10\n",
      "Current State : [0.09244824 0.99571747 4.280358  ]\n",
      "Currrent Action : tensor([[-0.8122]])\n",
      "Next State : [[0.09244824]\n",
      " [0.99571747]\n",
      " [4.280358  ]]\n",
      "Reward : [-2.95837848]\n",
      "--------------------------------------------------\n",
      "Step : 11\n",
      "Current State : [-0.15497872  0.9879178   4.963727  ]\n",
      "Currrent Action : tensor([[-0.4228]])\n",
      "Next State : [[-0.15497872]\n",
      " [ 0.9879178 ]\n",
      " [ 4.963727  ]]\n",
      "Reward : [-4.01744732]\n",
      "--------------------------------------------------\n",
      "Step : 12\n",
      "Current State : [-0.43690816  0.8995061   5.9310527 ]\n",
      "Currrent Action : tensor([[1.5092]])\n",
      "Next State : [[-0.43690816]\n",
      " [ 0.8995061 ]\n",
      " [ 5.9310527 ]]\n",
      "Reward : [-5.44660113]\n",
      "--------------------------------------------------\n",
      "Step : 13\n",
      "Current State : [-0.70392984  0.7102695   6.575135  ]\n",
      "Currrent Action : tensor([[-0.2036]])\n",
      "Next State : [[-0.70392984]\n",
      " [ 0.7102695 ]\n",
      " [ 6.575135  ]]\n",
      "Reward : [-7.61012628]\n",
      "--------------------------------------------------\n",
      "Step : 14\n",
      "Current State : [-0.90352863  0.42852777  6.940366  ]\n",
      "Currrent Action : tensor([[-1.1165]])\n",
      "Next State : [[-0.90352863]\n",
      " [ 0.42852777]\n",
      " [ 6.940366  ]]\n",
      "Reward : [-9.85503441]\n",
      "--------------------------------------------------\n",
      "Step : 15\n",
      "Current State : [-0.997135    0.07564298  7.3429484 ]\n",
      "Currrent Action : tensor([[0.5412]])\n",
      "Next State : [[-0.997135  ]\n",
      " [ 0.07564298]\n",
      " [ 7.3429484 ]]\n",
      "Reward : [-12.10030416]\n",
      "--------------------------------------------------\n",
      "Step : 16\n",
      "Current State : [-0.95860696 -0.28473264  7.2888565 ]\n",
      "Currrent Action : tensor([[-0.7388]])\n",
      "Next State : [[-0.95860696]\n",
      " [-0.28473264]\n",
      " [ 7.2888565 ]]\n",
      "Reward : [-14.79203905]\n",
      "--------------------------------------------------\n",
      "Step : 17\n",
      "Current State : [-0.809502  -0.5871171  6.7753067]\n",
      "Currrent Action : tensor([[-2.0276]])\n",
      "Next State : [[-0.809502 ]\n",
      " [-0.5871171]\n",
      " [ 6.7753067]]\n",
      "Reward : [-13.45558223]\n",
      "--------------------------------------------------\n",
      "Step : 18\n",
      "Current State : [-0.58169496 -0.813407    6.4498506 ]\n",
      "Currrent Action : tensor([[0.7659]])\n",
      "Next State : [[-0.58169496]\n",
      " [-0.813407  ]\n",
      " [ 6.4498506 ]]\n",
      "Reward : [-10.91176269]\n",
      "--------------------------------------------------\n",
      "Step : 19\n",
      "Current State : [-0.31745782 -0.9482724   5.95527   ]\n",
      "Currrent Action : tensor([[0.7698]])\n",
      "Next State : [[-0.31745782]\n",
      " [-0.9482724 ]\n",
      " [ 5.95527   ]]\n",
      "Reward : [-8.96379227]\n",
      "--------------------------------------------------\n",
      "Step : 20\n",
      "Current State : [-0.05386058 -0.99854845  5.383216  ]\n",
      "Currrent Action : tensor([[0.9277]])\n",
      "Next State : [[-0.05386058]\n",
      " [-0.99854845]\n",
      " [ 5.383216  ]]\n",
      "Reward : [-7.13402868]\n",
      "--------------------------------------------------\n",
      "Step : 21\n",
      "Current State : [ 0.17247613 -0.9850137   4.5445914 ]\n",
      "Currrent Action : tensor([[-0.5981]])\n",
      "Next State : [[ 0.17247613]\n",
      " [-0.9850137 ]\n",
      " [ 4.5445914 ]]\n",
      "Reward : [-5.53785378]\n",
      "--------------------------------------------------\n",
      "Step : 22\n",
      "Current State : [ 0.35201797 -0.93599325  3.7276654 ]\n",
      "Currrent Action : tensor([[-0.5211]])\n",
      "Next State : [[ 0.35201797]\n",
      " [-0.93599325]\n",
      " [ 3.7276654 ]]\n",
      "Reward : [-4.01847881]\n",
      "--------------------------------------------------\n",
      "Step : 23\n",
      "Current State : [ 0.4900272  -0.87170714  3.0478952 ]\n",
      "Currrent Action : tensor([[0.1482]])\n",
      "Next State : [[ 0.4900272 ]\n",
      " [-0.87170714]\n",
      " [ 3.0478952 ]]\n",
      "Reward : [-2.85626171]\n",
      "--------------------------------------------------\n",
      "Step : 24\n",
      "Current State : [ 0.5875578 -0.8091822  2.3183289]\n",
      "Currrent Action : tensor([[-0.5052]])\n",
      "Next State : [[ 0.5875578]\n",
      " [-0.8091822]\n",
      " [ 2.3183289]]\n",
      "Reward : [-2.05001537]\n",
      "--------------------------------------------------\n",
      "Step : 25\n",
      "Current State : [ 0.64487153 -0.764291    1.4563556 ]\n",
      "Currrent Action : tensor([[-1.7006]])\n",
      "Next State : [[ 0.64487153]\n",
      " [-0.764291  ]\n",
      " [ 1.4563556 ]]\n",
      "Reward : [-1.42915125]\n",
      "--------------------------------------------------\n",
      "Step : 26\n",
      "Current State : [ 0.67252576 -0.74007374  0.7352233 ]\n",
      "Currrent Action : tensor([[-0.9861]])\n",
      "Next State : [[ 0.67252576]\n",
      " [-0.74007374]\n",
      " [ 0.7352233 ]]\n",
      "Reward : [-0.96986712]\n",
      "--------------------------------------------------\n",
      "Step : 27\n",
      "Current State : [ 0.6867816  -0.72686386  0.3887109 ]\n",
      "Currrent Action : tensor([[1.3903]])\n",
      "Next State : [[ 0.6867816 ]\n",
      " [-0.72686386]\n",
      " [ 0.3887109 ]]\n",
      "Reward : [-0.75017711]\n",
      "--------------------------------------------------\n",
      "Step : 28\n",
      "Current State : [ 0.6827235  -0.73067683 -0.11136646]\n",
      "Currrent Action : tensor([[0.3005]])\n",
      "Next State : [[ 0.6827235 ]\n",
      " [-0.73067683]\n",
      " [-0.11136646]]\n",
      "Reward : [-0.67737991]\n",
      "--------------------------------------------------\n",
      "Step : 29\n",
      "Current State : [ 0.65968966 -0.7515381  -0.62155545]\n",
      "Currrent Action : tensor([[0.2521]])\n",
      "Next State : [[ 0.65968966]\n",
      " [-0.7515381 ]\n",
      " [-0.62155545]]\n",
      "Reward : [-0.67257722]\n",
      "--------------------------------------------------\n",
      "Step : 30\n",
      "Current State : [ 0.6257911  -0.77999073 -0.885209  ]\n",
      "Currrent Action : tensor([[2.2922]])\n",
      "Next State : [[ 0.6257911 ]\n",
      " [-0.77999073]\n",
      " [-0.885209  ]]\n",
      "Reward : [-0.76579718]\n",
      "--------------------------------------------------\n",
      "Step : 31\n",
      "Current State : [ 0.5643949  -0.82550496 -1.5289059 ]\n",
      "Currrent Action : tensor([[-0.3914]])\n",
      "Next State : [[ 0.5643949 ]\n",
      " [-0.82550496]\n",
      " [-1.5289059 ]]\n",
      "Reward : [-0.87891305]\n",
      "--------------------------------------------------\n",
      "Step : 32\n",
      "Current State : [ 0.48342273 -0.875387   -1.9027913 ]\n",
      "Currrent Action : tensor([[1.6350]])\n",
      "Next State : [[ 0.48342273]\n",
      " [-0.875387  ]\n",
      " [-1.9027913 ]]\n",
      "Reward : [-1.17945638]\n",
      "--------------------------------------------------\n",
      "Step : 33\n",
      "Current State : [ 0.36818618 -0.92975205 -2.5500615 ]\n",
      "Currrent Action : tensor([[0.0618]])\n",
      "Next State : [[ 0.36818618]\n",
      " [-0.92975205]\n",
      " [-2.5500615 ]]\n",
      "Reward : [-1.49892415]\n",
      "--------------------------------------------------\n",
      "Step : 34\n",
      "Current State : [ 0.22626397 -0.974066   -2.9763389 ]\n",
      "Currrent Action : tensor([[1.8069]])\n",
      "Next State : [[ 0.22626397]\n",
      " [-0.974066  ]\n",
      " [-2.9763389 ]]\n",
      "Reward : [-2.07855891]\n",
      "--------------------------------------------------\n",
      "Step : 35\n",
      "Current State : [ 0.05715597 -0.9983653  -3.4210665 ]\n",
      "Currrent Action : tensor([[1.9055]])\n",
      "Next State : [[ 0.05715597]\n",
      " [-0.9983653 ]\n",
      " [-3.4210665 ]]\n",
      "Reward : [-2.69194645]\n",
      "--------------------------------------------------\n",
      "Step : 36\n",
      "Current State : [-0.13659738 -0.99062663 -3.8842585 ]\n",
      "Currrent Action : tensor([[1.9039]])\n",
      "Next State : [[-0.13659738]\n",
      " [-0.99062663]\n",
      " [-3.8842585 ]]\n",
      "Reward : [-3.46500709]\n",
      "--------------------------------------------------\n",
      "Step : 37\n",
      "Current State : [-0.34607768 -0.93820584 -4.3272285 ]\n",
      "Currrent Action : tensor([[2.2997]])\n",
      "Next State : [[-0.34607768]\n",
      " [-0.93820584]\n",
      " [-4.3272285 ]]\n",
      "Reward : [-4.4294027]\n",
      "--------------------------------------------------\n",
      "Step : 38\n",
      "Current State : [-0.5677775  -0.82318205 -5.008325  ]\n",
      "Currrent Action : tensor([[0.1504]])\n",
      "Next State : [[-0.5677775 ]\n",
      " [-0.82318205]\n",
      " [-5.008325  ]]\n",
      "Reward : [-5.57499543]\n",
      "--------------------------------------------------\n",
      "Step : 39\n",
      "Current State : [-0.7668297  -0.64185053 -5.4016757 ]\n",
      "Currrent Action : tensor([[1.4936]])\n",
      "Next State : [[-0.7668297 ]\n",
      " [-0.64185053]\n",
      " [-5.4016757 ]]\n",
      "Reward : [-7.23944714]\n",
      "--------------------------------------------------\n",
      "Step : 40\n",
      "Current State : [-0.9180224 -0.3965285 -5.7835364]\n",
      "Currrent Action : tensor([[0.6635]])\n",
      "Next State : [[-0.9180224]\n",
      " [-0.3965285]\n",
      " [-5.7835364]]\n",
      "Reward : [-8.8947281]\n",
      "--------------------------------------------------\n",
      "Step : 41\n",
      "Current State : [-0.9939345  -0.10997372 -5.9507127 ]\n",
      "Currrent Action : tensor([[0.8681]])\n",
      "Next State : [[-0.9939345 ]\n",
      " [-0.10997372]\n",
      " [-5.9507127 ]]\n",
      "Reward : [-10.81967594]\n",
      "--------------------------------------------------\n",
      "Step : 42\n",
      "Current State : [-0.9819323   0.18923274 -6.0115466 ]\n",
      "Currrent Action : tensor([[0.1443]])\n",
      "Next State : [[-0.9819323 ]\n",
      " [ 0.18923274]\n",
      " [-6.0115466 ]]\n",
      "Reward : [-12.73048099]\n",
      "--------------------------------------------------\n",
      "Step : 43\n",
      "Current State : [-0.8861344   0.46342835 -5.8295894 ]\n",
      "Currrent Action : tensor([[0.2669]])\n",
      "Next State : [[-0.8861344 ]\n",
      " [ 0.46342835]\n",
      " [-5.8295894 ]]\n",
      "Reward : [-12.32359219]\n",
      "--------------------------------------------------\n",
      "Step : 44\n",
      "Current State : [-0.7361232  0.6768476 -5.232229 ]\n",
      "Currrent Action : tensor([[1.6653]])\n",
      "Next State : [[-0.7361232]\n",
      " [ 0.6768476]\n",
      " [-5.232229 ]]\n",
      "Reward : [-10.47536132]\n",
      "--------------------------------------------------\n",
      "Step : 45\n",
      "Current State : [-0.56637585  0.82414705 -4.504459  ]\n",
      "Currrent Action : tensor([[1.4676]])\n",
      "Next State : [[-0.56637585]\n",
      " [ 0.82414705]\n",
      " [-4.504459  ]]\n",
      "Reward : [-8.49076046]\n",
      "--------------------------------------------------\n",
      "Step : 46\n",
      "Current State : [-0.40809157  0.912941   -3.634781  ]\n",
      "Currrent Action : tensor([[1.6771]])\n",
      "Next State : [[-0.40809157]\n",
      " [ 0.912941  ]\n",
      " [-3.634781  ]]\n",
      "Reward : [-6.75331369]\n",
      "--------------------------------------------------\n",
      "Step : 47\n",
      "Current State : [-0.2695381   0.96298975 -2.9489868 ]\n",
      "Currrent Action : tensor([[0.0073]])\n",
      "Next State : [[-0.2695381 ]\n",
      " [ 0.96298975]\n",
      " [-2.9489868 ]]\n",
      "Reward : [-5.28587748]\n",
      "--------------------------------------------------\n",
      "Step : 48\n",
      "Current State : [-0.16586393  0.98614866 -2.1255865 ]\n",
      "Currrent Action : tensor([[0.6744]])\n",
      "Next State : [[-0.16586393]\n",
      " [ 0.98614866]\n",
      " [-2.1255865 ]]\n",
      "Reward : [-4.26937248]\n",
      "--------------------------------------------------\n",
      "Step : 49\n",
      "Current State : [-0.11054954  0.9938706  -1.117161  ]\n",
      "Currrent Action : tensor([[1.7921]])\n",
      "Next State : [[-0.11054954]\n",
      " [ 0.9938706 ]\n",
      " [-1.117161  ]]\n",
      "Reward : [-3.47368762]\n",
      "--------------------------------------------------\n",
      "Step : 50\n",
      "Current State : [-0.10548487  0.9944209  -0.10188971]\n",
      "Currrent Action : tensor([[1.7991]])\n",
      "Next State : [[-0.10548487]\n",
      " [ 0.9944209 ]\n",
      " [-0.10188971]]\n",
      "Reward : [-2.95572706]\n",
      "--------------------------------------------------\n",
      "Step : 51\n",
      "Current State : [-0.12849484  0.9917102   0.46339238]\n",
      "Currrent Action : tensor([[-1.2036]])\n",
      "Next State : [[-0.12849484]\n",
      " [ 0.9917102 ]\n",
      " [ 0.46339238]]\n",
      "Reward : [-2.81306452]\n",
      "--------------------------------------------------\n",
      "Step : 52\n",
      "Current State : [-0.17830785  0.98397475  1.0083076 ]\n",
      "Currrent Action : tensor([[-1.3258]])\n",
      "Next State : [[-0.17830785]\n",
      " [ 0.98397475]\n",
      " [ 1.0083076 ]]\n",
      "Reward : [-2.9120323]\n",
      "--------------------------------------------------\n",
      "Step : 53\n",
      "Current State : [-0.2549125  0.9669641  1.5698147]\n",
      "Currrent Action : tensor([[-1.1765]])\n",
      "Next State : [[-0.2549125]\n",
      " [ 0.9669641]\n",
      " [ 1.5698147]]\n",
      "Reward : [-3.16577234]\n",
      "--------------------------------------------------\n",
      "Step : 54\n",
      "Current State : [-0.36878827  0.92951345  2.3989573 ]\n",
      "Currrent Action : tensor([[0.6928]])\n",
      "Next State : [[-0.36878827]\n",
      " [ 0.92951345]\n",
      " [ 2.3989573 ]]\n",
      "Reward : [-3.5905198]\n",
      "--------------------------------------------------\n",
      "Step : 55\n",
      "Current State : [-0.5039498   0.86373293  3.0092118 ]\n",
      "Currrent Action : tensor([[-0.5792]])\n",
      "Next State : [[-0.5039498 ]\n",
      " [ 0.86373293]\n",
      " [ 3.0092118 ]]\n",
      "Reward : [-4.3724927]\n",
      "--------------------------------------------------\n",
      "Step : 56\n",
      "Current State : [-0.6480911  0.7615628  3.53819  ]\n",
      "Currrent Action : tensor([[-0.7921]])\n",
      "Next State : [[-0.6480911]\n",
      " [ 0.7615628]\n",
      " [ 3.53819  ]]\n",
      "Reward : [-5.31180445]\n",
      "--------------------------------------------------\n",
      "Step : 57\n",
      "Current State : [-0.7860949   0.61810577  3.9878082 ]\n",
      "Currrent Action : tensor([[-0.8104]])\n",
      "Next State : [[-0.7860949 ]\n",
      " [ 0.61810577]\n",
      " [ 3.9878082 ]]\n",
      "Reward : [-6.43212646]\n",
      "--------------------------------------------------\n",
      "Step : 58\n",
      "Current State : [-0.90277433  0.43011454  4.4342227 ]\n",
      "Currrent Action : tensor([[-0.1144]])\n",
      "Next State : [[-0.90277433]\n",
      " [ 0.43011454]\n",
      " [ 4.4342227 ]]\n",
      "Reward : [-7.71719585]\n",
      "--------------------------------------------------\n",
      "Step : 59\n",
      "Current State : [-0.9794069   0.20189638  4.826517  ]\n",
      "Currrent Action : tensor([[0.4647]])\n",
      "Next State : [[-0.9794069 ]\n",
      " [ 0.20189638]\n",
      " [ 4.826517  ]]\n",
      "Reward : [-9.24011227]\n",
      "--------------------------------------------------\n",
      "Step : 60\n",
      "Current State : [-0.9992144  -0.03963084  4.8587003 ]\n",
      "Currrent Action : tensor([[-0.7949]])\n",
      "Next State : [[-0.9992144 ]\n",
      " [-0.03963084]\n",
      " [ 4.8587003 ]]\n",
      "Reward : [-10.96375883]\n",
      "--------------------------------------------------\n",
      "Step : 61\n",
      "Current State : [-0.9622455  -0.27218303  4.7203956 ]\n",
      "Currrent Action : tensor([[-0.7239]])\n",
      "Next State : [[-0.9622455 ]\n",
      " [-0.27218303]\n",
      " [ 4.7203956 ]]\n",
      "Reward : [-11.98332346]\n",
      "--------------------------------------------------\n",
      "Step : 62\n",
      "Current State : [-0.87933344 -0.47620663  4.4134946 ]\n",
      "Currrent Action : tensor([[-0.6851]])\n",
      "Next State : [[-0.87933344]\n",
      " [-0.47620663]\n",
      " [ 4.4134946 ]]\n",
      "Reward : [-10.44224701]\n",
      "--------------------------------------------------\n",
      "Step : 63\n",
      "Current State : [-0.767151   -0.64146656  4.0014534 ]\n",
      "Currrent Action : tensor([[-0.3659]])\n",
      "Next State : [[-0.767151  ]\n",
      " [-0.64146656]\n",
      " [ 4.0014534 ]]\n",
      "Reward : [-8.9454115]\n",
      "--------------------------------------------------\n",
      "Step : 64\n",
      "Current State : [-0.63141006 -0.7754491   3.8203537 ]\n",
      "Currrent Action : tensor([[2.7138]])\n",
      "Next State : [[-0.63141006]\n",
      " [-0.7754491 ]\n",
      " [ 3.8203537 ]]\n",
      "Reward : [-7.58408916]\n",
      "--------------------------------------------------\n",
      "Step : 65\n",
      "Current State : [-0.49455246 -0.8691478   3.3210082 ]\n",
      "Currrent Action : tensor([[0.5483]])\n",
      "Next State : [[-0.49455246]\n",
      " [-0.8691478 ]\n",
      " [ 3.3210082 ]]\n",
      "Reward : [-6.54107779]\n",
      "--------------------------------------------------\n",
      "Step : 66\n",
      "Current State : [-0.3795354 -0.9251772  2.560515 ]\n",
      "Currrent Action : tensor([[-0.7242]])\n",
      "Next State : [[-0.3795354]\n",
      " [-0.9251772]\n",
      " [ 2.560515 ]]\n",
      "Reward : [-5.46366316]\n",
      "--------------------------------------------------\n",
      "Step : 67\n",
      "Current State : [-0.28056362 -0.9598354   2.0982563 ]\n",
      "Currrent Action : tensor([[1.5442]])\n",
      "Next State : [[-0.28056362]\n",
      " [-0.9598354 ]\n",
      " [ 2.0982563 ]]\n",
      "Reward : [-4.49996251]\n",
      "--------------------------------------------------\n",
      "Step : 68\n",
      "Current State : [-0.20794056 -0.9781415   1.4982446 ]\n",
      "Currrent Action : tensor([[0.7991]])\n",
      "Next State : [[-0.20794056]\n",
      " [-0.9781415 ]\n",
      " [ 1.4982446 ]]\n",
      "Reward : [-3.88259044]\n",
      "--------------------------------------------------\n",
      "Step : 69\n",
      "Current State : [-0.1788035 -0.9838848  0.5939761]\n",
      "Currrent Action : tensor([[-1.1377]])\n",
      "Next State : [[-0.1788035]\n",
      " [-0.9838848]\n",
      " [ 0.5939761]]\n",
      "Reward : [-3.39511292]\n",
      "--------------------------------------------------\n",
      "Step : 70\n",
      "Current State : [-0.1926729  -0.98126304 -0.28230238]\n",
      "Currrent Action : tensor([[-0.9224]])\n",
      "Next State : [[-0.1926729 ]\n",
      " [-0.98126304]\n",
      " [-0.28230238]]\n",
      "Reward : [-3.10061491]\n",
      "--------------------------------------------------\n",
      "Step : 71\n",
      "Current State : [-0.23986478 -0.9708063  -0.9668242 ]\n",
      "Currrent Action : tensor([[0.3428]])\n",
      "Next State : [[-0.23986478]\n",
      " [-0.9708063 ]\n",
      " [-0.9668242 ]]\n",
      "Reward : [-3.12218841]\n",
      "--------------------------------------------------\n",
      "Step : 72\n",
      "Current State : [-0.32261515 -0.9465302  -1.7252903 ]\n",
      "Currrent Action : tensor([[-0.2024]])\n",
      "Next State : [[-0.32261515]\n",
      " [-0.9465302 ]\n",
      " [-1.7252903 ]]\n",
      "Reward : [-3.38056785]\n",
      "--------------------------------------------------\n",
      "Step : 73\n",
      "Current State : [-0.44264263 -0.8966981  -2.6010504 ]\n",
      "Currrent Action : tensor([[-1.1057]])\n",
      "Next State : [[-0.44264263]\n",
      " [-0.8966981 ]\n",
      " [-2.6010504 ]]\n",
      "Reward : [-3.90617797]\n",
      "--------------------------------------------------\n",
      "Step : 74\n",
      "Current State : [-0.5786376 -0.8155848 -3.1702724]\n",
      "Currrent Action : tensor([[0.6887]])\n",
      "Next State : [[-0.5786376]\n",
      " [-0.8155848]\n",
      " [-3.1702724]]\n",
      "Reward : [-4.79524111]\n",
      "--------------------------------------------------\n",
      "Step : 75\n",
      "Current State : [-0.7121488 -0.7020286 -3.5099409]\n",
      "Currrent Action : tensor([[1.8135]])\n",
      "Next State : [[-0.7121488]\n",
      " [-0.7020286]\n",
      " [-3.5099409]]\n",
      "Reward : [-5.79505442]\n",
      "--------------------------------------------------\n",
      "Step : 76\n",
      "Current State : [-0.834186  -0.5514832 -3.8820171]\n",
      "Currrent Action : tensor([[1.0296]])\n",
      "Next State : [[-0.834186 ]\n",
      " [-0.5514832]\n",
      " [-3.8820171]]\n",
      "Reward : [-6.8184547]\n",
      "--------------------------------------------------\n",
      "Step : 77\n",
      "Current State : [-0.93188775 -0.36274683 -4.258548  ]\n",
      "Currrent Action : tensor([[0.2472]])\n",
      "Next State : [[-0.93188775]\n",
      " [-0.36274683]\n",
      " [-4.258548  ]]\n",
      "Reward : [-8.04762458]\n",
      "--------------------------------------------------\n",
      "Step : 78\n",
      "Current State : [-0.99001116 -0.14098907 -4.5950685 ]\n",
      "Currrent Action : tensor([[-0.4297]])\n",
      "Next State : [[-0.99001116]\n",
      " [-0.14098907]\n",
      " [-4.5950685 ]]\n",
      "Reward : [-9.48870655]\n",
      "--------------------------------------------------\n",
      "Step : 79\n",
      "Current State : [-0.99651384  0.08342761 -4.4997015 ]\n",
      "Currrent Action : tensor([[1.3407]])\n",
      "Next State : [[-0.99651384]\n",
      " [ 0.08342761]\n",
      " [-4.4997015 ]]\n",
      "Reward : [-11.11405674]\n",
      "--------------------------------------------------\n",
      "Step : 80\n",
      "Current State : [-0.95520884  0.29593247 -4.3381376 ]\n",
      "Currrent Action : tensor([[0.6600]])\n",
      "Next State : [[-0.95520884]\n",
      " [ 0.29593247]\n",
      " [-4.3381376 ]]\n",
      "Reward : [-11.37694669]\n",
      "--------------------------------------------------\n",
      "Step : 81\n",
      "Current State : [-0.875428    0.48334846 -4.080881  ]\n",
      "Currrent Action : tensor([[0.2354]])\n",
      "Next State : [[-0.875428  ]\n",
      " [ 0.48334846]\n",
      " [-4.080881  ]]\n",
      "Reward : [-9.95419541]\n",
      "--------------------------------------------------\n",
      "Step : 82\n",
      "Current State : [-0.77758527  0.62877744 -3.5100875 ]\n",
      "Currrent Action : tensor([[1.3886]])\n",
      "Next State : [[-0.77758527]\n",
      " [ 0.62877744]\n",
      " [-3.5100875 ]]\n",
      "Reward : [-8.62167347]\n",
      "--------------------------------------------------\n",
      "Step : 83\n",
      "Current State : [-0.676391    0.73654276 -2.9592938 ]\n",
      "Currrent Action : tensor([[0.5281]])\n",
      "Next State : [[-0.676391  ]\n",
      " [ 0.73654276]\n",
      " [-2.9592938 ]]\n",
      "Reward : [-7.29188706]\n",
      "--------------------------------------------------\n",
      "Step : 84\n",
      "Current State : [-0.58671284  0.809795   -2.3171601 ]\n",
      "Currrent Action : tensor([[0.5982]])\n",
      "Next State : [[-0.58671284]\n",
      " [ 0.809795  ]\n",
      " [-2.3171601 ]]\n",
      "Reward : [-6.22906672]\n",
      "--------------------------------------------------\n",
      "Step : 85\n",
      "Current State : [-0.51875067  0.8549256  -1.6320934 ]\n",
      "Currrent Action : tensor([[0.5181]])\n",
      "Next State : [[-0.51875067]\n",
      " [ 0.8549256 ]\n",
      " [-1.6320934 ]]\n",
      "Reward : [-5.36747226]\n",
      "--------------------------------------------------\n",
      "Step : 86\n",
      "Current State : [-0.47595805  0.87946796 -0.9867177 ]\n",
      "Currrent Action : tensor([[0.0279]])\n",
      "Next State : [[-0.47595805]\n",
      " [ 0.87946796]\n",
      " [-0.9867177 ]]\n",
      "Reward : [-4.74461383]\n",
      "--------------------------------------------------\n",
      "Step : 87\n",
      "Current State : [-0.4573024   0.88931125 -0.4218722 ]\n",
      "Currrent Action : tensor([[-0.6317]])\n",
      "Next State : [[-0.4573024 ]\n",
      " [ 0.88931125]\n",
      " [-0.4218722 ]]\n",
      "Reward : [-4.36962665]\n",
      "--------------------------------------------------\n",
      "Step : 88\n",
      "Current State : [-0.47235203  0.88141     0.33995768]\n",
      "Currrent Action : tensor([[0.6323]])\n",
      "Next State : [[-0.47235203]\n",
      " [ 0.88141   ]\n",
      " [ 0.33995768]]\n",
      "Reward : [-4.20331416]\n",
      "--------------------------------------------------\n",
      "Step : 89\n",
      "Current State : [-0.5140466  0.8577623  0.958769 ]\n",
      "Currrent Action : tensor([[-0.2816]])\n",
      "Next State : [[-0.5140466]\n",
      " [ 0.8577623]\n",
      " [ 0.958769 ]]\n",
      "Reward : [-4.26658915]\n",
      "--------------------------------------------------\n",
      "Step : 90\n",
      "Current State : [-0.5878802  0.808948   1.7708019]\n",
      "Currrent Action : tensor([[1.1247]])\n",
      "Next State : [[-0.5878802]\n",
      " [ 0.808948 ]\n",
      " [ 1.7708019]]\n",
      "Reward : [-4.54821007]\n",
      "--------------------------------------------------\n",
      "Step : 91\n",
      "Current State : [-0.6810775   0.73221135  2.4159455 ]\n",
      "Currrent Action : tensor([[0.2562]])\n",
      "Next State : [[-0.6810775 ]\n",
      " [ 0.73221135]\n",
      " [ 2.4159455 ]]\n",
      "Reward : [-5.15026195]\n",
      "--------------------------------------------------\n",
      "Step : 92\n",
      "Current State : [-0.7757456  0.6310458  2.7732503]\n",
      "Currrent Action : tensor([[-1.2790]])\n",
      "Next State : [[-0.7757456]\n",
      " [ 0.6310458]\n",
      " [ 2.7732503]]\n",
      "Reward : [-5.96785212]\n",
      "--------------------------------------------------\n",
      "Step : 93\n",
      "Current State : [-0.8613725   0.50797385  3.001394  ]\n",
      "Currrent Action : tensor([[-1.6343]])\n",
      "Next State : [[-0.8613725 ]\n",
      " [ 0.50797385]\n",
      " [ 3.001394  ]]\n",
      "Reward : [-6.81692903]\n",
      "--------------------------------------------------\n",
      "Step : 94\n",
      "Current State : [-0.9316549   0.36334452  3.2195132 ]\n",
      "Currrent Action : tensor([[-1.0857]])\n",
      "Next State : [[-0.9316549 ]\n",
      " [ 0.36334452]\n",
      " [ 3.2195132 ]]\n",
      "Reward : [-7.70765313]\n",
      "--------------------------------------------------\n",
      "Step : 95\n",
      "Current State : [-0.9820918   0.18840303  3.6463897 ]\n",
      "Currrent Action : tensor([[1.0291]])\n",
      "Next State : [[-0.9820918 ]\n",
      " [ 0.18840303]\n",
      " [ 3.6463897 ]]\n",
      "Reward : [-8.70903075]\n",
      "--------------------------------------------------\n",
      "Step : 96\n",
      "Current State : [-0.99998397 -0.00566346  3.9039855 ]\n",
      "Currrent Action : tensor([[0.7753]])\n",
      "Next State : [[-0.99998397]\n",
      " [-0.00566346]\n",
      " [ 3.9039855 ]]\n",
      "Reward : [-10.04485658]\n",
      "--------------------------------------------------\n",
      "Step : 97\n",
      "Current State : [-0.9815054 -0.1914346  3.7392018]\n",
      "Currrent Action : tensor([[-1.0702]])\n",
      "Next State : [[-0.9815054]\n",
      " [-0.1914346]\n",
      " [ 3.7392018]]\n",
      "Reward : [-11.35930748]\n",
      "--------------------------------------------------\n",
      "Step : 98\n",
      "Current State : [-0.9269013 -0.3753052  3.8420491]\n",
      "Currrent Action : tensor([[1.6428]])\n",
      "Next State : [[-0.9269013]\n",
      " [-0.3753052]\n",
      " [ 3.8420491]]\n",
      "Reward : [-10.09728048]\n",
      "--------------------------------------------------\n",
      "Step : 99\n",
      "Current State : [-0.848566   -0.52908957  3.4560256 ]\n",
      "Currrent Action : tensor([[-0.6970]])\n",
      "Next State : [[-0.848566  ]\n",
      " [-0.52908957]\n",
      " [ 3.4560256 ]]\n",
      "Reward : [-9.07693345]\n",
      "learning iteration : 15\n",
      "--------------------------------------------------\n",
      "Step : 0\n",
      "Current State : [-0.38928548 -0.9211172   0.05262079]\n",
      "Currrent Action : tensor([[-0.9031]])\n",
      "Next State : [[-0.38928548]\n",
      " [-0.9211172 ]\n",
      " [ 0.05262079]]\n",
      "Reward : [-3.97177706]\n",
      "--------------------------------------------------\n",
      "Step : 1\n",
      "Current State : [-0.409029   -0.91252136 -0.43067968]\n",
      "Currrent Action : tensor([[1.3836]])\n",
      "Next State : [[-0.409029  ]\n",
      " [-0.91252136]\n",
      " [-0.43067968]]\n",
      "Reward : [-3.88566081]\n",
      "--------------------------------------------------\n",
      "Step : 2\n",
      "Current State : [-0.45816827 -0.8888655  -1.0908726 ]\n",
      "Currrent Action : tensor([[0.1613]])\n",
      "Next State : [[-0.45816827]\n",
      " [-0.8888655 ]\n",
      " [-1.0908726 ]]\n",
      "Reward : [-3.98737982]\n",
      "--------------------------------------------------\n",
      "Step : 3\n",
      "Current State : [-0.52638465 -0.85024655 -1.5681885 ]\n",
      "Currrent Action : tensor([[1.2622]])\n",
      "Next State : [[-0.52638465]\n",
      " [-0.85024655]\n",
      " [-1.5681885 ]]\n",
      "Reward : [-4.30969593]\n",
      "--------------------------------------------------\n",
      "Step : 4\n",
      "Current State : [-0.6192073  -0.78522754 -2.267801  ]\n",
      "Currrent Action : tensor([[-0.4129]])\n",
      "Next State : [[-0.6192073 ]\n",
      " [-0.78522754]\n",
      " [-2.267801  ]]\n",
      "Reward : [-4.76230824]\n",
      "--------------------------------------------------\n",
      "Step : 5\n",
      "Current State : [-0.7294032  -0.68408406 -2.9943254 ]\n",
      "Currrent Action : tensor([[-0.9174]])\n",
      "Next State : [[-0.7294032 ]\n",
      " [-0.68408406]\n",
      " [-2.9943254 ]]\n",
      "Reward : [-5.52614653]\n",
      "--------------------------------------------------\n",
      "Step : 6\n",
      "Current State : [-0.83266187 -0.55378175 -3.3289611 ]\n",
      "Currrent Action : tensor([[1.1895]])\n",
      "Next State : [[-0.83266187]\n",
      " [-0.55378175]\n",
      " [-3.3289611 ]]\n",
      "Reward : [-6.60172961]\n",
      "--------------------------------------------------\n",
      "Step : 7\n",
      "Current State : [-0.91923743 -0.39370367 -3.6448395 ]\n",
      "Currrent Action : tensor([[0.6631]])\n",
      "Next State : [[-0.91923743]\n",
      " [-0.39370367]\n",
      " [-3.6448395 ]]\n",
      "Reward : [-7.63509662]\n",
      "--------------------------------------------------\n",
      "Step : 8\n",
      "Current State : [-0.9809682 -0.1941686 -4.184947 ]\n",
      "Currrent Action : tensor([[-1.6322]])\n",
      "Next State : [[-0.9809682]\n",
      " [-0.1941686]\n",
      " [-4.184947 ]]\n",
      "Reward : [-8.82196535]\n",
      "--------------------------------------------------\n",
      "Step : 9\n",
      "Current State : [-0.9999508   0.00991799 -4.10656   ]\n",
      "Currrent Action : tensor([[1.4934]])\n",
      "Next State : [[-0.9999508 ]\n",
      " [ 0.00991799]\n",
      " [-4.10656   ]]\n",
      "Reward : [-10.43360147]\n",
      "--------------------------------------------------\n",
      "Step : 10\n",
      "Current State : [-0.97782826  0.20940828 -4.021033  ]\n",
      "Currrent Action : tensor([[0.5206]])\n",
      "Next State : [[-0.97782826]\n",
      " [ 0.20940828]\n",
      " [-4.021033  ]]\n",
      "Reward : [-11.49403969]\n",
      "--------------------------------------------------\n",
      "Step : 11\n",
      "Current State : [-0.914326    0.40497896 -4.1197205 ]\n",
      "Currrent Action : tensor([[-1.7050]])\n",
      "Next State : [[-0.914326  ]\n",
      " [ 0.40497896]\n",
      " [-4.1197205 ]]\n",
      "Reward : [-10.2083277]\n",
      "--------------------------------------------------\n",
      "Step : 12\n",
      "Current State : [-0.82541686  0.5645237  -3.6580057 ]\n",
      "Currrent Action : tensor([[1.0532]])\n",
      "Next State : [[-0.82541686]\n",
      " [ 0.5645237 ]\n",
      " [-3.6580057 ]]\n",
      "Reward : [-9.12196487]\n",
      "--------------------------------------------------\n",
      "Step : 13\n",
      "Current State : [-0.72365224  0.6901648  -3.2372134 ]\n",
      "Currrent Action : tensor([[-0.0173]])\n",
      "Next State : [[-0.72365224]\n",
      " [ 0.6901648 ]\n",
      " [-3.2372134 ]]\n",
      "Reward : [-7.79852555]\n",
      "--------------------------------------------------\n",
      "Step : 14\n",
      "Current State : [-0.6316245   0.77527446 -2.5086539 ]\n",
      "Currrent Action : tensor([[1.4062]])\n",
      "Next State : [[-0.6316245 ]\n",
      " [ 0.77527446]\n",
      " [-2.5086539 ]]\n",
      "Reward : [-6.71374168]\n",
      "--------------------------------------------------\n",
      "Step : 15\n",
      "Current State : [-0.556861    0.83060575 -1.8609014 ]\n",
      "Currrent Action : tensor([[0.4420]])\n",
      "Next State : [[-0.556861  ]\n",
      " [ 0.83060575]\n",
      " [-1.8609014 ]]\n",
      "Reward : [-5.71204387]\n",
      "--------------------------------------------------\n",
      "Step : 16\n",
      "Current State : [-0.5086748   0.86095876 -1.139138  ]\n",
      "Currrent Action : tensor([[0.6587]])\n",
      "Next State : [[-0.5086748 ]\n",
      " [ 0.86095876]\n",
      " [-1.139138  ]]\n",
      "Reward : [-5.01837111]\n",
      "--------------------------------------------------\n",
      "Step : 17\n",
      "Current State : [-0.49630603  0.8681477  -0.2861265 ]\n",
      "Currrent Action : tensor([[1.3819]])\n",
      "Next State : [[-0.49630603]\n",
      " [ 0.8681477 ]\n",
      " [-0.2861265 ]]\n",
      "Reward : [-4.56034613]\n",
      "--------------------------------------------------\n",
      "Step : 18\n",
      "Current State : [-0.50968874  0.8603589   0.30968788]\n",
      "Currrent Action : tensor([[-0.3686]])\n",
      "Next State : [[-0.50968874]\n",
      " [ 0.8603589 ]\n",
      " [ 0.30968788]]\n",
      "Reward : [-4.37698657]\n",
      "--------------------------------------------------\n",
      "Step : 19\n",
      "Current State : [-0.5477706   0.83662856  0.8974841 ]\n",
      "Currrent Action : tensor([[-0.3832]])\n",
      "Next State : [[-0.5477706 ]\n",
      " [ 0.83662856]\n",
      " [ 0.8974841 ]]\n",
      "Reward : [-4.44337002]\n",
      "--------------------------------------------------\n",
      "Step : 20\n",
      "Current State : [-0.608611   0.7934687  1.4922346]\n",
      "Currrent Action : tensor([[-0.2181]])\n",
      "Next State : [[-0.608611 ]\n",
      " [ 0.7934687]\n",
      " [ 1.4922346]]\n",
      "Reward : [-4.70521759]\n",
      "--------------------------------------------------\n",
      "Step : 21\n",
      "Current State : [-0.6769977  0.7359851  1.7873361]\n",
      "Currrent Action : tensor([[-2.4288]])\n",
      "Next State : [[-0.6769977]\n",
      " [ 0.7359851]\n",
      " [ 1.7873361]]\n",
      "Reward : [-5.17776961]\n",
      "--------------------------------------------------\n",
      "Step : 22\n",
      "Current State : [-0.76058    0.6492442  2.4106069]\n",
      "Currrent Action : tensor([[0.4752]])\n",
      "Next State : [[-0.76058  ]\n",
      " [ 0.6492442]\n",
      " [ 2.4106069]]\n",
      "Reward : [-5.67646359]\n",
      "--------------------------------------------------\n",
      "Step : 23\n",
      "Current State : [-0.8476519   0.53055274  2.9467554 ]\n",
      "Currrent Action : tensor([[0.3281]])\n",
      "Next State : [[-0.8476519 ]\n",
      " [ 0.53055274]\n",
      " [ 2.9467554 ]]\n",
      "Reward : [-6.51044676]\n",
      "--------------------------------------------------\n",
      "Step : 24\n",
      "Current State : [-0.92407376  0.38221416  3.341226  ]\n",
      "Currrent Action : tensor([[-0.0230]])\n",
      "Next State : [[-0.92407376]\n",
      " [ 0.38221416]\n",
      " [ 3.341226  ]]\n",
      "Reward : [-7.53681782]\n",
      "--------------------------------------------------\n",
      "Step : 25\n",
      "Current State : [-0.97931874  0.20232344  3.7692273 ]\n",
      "Currrent Action : tensor([[0.9423]])\n",
      "Next State : [[-0.97931874]\n",
      " [ 0.20232344]\n",
      " [ 3.7692273 ]]\n",
      "Reward : [-8.67647538]\n",
      "--------------------------------------------------\n",
      "Step : 26\n",
      "Current State : [-0.9999707   0.00765727  3.9214501 ]\n",
      "Currrent Action : tensor([[0.0032]])\n",
      "Next State : [[-0.9999707 ]\n",
      " [ 0.00765727]\n",
      " [ 3.9214501 ]]\n",
      "Reward : [-10.05174523]\n",
      "--------------------------------------------------\n",
      "Step : 27\n",
      "Current State : [-0.9813025  -0.19247173  4.026754  ]\n",
      "Currrent Action : tensor([[0.6637]])\n",
      "Next State : [[-0.9813025 ]\n",
      " [-0.19247173]\n",
      " [ 4.026754  ]]\n",
      "Reward : [-11.35976807]\n",
      "--------------------------------------------------\n",
      "Step : 28\n",
      "Current State : [-0.92910683 -0.36981142  3.7025137 ]\n",
      "Currrent Action : tensor([[-1.1992]])\n",
      "Next State : [[-0.92910683]\n",
      " [-0.36981142]\n",
      " [ 3.7025137 ]]\n",
      "Reward : [-10.31309986]\n",
      "--------------------------------------------------\n",
      "Step : 29\n",
      "Current State : [-0.85386866 -0.52048856  3.3723392 ]\n",
      "Currrent Action : tensor([[-0.3521]])\n",
      "Next State : [[-0.85386866]\n",
      " [-0.52048856]\n",
      " [ 3.3723392 ]]\n",
      "Reward : [-9.00397466]\n",
      "--------------------------------------------------\n",
      "Step : 30\n",
      "Current State : [-0.75736916 -0.65298694  3.281973  ]\n",
      "Currrent Action : tensor([[2.1191]])\n",
      "Next State : [[-0.75736916]\n",
      " [-0.65298694]\n",
      " [ 3.281973  ]]\n",
      "Reward : [-7.87098346]\n",
      "--------------------------------------------------\n",
      "Step : 31\n",
      "Current State : [-0.6599583  -0.75130224  2.770229  ]\n",
      "Currrent Action : tensor([[-0.1467]])\n",
      "Next State : [[-0.6599583 ]\n",
      " [-0.75130224]\n",
      " [ 2.770229  ]]\n",
      "Reward : [-6.98240127]\n",
      "--------------------------------------------------\n",
      "Step : 32\n",
      "Current State : [-0.5608612 -0.8279099  2.5067525]\n",
      "Currrent Action : tensor([[3.0187]])\n",
      "Next State : [[-0.5608612]\n",
      " [-0.8279099]\n",
      " [ 2.5067525]]\n",
      "Reward : [-6.02266214]\n",
      "--------------------------------------------------\n",
      "Step : 33\n",
      "Current State : [-0.48191097 -0.87622017  1.8518254 ]\n",
      "Currrent Action : tensor([[-0.2266]])\n",
      "Next State : [[-0.48191097]\n",
      " [-0.87622017]\n",
      " [ 1.8518254 ]]\n",
      "Reward : [-5.32094961]\n",
      "--------------------------------------------------\n",
      "Step : 34\n",
      "Current State : [-0.42717642 -0.9041683   1.2293346 ]\n",
      "Currrent Action : tensor([[0.2312]])\n",
      "Next State : [[-0.42717642]\n",
      " [-0.9041683 ]\n",
      " [ 1.2293346 ]]\n",
      "Reward : [-4.64292329]\n",
      "--------------------------------------------------\n",
      "Step : 35\n",
      "Current State : [-0.4015235  -0.91584873  0.56375754]\n",
      "Currrent Action : tensor([[0.0837]])\n",
      "Next State : [[-0.4015235 ]\n",
      " [-0.91584873]\n",
      " [ 0.56375754]]\n",
      "Reward : [-4.19993708]\n",
      "--------------------------------------------------\n",
      "Step : 36\n",
      "Current State : [-0.39471844 -0.91880214  0.14836727]\n",
      "Currrent Action : tensor([[1.8100]])\n",
      "Next State : [[-0.39471844]\n",
      " [-0.91880214]\n",
      " [ 0.14836727]]\n",
      "Reward : [-3.97121928]\n",
      "--------------------------------------------------\n",
      "Step : 37\n",
      "Current State : [-0.41560465 -0.90954536 -0.4569222 ]\n",
      "Currrent Action : tensor([[0.5587]])\n",
      "Next State : [[-0.41560465]\n",
      " [-0.90954536]\n",
      " [-0.4569222 ]]\n",
      "Reward : [-3.90929381]\n",
      "--------------------------------------------------\n",
      "Step : 38\n",
      "Current State : [-0.47067806 -0.882305   -1.2290334 ]\n",
      "Currrent Action : tensor([[-0.5997]])\n",
      "Next State : [[-0.47067806]\n",
      " [-0.882305  ]\n",
      " [-1.2290334 ]]\n",
      "Reward : [-4.01885299]\n",
      "--------------------------------------------------\n",
      "Step : 39\n",
      "Current State : [-0.53929293 -0.84211826 -1.5907621 ]\n",
      "Currrent Action : tensor([[3.1087]])\n",
      "Next State : [[-0.53929293]\n",
      " [-0.84211826]\n",
      " [-1.5907621 ]]\n",
      "Reward : [-4.40217758]\n",
      "--------------------------------------------------\n",
      "Step : 40\n",
      "Current State : [-0.62634885 -0.77954286 -2.1452684 ]\n",
      "Currrent Action : tensor([[0.5139]])\n",
      "Next State : [[-0.62634885]\n",
      " [-0.77954286]\n",
      " [-2.1452684 ]]\n",
      "Reward : [-4.83460115]\n",
      "--------------------------------------------------\n",
      "Step : 41\n",
      "Current State : [-0.7223694 -0.6915074 -2.6072388]\n",
      "Currrent Action : tensor([[0.8179]])\n",
      "Next State : [[-0.7223694]\n",
      " [-0.6915074]\n",
      " [-2.6072388]]\n",
      "Reward : [-5.51284859]\n",
      "--------------------------------------------------\n",
      "Step : 42\n",
      "Current State : [-0.82115436 -0.5707062  -3.1241624 ]\n",
      "Currrent Action : tensor([[0.0114]])\n",
      "Next State : [[-0.82115436]\n",
      " [-0.5707062 ]\n",
      " [-3.1241624 ]]\n",
      "Reward : [-6.33474362]\n",
      "--------------------------------------------------\n",
      "Step : 43\n",
      "Current State : [-0.90949816 -0.41570792 -3.5728917 ]\n",
      "Currrent Action : tensor([[-0.1380]])\n",
      "Next State : [[-0.90949816]\n",
      " [-0.41570792]\n",
      " [-3.5728917 ]]\n",
      "Reward : [-7.39836489]\n",
      "--------------------------------------------------\n",
      "Step : 44\n",
      "Current State : [-0.97077906 -0.23997498 -3.7276187 ]\n",
      "Currrent Action : tensor([[1.0470]])\n",
      "Next State : [[-0.97077906]\n",
      " [-0.23997498]\n",
      " [-3.7276187 ]]\n",
      "Reward : [-8.63732437]\n",
      "--------------------------------------------------\n",
      "Step : 45\n",
      "Current State : [-0.9987456  -0.05007181 -3.8449464 ]\n",
      "Currrent Action : tensor([[0.4177]])\n",
      "Next State : [[-0.9987456 ]\n",
      " [-0.05007181]\n",
      " [-3.8449464 ]]\n",
      "Reward : [-9.79535416]\n",
      "--------------------------------------------------\n",
      "Step : 46\n",
      "Current State : [-0.99035424  0.13855839 -3.7819674 ]\n",
      "Currrent Action : tensor([[0.6702]])\n",
      "Next State : [[-0.99035424]\n",
      " [ 0.13855839]\n",
      " [-3.7819674 ]]\n",
      "Reward : [-11.03618212]\n",
      "--------------------------------------------------\n",
      "Step : 47\n",
      "Current State : [-0.94883335  0.31577724 -3.6454017 ]\n",
      "Currrent Action : tensor([[0.2176]])\n",
      "Next State : [[-0.94883335]\n",
      " [ 0.31577724]\n",
      " [-3.6454017 ]]\n",
      "Reward : [-10.44590402]\n",
      "--------------------------------------------------\n",
      "Step : 48\n",
      "Current State : [-0.87764734  0.4793069  -3.571782  ]\n",
      "Currrent Action : tensor([[-1.0881]])\n",
      "Next State : [[-0.87764734]\n",
      " [ 0.4793069 ]\n",
      " [-3.571782  ]]\n",
      "Reward : [-9.28426698]\n",
      "--------------------------------------------------\n",
      "Step : 49\n",
      "Current State : [-0.7893108  0.6139938 -3.2249155]\n",
      "Currrent Action : tensor([[-0.0841]])\n",
      "Next State : [[-0.7893108]\n",
      " [ 0.6139938]\n",
      " [-3.2249155]]\n",
      "Reward : [-8.25449586]\n",
      "--------------------------------------------------\n",
      "Step : 50\n",
      "Current State : [-0.6948087  0.7191946 -2.8306367]\n",
      "Currrent Action : tensor([[-0.4414]])\n",
      "Next State : [[-0.6948087]\n",
      " [ 0.7191946]\n",
      " [-2.8306367]]\n",
      "Reward : [-7.19299414]\n",
      "--------------------------------------------------\n",
      "Step : 51\n",
      "Current State : [-0.6113415   0.79136693 -2.2079859 ]\n",
      "Currrent Action : tensor([[0.5550]])\n",
      "Next State : [[-0.6113415 ]\n",
      " [ 0.79136693]\n",
      " [-2.2079859 ]]\n",
      "Reward : [-6.27224667]\n",
      "--------------------------------------------------\n",
      "Step : 52\n",
      "Current State : [-0.54586804  0.8378712  -1.6065972 ]\n",
      "Currrent Action : tensor([[0.0524]])\n",
      "Next State : [[-0.54586804]\n",
      " [ 0.8378712 ]\n",
      " [-1.6065972 ]]\n",
      "Reward : [-5.45396217]\n",
      "--------------------------------------------------\n",
      "Step : 53\n",
      "Current State : [-0.4954682   0.86862606 -1.1810194 ]\n",
      "Currrent Action : tensor([[-1.3522]])\n",
      "Next State : [[-0.4954682 ]\n",
      " [ 0.86862606]\n",
      " [-1.1810194 ]]\n",
      "Reward : [-4.87479757]\n",
      "--------------------------------------------------\n",
      "Step : 54\n",
      "Current State : [-0.46987304  0.882734   -0.5845363 ]\n",
      "Currrent Action : tensor([[-0.3666]])\n",
      "Next State : [[-0.46987304]\n",
      " [ 0.882734  ]\n",
      " [-0.5845363 ]]\n",
      "Reward : [-4.50424676]\n",
      "--------------------------------------------------\n",
      "Step : 55\n",
      "Current State : [-0.47994736  0.8772973   0.22895533]\n",
      "Currrent Action : tensor([[1.0096]])\n",
      "Next State : [[-0.47994736]\n",
      " [ 0.8772973 ]\n",
      " [ 0.22895533]]\n",
      "Reward : [-4.2785539]\n",
      "--------------------------------------------------\n",
      "Step : 56\n",
      "Current State : [-0.51930803  0.85458714  0.90892655]\n",
      "Currrent Action : tensor([[0.1467]])\n",
      "Next State : [[-0.51930803]\n",
      " [ 0.85458714]\n",
      " [ 0.90892655]]\n",
      "Reward : [-4.29592443]\n",
      "--------------------------------------------------\n",
      "Step : 57\n",
      "Current State : [-0.5729523  0.8195887  1.2812489]\n",
      "Currrent Action : tensor([[-1.7908]])\n",
      "Next State : [[-0.5729523]\n",
      " [ 0.8195887]\n",
      " [ 1.2812489]]\n",
      "Reward : [-4.56682214]\n",
      "--------------------------------------------------\n",
      "Step : 58\n",
      "Current State : [-0.64005345  0.7683304   1.6892866 ]\n",
      "Currrent Action : tensor([[-1.3777]])\n",
      "Next State : [[-0.64005345]\n",
      " [ 0.7683304 ]\n",
      " [ 1.6892866 ]]\n",
      "Reward : [-4.92238196]\n",
      "--------------------------------------------------\n",
      "Step : 59\n",
      "Current State : [-0.7194968   0.69449574  2.1701932 ]\n",
      "Currrent Action : tensor([[-0.6356]])\n",
      "Next State : [[-0.7194968 ]\n",
      " [ 0.69449574]\n",
      " [ 2.1701932 ]]\n",
      "Reward : [-5.41764765]\n",
      "--------------------------------------------------\n",
      "Step : 60\n",
      "Current State : [-0.81494296  0.5795412   2.991065  ]\n",
      "Currrent Action : tensor([[2.0127]])\n",
      "Next State : [[-0.81494296]\n",
      " [ 0.5795412 ]\n",
      " [ 2.991065  ]]\n",
      "Reward : [-6.11025065]\n",
      "--------------------------------------------------\n",
      "Step : 61\n",
      "Current State : [-0.9001059   0.43567118  3.3476357 ]\n",
      "Currrent Action : tensor([[-0.5206]])\n",
      "Next State : [[-0.9001059 ]\n",
      " [ 0.43567118]\n",
      " [ 3.3476357 ]]\n",
      "Reward : [-7.26260201]\n",
      "--------------------------------------------------\n",
      "Step : 62\n",
      "Current State : [-0.9631066  0.2691204  3.5660841]\n",
      "Currrent Action : tensor([[-0.7220]])\n",
      "Next State : [[-0.9631066]\n",
      " [ 0.2691204]\n",
      " [ 3.5660841]]\n",
      "Reward : [-8.36164]\n",
      "--------------------------------------------------\n",
      "Step : 63\n",
      "Current State : [-0.99642205  0.08451696  3.757234  ]\n",
      "Currrent Action : tensor([[-0.0713]])\n",
      "Next State : [[-0.99642205]\n",
      " [ 0.08451696]\n",
      " [ 3.757234  ]]\n",
      "Reward : [-9.50351039]\n",
      "--------------------------------------------------\n",
      "Step : 64\n",
      "Current State : [-0.9945336 -0.104417   3.7845118]\n",
      "Currrent Action : tensor([[-0.2407]])\n",
      "Next State : [[-0.9945336]\n",
      " [-0.104417 ]\n",
      " [ 3.7845118]]\n",
      "Reward : [-10.75683327]\n",
      "--------------------------------------------------\n",
      "Step : 65\n",
      "Current State : [-0.95710385 -0.28974515  3.7870579 ]\n",
      "Currrent Action : tensor([[0.5391]])\n",
      "Next State : [[-0.95710385]\n",
      " [-0.28974515]\n",
      " [ 3.7870579 ]]\n",
      "Reward : [-10.65582125]\n",
      "--------------------------------------------------\n",
      "Step : 66\n",
      "Current State : [-0.8904188  -0.45514208  3.5714238 ]\n",
      "Currrent Action : tensor([[0.0112]])\n",
      "Next State : [[-0.8904188 ]\n",
      " [-0.45514208]\n",
      " [ 3.5714238 ]]\n",
      "Reward : [-9.54318929]\n",
      "--------------------------------------------------\n",
      "Step : 67\n",
      "Current State : [-0.7993941 -0.600807   3.4395707]\n",
      "Currrent Action : tensor([[1.3967]])\n",
      "Next State : [[-0.7993941]\n",
      " [-0.600807 ]\n",
      " [ 3.4395707]]\n",
      "Reward : [-8.4013436]\n",
      "--------------------------------------------------\n",
      "Step : 68\n",
      "Current State : [-0.69991255 -0.7142286   3.0202203 ]\n",
      "Currrent Action : tensor([[0.2084]])\n",
      "Next State : [[-0.69991255]\n",
      " [-0.7142286 ]\n",
      " [ 3.0202203 ]]\n",
      "Reward : [-7.41852832]\n",
      "--------------------------------------------------\n",
      "Step : 69\n",
      "Current State : [-0.6121927 -0.7907086  2.3288863]\n",
      "Currrent Action : tensor([[-1.0378]])\n",
      "Next State : [[-0.6121927]\n",
      " [-0.7907086]\n",
      " [ 2.3288863]]\n",
      "Reward : [-6.41730078]\n",
      "--------------------------------------------------\n",
      "Step : 70\n",
      "Current State : [-0.5439995 -0.8390856  1.6726887]\n",
      "Currrent Action : tensor([[-0.4211]])\n",
      "Next State : [[-0.5439995]\n",
      " [-0.8390856]\n",
      " [ 1.6726887]]\n",
      "Reward : [-5.51378519]\n",
      "--------------------------------------------------\n",
      "Step : 71\n",
      "Current State : [-0.5124432  -0.85872114  0.7433746 ]\n",
      "Currrent Action : tensor([[-2.0781]])\n",
      "Next State : [[-0.5124432 ]\n",
      " [-0.85872114]\n",
      " [ 0.7433746 ]]\n",
      "Reward : [-4.88907295]\n",
      "--------------------------------------------------\n",
      "Step : 72\n",
      "Current State : [-0.49961147 -0.8662496   0.29754657]\n",
      "Currrent Action : tensor([[1.3214]])\n",
      "Next State : [[-0.49961147]\n",
      " [-0.8662496 ]\n",
      " [ 0.29754657]]\n",
      "Reward : [-4.5041448]\n",
      "--------------------------------------------------\n",
      "Step : 73\n",
      "Current State : [-0.509893   -0.86023784 -0.23820448]\n",
      "Currrent Action : tensor([[0.7596]])\n",
      "Next State : [[-0.509893  ]\n",
      " [-0.86023784]\n",
      " [-0.23820448]]\n",
      "Reward : [-4.39404243]\n",
      "--------------------------------------------------\n",
      "Step : 74\n",
      "Current State : [-0.53932536 -0.84209746 -0.6915062 ]\n",
      "Currrent Action : tensor([[1.2792]])\n",
      "Next State : [[-0.53932536]\n",
      " [-0.84209746]\n",
      " [-0.6915062 ]]\n",
      "Reward : [-4.44194311]\n",
      "--------------------------------------------------\n",
      "Step : 75\n",
      "Current State : [-0.59153736 -0.80627763 -1.2665694 ]\n",
      "Currrent Action : tensor([[0.3767]])\n",
      "Next State : [[-0.59153736]\n",
      " [-0.80627763]\n",
      " [-1.2665694 ]]\n",
      "Reward : [-4.62940945]\n",
      "--------------------------------------------------\n",
      "Step : 76\n",
      "Current State : [-0.6608112 -0.7505522 -1.7786967]\n",
      "Currrent Action : tensor([[0.6172]])\n",
      "Next State : [[-0.6608112]\n",
      " [-0.7505522]\n",
      " [-1.7786967]]\n",
      "Reward : [-5.01736122]\n",
      "--------------------------------------------------\n",
      "Step : 77\n",
      "Current State : [-0.7435969  -0.66862816 -2.3306978 ]\n",
      "Currrent Action : tensor([[0.0728]])\n",
      "Next State : [[-0.7435969 ]\n",
      " [-0.66862816]\n",
      " [-2.3306978 ]]\n",
      "Reward : [-5.57283352]\n",
      "--------------------------------------------------\n",
      "Step : 78\n",
      "Current State : [-0.82207304 -0.569382   -2.5321689 ]\n",
      "Currrent Action : tensor([[2.4246]])\n",
      "Next State : [[-0.82207304]\n",
      " [-0.569382  ]\n",
      " [-2.5321689 ]]\n",
      "Reward : [-6.35160565]\n",
      "--------------------------------------------------\n",
      "Step : 79\n",
      "Current State : [-0.8984328 -0.4391111 -3.022898 ]\n",
      "Currrent Action : tensor([[-0.4246]])\n",
      "Next State : [[-0.8984328]\n",
      " [-0.4391111]\n",
      " [-3.022898 ]]\n",
      "Reward : [-7.07184616]\n",
      "--------------------------------------------------\n",
      "Step : 80\n",
      "Current State : [-0.956727   -0.29098696 -3.1870155 ]\n",
      "Currrent Action : tensor([[1.1014]])\n",
      "Next State : [[-0.956727  ]\n",
      " [-0.29098696]\n",
      " [-3.1870155 ]]\n",
      "Reward : [-8.13488514]\n",
      "--------------------------------------------------\n",
      "Step : 81\n",
      "Current State : [-0.992141  -0.1251246 -3.3960972]\n",
      "Currrent Action : tensor([[0.0611]])\n",
      "Next State : [[-0.992141 ]\n",
      " [-0.1251246]\n",
      " [-3.3960972]]\n",
      "Reward : [-9.11732983]\n",
      "--------------------------------------------------\n",
      "Step : 82\n",
      "Current State : [-0.9987512   0.04995934 -3.5086715 ]\n",
      "Currrent Action : tensor([[-0.1249]])\n",
      "Next State : [[-0.9987512 ]\n",
      " [ 0.04995934]\n",
      " [-3.5086715 ]]\n",
      "Reward : [-10.25045902]\n",
      "--------------------------------------------------\n",
      "Step : 83\n",
      "Current State : [-0.9756232   0.21945247 -3.4254618 ]\n",
      "Currrent Action : tensor([[0.3049]])\n",
      "Next State : [[-0.9756232 ]\n",
      " [ 0.21945247]\n",
      " [-3.4254618 ]]\n",
      "Reward : [-10.78923844]\n",
      "--------------------------------------------------\n",
      "Step : 84\n",
      "Current State : [-0.9282598   0.37193248 -3.196735  ]\n",
      "Currrent Action : tensor([[0.4276]])\n",
      "Next State : [[-0.9282598 ]\n",
      " [ 0.37193248]\n",
      " [-3.196735  ]]\n",
      "Reward : [-9.70194395]\n",
      "--------------------------------------------------\n",
      "Step : 85\n",
      "Current State : [-0.86216867  0.50662136 -3.0034306 ]\n",
      "Currrent Action : tensor([[-0.5710]])\n",
      "Next State : [[-0.86216867]\n",
      " [ 0.50662136]\n",
      " [-3.0034306 ]]\n",
      "Reward : [-8.64261244]\n",
      "--------------------------------------------------\n",
      "Step : 86\n",
      "Current State : [-0.78749025  0.61632717 -2.6561728 ]\n",
      "Currrent Action : tensor([[-0.2181]])\n",
      "Next State : [[-0.78749025]\n",
      " [ 0.61632717]\n",
      " [-2.6561728 ]]\n",
      "Reward : [-7.7159358]\n",
      "--------------------------------------------------\n",
      "Step : 87\n",
      "Current State : [-0.7160419   0.69805723 -2.1722114 ]\n",
      "Currrent Action : tensor([[0.1448]])\n",
      "Next State : [[-0.7160419 ]\n",
      " [ 0.69805723]\n",
      " [-2.1722114 ]]\n",
      "Reward : [-6.84366405]\n",
      "--------------------------------------------------\n",
      "Step : 88\n",
      "Current State : [-0.6673778  0.7447193 -1.3486686]\n",
      "Currrent Action : tensor([[2.2061]])\n",
      "Next State : [[-0.6673778]\n",
      " [ 0.7447193]\n",
      " [-1.3486686]]\n",
      "Reward : [-6.08759402]\n",
      "--------------------------------------------------\n",
      "Step : 89\n",
      "Current State : [-0.6340337  0.7733054 -0.8784774]\n",
      "Currrent Action : tensor([[-0.5890]])\n",
      "Next State : [[-0.6340337]\n",
      " [ 0.7733054]\n",
      " [-0.8784774]]\n",
      "Reward : [-5.47904091]\n",
      "--------------------------------------------------\n",
      "Step : 90\n",
      "Current State : [-0.62249887  0.78262067 -0.2965333 ]\n",
      "Currrent Action : tensor([[0.0131]])\n",
      "Next State : [[-0.62249887]\n",
      " [ 0.78262067]\n",
      " [-0.2965333 ]]\n",
      "Reward : [-5.17372536]\n",
      "--------------------------------------------------\n",
      "Step : 91\n",
      "Current State : [-0.64101505  0.7675283   0.47776803]\n",
      "Currrent Action : tensor([[1.2489]])\n",
      "Next State : [[-0.64101505]\n",
      " [ 0.7675283 ]\n",
      " [ 0.47776803]]\n",
      "Reward : [-5.04018172]\n",
      "--------------------------------------------------\n",
      "Step : 92\n",
      "Current State : [-0.6804834   0.73276347  1.0520418 ]\n",
      "Currrent Action : tensor([[-0.0091]])\n",
      "Next State : [[-0.6804834 ]\n",
      " [ 0.73276347]\n",
      " [ 1.0520418 ]]\n",
      "Reward : [-5.1603761]\n",
      "--------------------------------------------------\n",
      "Step : 93\n",
      "Current State : [-0.7419309  0.6704764  1.750471 ]\n",
      "Currrent Action : tensor([[0.9924]])\n",
      "Next State : [[-0.7419309]\n",
      " [ 0.6704764]\n",
      " [ 1.750471 ]]\n",
      "Reward : [-5.49043827]\n",
      "--------------------------------------------------\n",
      "Step : 94\n",
      "Current State : [-0.8068505   0.59075564  2.0571115 ]\n",
      "Currrent Action : tensor([[-1.3081]])\n",
      "Next State : [[-0.8068505 ]\n",
      " [ 0.59075564]\n",
      " [ 2.0571115 ]]\n",
      "Reward : [-6.10053311]\n",
      "--------------------------------------------------\n",
      "Step : 95\n",
      "Current State : [-0.8729962  0.487727   2.450218 ]\n",
      "Currrent Action : tensor([[-0.3331]])\n",
      "Next State : [[-0.8729962]\n",
      " [ 0.487727 ]\n",
      " [ 2.450218 ]]\n",
      "Reward : [-6.72136174]\n",
      "--------------------------------------------------\n",
      "Step : 96\n",
      "Current State : [-0.933542   0.358468   2.8571572]\n",
      "Currrent Action : tensor([[0.2743]])\n",
      "Next State : [[-0.933542 ]\n",
      " [ 0.358468 ]\n",
      " [ 2.8571572]]\n",
      "Reward : [-7.52842704]\n",
      "--------------------------------------------------\n",
      "Step : 97\n",
      "Current State : [-0.97556424  0.21971428  2.9020946 ]\n",
      "Currrent Action : tensor([[-1.4928]])\n",
      "Next State : [[-0.97556424]\n",
      " [ 0.21971428]\n",
      " [ 2.9020946 ]]\n",
      "Reward : [-8.51900131]\n",
      "--------------------------------------------------\n",
      "Step : 98\n",
      "Current State : [-0.99654275  0.08308169  2.7668803 ]\n",
      "Currrent Action : tensor([[-2.0535]])\n",
      "Next State : [[-0.99654275]\n",
      " [ 0.08308169]\n",
      " [ 2.7668803 ]]\n",
      "Reward : [-9.37303035]\n",
      "--------------------------------------------------\n",
      "Step : 99\n",
      "Current State : [-0.9980095  -0.06306415  2.925672  ]\n",
      "Currrent Action : tensor([[0.6432]])\n",
      "Next State : [[-0.9980095 ]\n",
      " [-0.06306415]\n",
      " [ 2.925672  ]]\n",
      "Reward : [-10.11987919]\n",
      "learning iteration : 16\n",
      "--------------------------------------------------\n",
      "Step : 0\n",
      "Current State : [0.9938405  0.11081976 0.37302274]\n",
      "Currrent Action : tensor([[-1.3049]])\n",
      "Next State : [[0.9938405 ]\n",
      " [0.11081976]\n",
      " [0.37302274]]\n",
      "Reward : [-0.03519545]\n",
      "--------------------------------------------------\n",
      "Step : 1\n",
      "Current State : [0.9919104  0.12693982 0.32470748]\n",
      "Currrent Action : tensor([[-0.8762]])\n",
      "Next State : [[0.9919104 ]\n",
      " [0.12693982]\n",
      " [0.32470748]]\n",
      "Reward : [-0.02701395]\n",
      "--------------------------------------------------\n",
      "Step : 2\n",
      "Current State : [0.98928386 0.14600486 0.38490823]\n",
      "Currrent Action : tensor([[-0.2334]])\n",
      "Next State : [[0.98928386]\n",
      " [0.14600486]\n",
      " [0.38490823]]\n",
      "Reward : [-0.02679897]\n",
      "--------------------------------------------------\n",
      "Step : 3\n",
      "Current State : [0.9864443  0.16409646 0.3662671 ]\n",
      "Currrent Action : tensor([[-0.8543]])\n",
      "Next State : [[0.9864443 ]\n",
      " [0.16409646]\n",
      " [0.3662671 ]]\n",
      "Reward : [-0.0370159]\n",
      "--------------------------------------------------\n",
      "Step : 4\n",
      "Current State : [0.98164916 0.19069596 0.54058146]\n",
      "Currrent Action : tensor([[0.3416]])\n",
      "Next State : [[0.98164916]\n",
      " [0.19069596]\n",
      " [0.54058146]]\n",
      "Reward : [-0.04070474]\n",
      "--------------------------------------------------\n",
      "Step : 5\n",
      "Current State : [0.97251683 0.23283246 0.8623622 ]\n",
      "Currrent Action : tensor([[1.1917]])\n",
      "Next State : [[0.97251683]\n",
      " [0.23283246]\n",
      " [0.8623622 ]]\n",
      "Reward : [-0.06745755]\n",
      "--------------------------------------------------\n",
      "Step : 6\n",
      "Current State : [0.95994633 0.28018403 0.9799327 ]\n",
      "Currrent Action : tensor([[-0.3804]])\n",
      "Next State : [[0.95994633]\n",
      " [0.28018403]\n",
      " [0.9799327 ]]\n",
      "Reward : [-0.12973144]\n",
      "--------------------------------------------------\n",
      "Step : 7\n",
      "Current State : [0.94205153 0.3354681  1.1623259 ]\n",
      "Currrent Action : tensor([[-0.1850]])\n",
      "Next State : [[0.94205153]\n",
      " [0.3354681 ]\n",
      " [1.1623259 ]]\n",
      "Reward : [-0.17670897]\n",
      "--------------------------------------------------\n",
      "Step : 8\n",
      "Current State : [0.9170852 0.3986914 1.3597475]\n",
      "Currrent Action : tensor([[-0.3612]])\n",
      "Next State : [[0.9170852]\n",
      " [0.3986914]\n",
      " [1.3597475]]\n",
      "Reward : [-0.25226444]\n",
      "--------------------------------------------------\n",
      "Step : 9\n",
      "Current State : [0.8812268  0.47269374 1.6451119 ]\n",
      "Currrent Action : tensor([[-0.0910]])\n",
      "Next State : [[0.8812268 ]\n",
      " [0.47269374]\n",
      " [1.6451119 ]]\n",
      "Reward : [-0.35307301]\n",
      "--------------------------------------------------\n",
      "Step : 10\n",
      "Current State : [0.8330949 0.5531301 1.8754343]\n",
      "Currrent Action : tensor([[-0.8280]])\n",
      "Next State : [[0.8330949]\n",
      " [0.5531301]\n",
      " [1.8754343]]\n",
      "Reward : [-0.51372855]\n",
      "--------------------------------------------------\n",
      "Step : 11\n",
      "Current State : [0.76550967 0.6434244  2.2569294 ]\n",
      "Currrent Action : tensor([[-0.2223]])\n",
      "Next State : [[0.76550967]\n",
      " [0.6434244 ]\n",
      " [2.2569294 ]]\n",
      "Reward : [-0.69530771]\n",
      "--------------------------------------------------\n",
      "Step : 12\n",
      "Current State : [0.67221683 0.74035436 2.6926813 ]\n",
      "Currrent Action : tensor([[-0.3121]])\n",
      "Next State : [[0.67221683]\n",
      " [0.74035436]\n",
      " [2.6926813 ]]\n",
      "Reward : [-0.99802012]\n",
      "--------------------------------------------------\n",
      "Step : 13\n",
      "Current State : [0.5425194 0.8400433 3.2753136]\n",
      "Currrent Action : tensor([[0.1824]])\n",
      "Next State : [[0.5425194]\n",
      " [0.8400433]\n",
      " [3.2753136]]\n",
      "Reward : [-1.41997105]\n",
      "--------------------------------------------------\n",
      "Step : 14\n",
      "Current State : [0.37047562 0.9288422  3.8782458 ]\n",
      "Currrent Action : tensor([[-0.1807]])\n",
      "Next State : [[0.37047562]\n",
      " [0.9288422 ]\n",
      " [3.8782458 ]]\n",
      "Reward : [-2.06753351]\n",
      "--------------------------------------------------\n",
      "Step : 15\n",
      "Current State : [0.16203785 0.9867845  4.3353114 ]\n",
      "Currrent Action : tensor([[-1.5971]])\n",
      "Next State : [[0.16203785]\n",
      " [0.9867845 ]\n",
      " [4.3353114 ]]\n",
      "Reward : [-2.92576676]\n",
      "--------------------------------------------------\n",
      "Step : 16\n",
      "Current State : [-0.08088738  0.99672323  4.8746257 ]\n",
      "Currrent Action : tensor([[-1.3385]])\n",
      "Next State : [[-0.08088738]\n",
      " [ 0.99672323]\n",
      " [ 4.8746257 ]]\n",
      "Reward : [-3.86386339]\n",
      "--------------------------------------------------\n",
      "Step : 17\n",
      "Current State : [-0.35045236  0.93658054  5.5415645 ]\n",
      "Currrent Action : tensor([[-0.5374]])\n",
      "Next State : [[-0.35045236]\n",
      " [ 0.93658054]\n",
      " [ 5.5415645 ]]\n",
      "Reward : [-5.10483768]\n",
      "--------------------------------------------------\n",
      "Step : 18\n",
      "Current State : [-0.6097334  0.7926066  5.953403 ]\n",
      "Currrent Action : tensor([[-1.9373]])\n",
      "Next State : [[-0.6097334]\n",
      " [ 0.7926066]\n",
      " [ 5.953403 ]]\n",
      "Reward : [-6.79511043]\n",
      "--------------------------------------------------\n",
      "Step : 19\n",
      "Current State : [-0.8367817   0.54753673  6.71309   ]\n",
      "Currrent Action : tensor([[1.1015]])\n",
      "Next State : [[-0.8367817 ]\n",
      " [ 0.54753673]\n",
      " [ 6.71309   ]]\n",
      "Reward : [-8.50290776]\n",
      "--------------------------------------------------\n",
      "Step : 20\n",
      "Current State : [-0.9751478   0.22155552  7.120166  ]\n",
      "Currrent Action : tensor([[-0.0238]])\n",
      "Next State : [[-0.9751478 ]\n",
      " [ 0.22155552]\n",
      " [ 7.120166  ]]\n",
      "Reward : [-11.07129874]\n",
      "--------------------------------------------------\n",
      "Step : 21\n",
      "Current State : [-0.9897248 -0.1429852  7.3377256]\n",
      "Currrent Action : tensor([[0.3426]])\n",
      "Next State : [[-0.9897248]\n",
      " [-0.1429852]\n",
      " [ 7.3377256]]\n",
      "Reward : [-13.58558736]\n",
      "--------------------------------------------------\n",
      "Step : 22\n",
      "Current State : [-0.8747778  -0.48452425  7.2468457 ]\n",
      "Currrent Action : tensor([[0.1091]])\n",
      "Next State : [[-0.8747778 ]\n",
      " [-0.48452425]\n",
      " [ 7.2468457 ]]\n",
      "Reward : [-14.37293176]\n",
      "--------------------------------------------------\n",
      "Step : 23\n",
      "Current State : [-0.6642804 -0.7474835  6.7689276]\n",
      "Currrent Action : tensor([[-0.7635]])\n",
      "Next State : [[-0.6642804]\n",
      " [-0.7474835]\n",
      " [ 6.7689276]]\n",
      "Reward : [-12.199562]\n",
      "--------------------------------------------------\n",
      "Step : 24\n",
      "Current State : [-0.41124347 -0.91152555  6.054258  ]\n",
      "Currrent Action : tensor([[-1.0270]])\n",
      "Next State : [[-0.41124347]\n",
      " [-0.91152555]\n",
      " [ 6.054258  ]]\n",
      "Reward : [-9.86060448]\n",
      "--------------------------------------------------\n",
      "Step : 25\n",
      "Current State : [-0.15705211 -0.9875903   5.322256  ]\n",
      "Currrent Action : tensor([[-0.3224]])\n",
      "Next State : [[-0.15705211]\n",
      " [-0.9875903 ]\n",
      " [ 5.322256  ]]\n",
      "Reward : [-7.6439931]\n",
      "--------------------------------------------------\n",
      "Step : 26\n",
      "Current State : [ 0.07795335 -0.996957    4.7147503 ]\n",
      "Currrent Action : tensor([[0.8879]])\n",
      "Next State : [[ 0.07795335]\n",
      " [-0.996957  ]\n",
      " [ 4.7147503 ]]\n",
      "Reward : [-5.82114624]\n",
      "--------------------------------------------------\n",
      "Step : 27\n",
      "Current State : [ 0.27050632 -0.9627182   3.9177284 ]\n",
      "Currrent Action : tensor([[-0.3287]])\n",
      "Next State : [[ 0.27050632]\n",
      " [-0.9627182 ]\n",
      " [ 3.9177284 ]]\n",
      "Reward : [-4.45133896]\n",
      "--------------------------------------------------\n",
      "Step : 28\n",
      "Current State : [ 0.41000468 -0.9120834   2.9708054 ]\n",
      "Currrent Action : tensor([[-1.4992]])\n",
      "Next State : [[ 0.41000468]\n",
      " [-0.9120834 ]\n",
      " [ 2.9708054 ]]\n",
      "Reward : [-3.21899824]\n",
      "--------------------------------------------------\n",
      "Step : 29\n",
      "Current State : [ 0.51537    -0.85696775  2.3796034 ]\n",
      "Currrent Action : tensor([[0.6191]])\n",
      "Next State : [[ 0.51537   ]\n",
      " [-0.85696775]\n",
      " [ 2.3796034 ]]\n",
      "Reward : [-2.20162991]\n",
      "--------------------------------------------------\n",
      "Step : 30\n",
      "Current State : [ 0.5845509 -0.811357   1.657742 ]\n",
      "Currrent Action : tensor([[-0.5276]])\n",
      "Next State : [[ 0.5845509]\n",
      " [-0.811357 ]\n",
      " [ 1.657742 ]]\n",
      "Reward : [-1.62610532]\n",
      "--------------------------------------------------\n",
      "Step : 31\n",
      "Current State : [ 0.6303654 -0.7762986  1.1539456]\n",
      "Currrent Action : tensor([[0.6981]])\n",
      "Next State : [[ 0.6303654]\n",
      " [-0.7762986]\n",
      " [ 1.1539456]]\n",
      "Reward : [-1.17110343]\n",
      "--------------------------------------------------\n",
      "Step : 32\n",
      "Current State : [ 0.6487137  -0.7610326   0.47738487]\n",
      "Currrent Action : tensor([[-0.6289]])\n",
      "Next State : [[ 0.6487137 ]\n",
      " [-0.7610326 ]\n",
      " [ 0.47738487]]\n",
      "Reward : [-0.92347125]\n",
      "--------------------------------------------------\n",
      "Step : 33\n",
      "Current State : [ 0.6513419  -0.75878435  0.06917229]\n",
      "Currrent Action : tensor([[1.0837]])\n",
      "Next State : [[ 0.6513419 ]\n",
      " [-0.75878435]\n",
      " [ 0.06917229]]\n",
      "Reward : [-0.77202191]\n",
      "--------------------------------------------------\n",
      "Step : 34\n",
      "Current State : [ 0.6359045  -0.7717678  -0.40343422]\n",
      "Currrent Action : tensor([[0.6432]])\n",
      "Next State : [[ 0.6359045 ]\n",
      " [-0.7717678 ]\n",
      " [-0.40343422]]\n",
      "Reward : [-0.7429792]\n",
      "--------------------------------------------------\n",
      "Step : 35\n",
      "Current State : [ 0.5967039  -0.8024615  -0.99585134]\n",
      "Currrent Action : tensor([[-0.0906]])\n",
      "Next State : [[ 0.5967039 ]\n",
      " [-0.8024615 ]\n",
      " [-0.99585134]]\n",
      "Reward : [-0.79353165]\n",
      "--------------------------------------------------\n",
      "Step : 36\n",
      "Current State : [ 0.5409725 -0.8410403 -1.3558873]\n",
      "Currrent Action : tensor([[1.6121]])\n",
      "Next State : [[ 0.5409725]\n",
      " [-0.8410403]\n",
      " [-1.3558873]]\n",
      "Reward : [-0.96929347]\n",
      "--------------------------------------------------\n",
      "Step : 37\n",
      "Current State : [ 0.46523133 -0.8851891  -1.7539408 ]\n",
      "Currrent Action : tensor([[1.5515]])\n",
      "Next State : [[ 0.46523133]\n",
      " [-0.8851891 ]\n",
      " [-1.7539408 ]]\n",
      "Reward : [-1.18465759]\n",
      "--------------------------------------------------\n",
      "Step : 38\n",
      "Current State : [ 0.35904115 -0.9333217  -2.333112  ]\n",
      "Currrent Action : tensor([[0.5648]])\n",
      "Next State : [[ 0.35904115]\n",
      " [-0.9333217 ]\n",
      " [-2.333112  ]]\n",
      "Reward : [-1.48930236]\n",
      "--------------------------------------------------\n",
      "Step : 39\n",
      "Current State : [ 0.21725956 -0.97611386 -2.964685  ]\n",
      "Currrent Action : tensor([[0.4561]])\n",
      "Next State : [[ 0.21725956]\n",
      " [-0.97611386]\n",
      " [-2.964685  ]]\n",
      "Reward : [-1.9930962]\n",
      "--------------------------------------------------\n",
      "Step : 40\n",
      "Current State : [ 0.04169223 -0.9991305  -3.5460353 ]\n",
      "Currrent Action : tensor([[1.0049]])\n",
      "Next State : [[ 0.04169223]\n",
      " [-0.9991305 ]\n",
      " [-3.5460353 ]]\n",
      "Reward : [-2.70728234]\n",
      "--------------------------------------------------\n",
      "Step : 41\n",
      "Current State : [-0.1739181 -0.9847601 -4.330227 ]\n",
      "Currrent Action : tensor([[-0.2323]])\n",
      "Next State : [[-0.1739181]\n",
      " [-0.9847601]\n",
      " [-4.330227 ]]\n",
      "Reward : [-3.59561293]\n",
      "--------------------------------------------------\n",
      "Step : 42\n",
      "Current State : [-0.40359434 -0.91493803 -4.812699  ]\n",
      "Currrent Action : tensor([[1.7073]])\n",
      "Next State : [[-0.40359434]\n",
      " [-0.91493803]\n",
      " [-4.812699  ]]\n",
      "Reward : [-4.92513243]\n",
      "--------------------------------------------------\n",
      "Step : 43\n",
      "Current State : [-0.6472284 -0.7622961 -5.7700205]\n",
      "Currrent Action : tensor([[-1.8075]])\n",
      "Next State : [[-0.6472284]\n",
      " [-0.7622961]\n",
      " [-5.7700205]]\n",
      "Reward : [-6.2646163]\n",
      "--------------------------------------------------\n",
      "Step : 44\n",
      "Current State : [-0.8490597  -0.52829695 -6.205197  ]\n",
      "Currrent Action : tensor([[0.9103]])\n",
      "Next State : [[-0.8490597 ]\n",
      " [-0.52829695]\n",
      " [-6.205197  ]]\n",
      "Reward : [-8.50458123]\n",
      "--------------------------------------------------\n",
      "Step : 45\n",
      "Current State : [-0.9712512  -0.23805708 -6.3245716 ]\n",
      "Currrent Action : tensor([[1.8457]])\n",
      "Next State : [[-0.9712512 ]\n",
      " [-0.23805708]\n",
      " [-6.3245716 ]]\n",
      "Reward : [-10.53607398]\n",
      "--------------------------------------------------\n",
      "Step : 46\n",
      "Current State : [-0.9974046   0.07200114 -6.2485685 ]\n",
      "Currrent Action : tensor([[1.6970]])\n",
      "Next State : [[-0.9974046 ]\n",
      " [ 0.07200114]\n",
      " [-6.2485685 ]]\n",
      "Reward : [-12.42002248]\n",
      "--------------------------------------------------\n",
      "Step : 47\n",
      "Current State : [-0.93248457  0.36120987 -5.950031  ]\n",
      "Currrent Action : tensor([[1.6302]])\n",
      "Next State : [[-0.93248457]\n",
      " [ 0.36120987]\n",
      " [-5.950031  ]]\n",
      "Reward : [-13.32912766]\n",
      "--------------------------------------------------\n",
      "Step : 48\n",
      "Current State : [-0.7960841   0.60518605 -5.60869   ]\n",
      "Currrent Action : tensor([[0.4696]])\n",
      "Next State : [[-0.7960841 ]\n",
      " [ 0.60518605]\n",
      " [-5.60869   ]]\n",
      "Reward : [-11.22464409]\n",
      "--------------------------------------------------\n",
      "Step : 49\n",
      "Current State : [-0.61245966  0.79050183 -5.232569  ]\n",
      "Currrent Action : tensor([[-0.5185]])\n",
      "Next State : [[-0.61245966]\n",
      " [ 0.79050183]\n",
      " [-5.232569  ]]\n",
      "Reward : [-9.35404531]\n",
      "--------------------------------------------------\n",
      "Step : 50\n",
      "Current State : [-0.41137353  0.91146684 -4.7041574 ]\n",
      "Currrent Action : tensor([[-0.4298]])\n",
      "Next State : [[-0.41137353]\n",
      " [ 0.91146684]\n",
      " [-4.7041574 ]]\n",
      "Reward : [-7.71090525]\n",
      "--------------------------------------------------\n",
      "Step : 51\n",
      "Current State : [-0.22405437  0.97457665 -3.959757  ]\n",
      "Currrent Action : tensor([[0.4053]])\n",
      "Next State : [[-0.22405437]\n",
      " [ 0.97457665]\n",
      " [-3.959757  ]]\n",
      "Reward : [-6.1921287]\n",
      "--------------------------------------------------\n",
      "Step : 52\n",
      "Current State : [-0.07727097  0.9970101  -2.972491  ]\n",
      "Currrent Action : tensor([[1.7089]])\n",
      "Next State : [[-0.07727097]\n",
      " [ 0.9970101 ]\n",
      " [-2.972491  ]]\n",
      "Reward : [-4.79926658]\n",
      "--------------------------------------------------\n",
      "Step : 53\n",
      "Current State : [ 0.01888748  0.9998216  -1.9247334 ]\n",
      "Currrent Action : tensor([[2.0110]])\n",
      "Next State : [[ 0.01888748]\n",
      " [ 0.9998216 ]\n",
      " [-1.9247334 ]]\n",
      "Reward : [-3.60395024]\n",
      "--------------------------------------------------\n",
      "Step : 54\n",
      "Current State : [ 0.06944767  0.9975856  -1.0123004 ]\n",
      "Currrent Action : tensor([[1.0838]])\n",
      "Next State : [[ 0.06944767]\n",
      " [ 0.9975856 ]\n",
      " [-1.0123004 ]]\n",
      "Reward : [-2.78005204]\n",
      "--------------------------------------------------\n",
      "Step : 55\n",
      "Current State : [ 0.07835518  0.99692553 -0.17863905]\n",
      "Currrent Action : tensor([[0.5698]])\n",
      "Next State : [[ 0.07835518]\n",
      " [ 0.99692553]\n",
      " [-0.17863905]]\n",
      "Reward : [-2.35667969]\n",
      "--------------------------------------------------\n",
      "Step : 56\n",
      "Current State : [0.0397581  0.99920934 0.77333975]\n",
      "Currrent Action : tensor([[1.3619]])\n",
      "Next State : [[0.0397581 ]\n",
      " [0.99920934]\n",
      " [0.77333975]]\n",
      "Reward : [-2.23218657]\n",
      "--------------------------------------------------\n",
      "Step : 57\n",
      "Current State : [-0.02995287  0.9995513   1.3945187 ]\n",
      "Currrent Action : tensor([[-0.8549]])\n",
      "Next State : [[-0.02995287]\n",
      " [ 0.9995513 ]\n",
      " [ 1.3945187 ]]\n",
      "Reward : [-2.40458214]\n",
      "--------------------------------------------------\n",
      "Step : 58\n",
      "Current State : [-0.13494243  0.9908534   2.1079602 ]\n",
      "Currrent Action : tensor([[-0.2415]])\n",
      "Next State : [[-0.13494243]\n",
      " [ 0.9908534 ]\n",
      " [ 2.1079602 ]]\n",
      "Reward : [-2.7569389]\n",
      "--------------------------------------------------\n",
      "Step : 59\n",
      "Current State : [-0.2837177   0.95890784  3.0462706 ]\n",
      "Currrent Action : tensor([[1.3011]])\n",
      "Next State : [[-0.2837177 ]\n",
      " [ 0.95890784]\n",
      " [ 3.0462706 ]]\n",
      "Reward : [-3.35699615]\n",
      "--------------------------------------------------\n",
      "Step : 60\n",
      "Current State : [-0.4550387   0.89047164  3.6949356 ]\n",
      "Currrent Action : tensor([[-0.4701]])\n",
      "Next State : [[-0.4550387 ]\n",
      " [ 0.89047164]\n",
      " [ 3.6949356 ]]\n",
      "Reward : [-4.38209041]\n",
      "--------------------------------------------------\n",
      "Step : 61\n",
      "Current State : [-0.64152396  0.767103    4.4813514 ]\n",
      "Currrent Action : tensor([[0.7904]])\n",
      "Next State : [[-0.64152396]\n",
      " [ 0.767103  ]\n",
      " [ 4.4813514 ]]\n",
      "Reward : [-5.54059491]\n",
      "--------------------------------------------------\n",
      "Step : 62\n",
      "Current State : [-0.8101812  0.5861795  4.9595585]\n",
      "Currrent Action : tensor([[-0.6475]])\n",
      "Next State : [[-0.8101812]\n",
      " [ 0.5861795]\n",
      " [ 4.9595585]]\n",
      "Reward : [-7.149227]\n",
      "--------------------------------------------------\n",
      "Step : 63\n",
      "Current State : [-0.94218904  0.33508185  5.6928616 ]\n",
      "Currrent Action : tensor([[1.9578]])\n",
      "Next State : [[-0.94218904]\n",
      " [ 0.33508185]\n",
      " [ 5.6928616 ]]\n",
      "Reward : [-8.79007504]\n",
      "--------------------------------------------------\n",
      "Step : 64\n",
      "Current State : [-0.9991423   0.04140856  6.0054336 ]\n",
      "Currrent Action : tensor([[0.4084]])\n",
      "Next State : [[-0.9991423 ]\n",
      " [ 0.04140856]\n",
      " [ 6.0054336 ]]\n",
      "Reward : [-11.08047734]\n",
      "--------------------------------------------------\n",
      "Step : 65\n",
      "Current State : [-0.9642356 -0.2650467  6.193455 ]\n",
      "Currrent Action : tensor([[1.0464]])\n",
      "Next State : [[-0.9642356]\n",
      " [-0.2650467]\n",
      " [ 6.193455 ]]\n",
      "Reward : [-13.2186864]\n",
      "--------------------------------------------------\n",
      "Step : 66\n",
      "Current State : [-0.84036607 -0.5420193   6.091716  ]\n",
      "Currrent Action : tensor([[0.6470]])\n",
      "Next State : [[-0.84036607]\n",
      " [-0.5420193 ]\n",
      " [ 6.091716  ]]\n",
      "Reward : [-12.09239158]\n",
      "--------------------------------------------------\n",
      "Step : 67\n",
      "Current State : [-0.6523418 -0.7579249  5.7457623]\n",
      "Currrent Action : tensor([[0.4037]])\n",
      "Next State : [[-0.6523418]\n",
      " [-0.7579249]\n",
      " [ 5.7457623]]\n",
      "Reward : [-10.3095628]\n",
      "--------------------------------------------------\n",
      "Step : 68\n",
      "Current State : [-0.43565452 -0.90011394  5.1980953 ]\n",
      "Currrent Action : tensor([[0.1385]])\n",
      "Next State : [[-0.43565452]\n",
      " [-0.90011394]\n",
      " [ 5.1980953 ]]\n",
      "Reward : [-8.50648649]\n",
      "--------------------------------------------------\n",
      "Step : 69\n",
      "Current State : [-0.2240978  -0.97456664  4.494962  ]\n",
      "Currrent Action : tensor([[-0.1870]])\n",
      "Next State : [[-0.2240978 ]\n",
      " [-0.97456664]\n",
      " [ 4.494962  ]]\n",
      "Reward : [-6.78876607]\n",
      "--------------------------------------------------\n",
      "Step : 70\n",
      "Current State : [-0.04144487 -0.9991408   3.691209  ]\n",
      "Currrent Action : tensor([[-0.4855]])\n",
      "Next State : [[-0.04144487]\n",
      " [-0.9991408 ]\n",
      " [ 3.691209  ]]\n",
      "Reward : [-5.24924302]\n",
      "--------------------------------------------------\n",
      "Step : 71\n",
      "Current State : [ 0.11386203 -0.9934966   3.111325  ]\n",
      "Currrent Action : tensor([[1.1298]])\n",
      "Next State : [[ 0.11386203]\n",
      " [-0.9934966 ]\n",
      " [ 3.111325  ]]\n",
      "Reward : [-3.96313891]\n",
      "--------------------------------------------------\n",
      "Step : 72\n",
      "Current State : [ 0.2281185  -0.97363335  2.3207054 ]\n",
      "Currrent Action : tensor([[-0.3033]])\n",
      "Next State : [[ 0.2281185 ]\n",
      " [-0.97363335]\n",
      " [ 2.3207054 ]]\n",
      "Reward : [-3.09006287]\n",
      "--------------------------------------------------\n",
      "Step : 73\n",
      "Current State : [ 0.29042295 -0.9568984   1.2904805 ]\n",
      "Currrent Action : tensor([[-2.3108]])\n",
      "Next State : [[ 0.29042295]\n",
      " [-0.9568984 ]\n",
      " [ 1.2904805 ]]\n",
      "Reward : [-2.33991397]\n",
      "--------------------------------------------------\n",
      "Step : 74\n",
      "Current State : [ 0.30371478 -0.952763    0.2784078 ]\n",
      "Currrent Action : tensor([[-1.9627]])\n",
      "Next State : [[ 0.30371478]\n",
      " [-0.952763  ]\n",
      " [ 0.2784078 ]]\n",
      "Reward : [-1.79888746]\n",
      "--------------------------------------------------\n",
      "Step : 75\n",
      "Current State : [ 0.28424144 -0.95875275 -0.4074814 ]\n",
      "Currrent Action : tensor([[0.1912]])\n",
      "Next State : [[ 0.28424144]\n",
      " [-0.95875275]\n",
      " [-0.4074814 ]]\n",
      "Reward : [-1.60095449]\n",
      "--------------------------------------------------\n",
      "Step : 76\n",
      "Current State : [ 0.23373973 -0.9722992  -1.0458587 ]\n",
      "Currrent Action : tensor([[0.5379]])\n",
      "Next State : [[ 0.23373973]\n",
      " [-0.9722992 ]\n",
      " [-1.0458587 ]]\n",
      "Reward : [-1.661908]\n",
      "--------------------------------------------------\n",
      "Step : 77\n",
      "Current State : [ 0.1523487 -0.9883268 -1.6595582]\n",
      "Currrent Action : tensor([[0.7702]])\n",
      "Next State : [[ 0.1523487]\n",
      " [-0.9883268]\n",
      " [-1.6595582]]\n",
      "Reward : [-1.89186415]\n",
      "--------------------------------------------------\n",
      "Step : 78\n",
      "Current State : [ 0.03829525 -0.99926645 -2.2927933 ]\n",
      "Currrent Action : tensor([[0.7201]])\n",
      "Next State : [[ 0.03829525]\n",
      " [-0.99926645]\n",
      " [-2.2927933 ]]\n",
      "Reward : [-2.28623625]\n",
      "--------------------------------------------------\n",
      "Step : 79\n",
      "Current State : [-0.11127691 -0.99378943 -2.9962494 ]\n",
      "Currrent Action : tensor([[0.3066]])\n",
      "Next State : [[-0.11127691]\n",
      " [-0.99378943]\n",
      " [-2.9962494 ]]\n",
      "Reward : [-2.87431493]\n",
      "--------------------------------------------------\n",
      "Step : 80\n",
      "Current State : [-0.28454894 -0.9586615  -3.540561  ]\n",
      "Currrent Action : tensor([[1.3402]])\n",
      "Next State : [[-0.28454894]\n",
      " [-0.9586615 ]\n",
      " [-3.540561  ]]\n",
      "Reward : [-3.72969448]\n",
      "--------------------------------------------------\n",
      "Step : 81\n",
      "Current State : [-0.47692204 -0.8789456  -4.1722717 ]\n",
      "Currrent Action : tensor([[0.5819]])\n",
      "Next State : [[-0.47692204]\n",
      " [-0.8789456 ]\n",
      " [-4.1722717 ]]\n",
      "Reward : [-4.71101218]\n",
      "--------------------------------------------------\n",
      "Step : 82\n",
      "Current State : [-0.67506975 -0.7377539  -4.878195  ]\n",
      "Currrent Action : tensor([[-0.3114]])\n",
      "Next State : [[-0.67506975]\n",
      " [-0.7377539 ]\n",
      " [-4.878195  ]]\n",
      "Reward : [-6.01728186]\n",
      "--------------------------------------------------\n",
      "Step : 83\n",
      "Current State : [-0.8418441  -0.53972083 -5.192633  ]\n",
      "Currrent Action : tensor([[1.5925]])\n",
      "Next State : [[-0.8418441 ]\n",
      " [-0.53972083]\n",
      " [-5.192633  ]]\n",
      "Reward : [-7.72689051]\n",
      "--------------------------------------------------\n",
      "Step : 84\n",
      "Current State : [-0.9574762  -0.28851235 -5.5486536 ]\n",
      "Currrent Action : tensor([[0.3251]])\n",
      "Next State : [[-0.9574762 ]\n",
      " [-0.28851235]\n",
      " [-5.5486536 ]]\n",
      "Reward : [-9.30899629]\n",
      "--------------------------------------------------\n",
      "Step : 85\n",
      "Current State : [-0.9998114  -0.01941964 -5.465038  ]\n",
      "Currrent Action : tensor([[2.2790]])\n",
      "Next State : [[-0.9998114 ]\n",
      " [-0.01941964]\n",
      " [-5.465038  ]]\n",
      "Reward : [-11.19910041]\n",
      "--------------------------------------------------\n",
      "Step : 86\n",
      "Current State : [-0.96819216  0.25020775 -5.446314  ]\n",
      "Currrent Action : tensor([[0.2219]])\n",
      "Next State : [[-0.96819216]\n",
      " [ 0.25020775]\n",
      " [-5.446314  ]]\n",
      "Reward : [-12.73467002]\n",
      "--------------------------------------------------\n",
      "Step : 87\n",
      "Current State : [-0.86780876  0.49689838 -5.3425236 ]\n",
      "Currrent Action : tensor([[-0.5591]])\n",
      "Next State : [[-0.86780876]\n",
      " [ 0.49689838]\n",
      " [-5.3425236 ]]\n",
      "Reward : [-11.31112126]\n",
      "--------------------------------------------------\n",
      "Step : 88\n",
      "Current State : [-0.7292894   0.68420535 -4.66985   ]\n",
      "Currrent Action : tensor([[2.2392]])\n",
      "Next State : [[-0.7292894 ]\n",
      " [ 0.68420535]\n",
      " [-4.66985   ]]\n",
      "Reward : [-9.73089364]\n",
      "--------------------------------------------------\n",
      "Step : 89\n",
      "Current State : [-0.56936246  0.8220866  -4.2310534 ]\n",
      "Currrent Action : tensor([[-0.4957]])\n",
      "Next State : [[-0.56936246]\n",
      " [ 0.8220866 ]\n",
      " [-4.2310534 ]]\n",
      "Reward : [-7.88391744]\n",
      "--------------------------------------------------\n",
      "Step : 90\n",
      "Current State : [-0.4025243  0.9154093 -3.8291483]\n",
      "Currrent Action : tensor([[-1.4311]])\n",
      "Next State : [[-0.4025243]\n",
      " [ 0.9154093]\n",
      " [-3.8291483]]\n",
      "Reward : [-6.52949681]\n",
      "--------------------------------------------------\n",
      "Step : 91\n",
      "Current State : [-0.26381227  0.96457404 -2.9460075 ]\n",
      "Currrent Action : tensor([[1.3106]])\n",
      "Next State : [[-0.26381227]\n",
      " [ 0.96457404]\n",
      " [-2.9460075 ]]\n",
      "Reward : [-5.40845433]\n",
      "--------------------------------------------------\n",
      "Step : 92\n",
      "Current State : [-0.15423651  0.98803395 -2.2423537 ]\n",
      "Currrent Action : tensor([[-0.1318]])\n",
      "Next State : [[-0.15423651]\n",
      " [ 0.98803395]\n",
      " [-2.2423537 ]]\n",
      "Reward : [-4.2453071]\n",
      "--------------------------------------------------\n",
      "Step : 93\n",
      "Current State : [-0.08560199  0.9963294  -1.3829558 ]\n",
      "Currrent Action : tensor([[0.7891]])\n",
      "Next State : [[-0.08560199]\n",
      " [ 0.9963294 ]\n",
      " [-1.3829558 ]]\n",
      "Reward : [-3.48130915]\n",
      "--------------------------------------------------\n",
      "Step : 94\n",
      "Current State : [-0.06312805  0.99800545 -0.45073652]\n",
      "Currrent Action : tensor([[1.2331]])\n",
      "Next State : [[-0.06312805]\n",
      " [ 0.99800545]\n",
      " [-0.45073652]]\n",
      "Reward : [-2.93678021]\n",
      "--------------------------------------------------\n",
      "Step : 95\n",
      "Current State : [-0.08817898  0.99610466  0.502472  ]\n",
      "Currrent Action : tensor([[1.3647]])\n",
      "Next State : [[-0.08817898]\n",
      " [ 0.99610466]\n",
      " [ 0.502472  ]]\n",
      "Reward : [-2.69202487]\n",
      "--------------------------------------------------\n",
      "Step : 96\n",
      "Current State : [-0.14953682  0.9887562   1.2361231 ]\n",
      "Currrent Action : tensor([[-0.0895]])\n",
      "Next State : [[-0.14953682]\n",
      " [ 0.9887562 ]\n",
      " [ 1.2361231 ]]\n",
      "Reward : [-2.77783538]\n",
      "--------------------------------------------------\n",
      "Step : 97\n",
      "Current State : [-0.25173414  0.96779644  2.0874376 ]\n",
      "Currrent Action : tensor([[0.7316]])\n",
      "Next State : [[-0.25173414]\n",
      " [ 0.96779644]\n",
      " [ 2.0874376 ]]\n",
      "Reward : [-3.11481883]\n",
      "--------------------------------------------------\n",
      "Step : 98\n",
      "Current State : [-0.3840526   0.92331123  2.794197  ]\n",
      "Currrent Action : tensor([[-0.1273]])\n",
      "Next State : [[-0.3840526 ]\n",
      " [ 0.92331123]\n",
      " [ 2.794197  ]]\n",
      "Reward : [-3.76735905]\n",
      "--------------------------------------------------\n",
      "Step : 99\n",
      "Current State : [-0.5405274  0.8413264  3.537647 ]\n",
      "Currrent Action : tensor([[0.3398]])\n",
      "Next State : [[-0.5405274]\n",
      " [ 0.8413264]\n",
      " [ 3.537647 ]]\n",
      "Reward : [-4.64200711]\n",
      "learning iteration : 17\n",
      "--------------------------------------------------\n",
      "Step : 0\n",
      "Current State : [-0.24229568  0.97020245  1.2899567 ]\n",
      "Currrent Action : tensor([[-0.5249]])\n",
      "Next State : [[-0.24229568]\n",
      " [ 0.97020245]\n",
      " [ 1.2899567 ]]\n",
      "Reward : [-3.1061764]\n",
      "--------------------------------------------------\n",
      "Step : 1\n",
      "Current State : [-0.34375373  0.93905985  2.1235993 ]\n",
      "Currrent Action : tensor([[0.7066]])\n",
      "Next State : [[-0.34375373]\n",
      " [ 0.93905985]\n",
      " [ 2.1235993 ]]\n",
      "Reward : [-3.46303883]\n",
      "--------------------------------------------------\n",
      "Step : 2\n",
      "Current State : [-0.4839191  0.8751127  3.0843263]\n",
      "Currrent Action : tensor([[1.7095]])\n",
      "Next State : [[-0.4839191]\n",
      " [ 0.8751127]\n",
      " [ 3.0843263]]\n",
      "Reward : [-4.14685022]\n",
      "--------------------------------------------------\n",
      "Step : 3\n",
      "Current State : [-0.6416729   0.76697844  3.8309963 ]\n",
      "Currrent Action : tensor([[0.6022]])\n",
      "Next State : [[-0.6416729 ]\n",
      " [ 0.76697844]\n",
      " [ 3.8309963 ]]\n",
      "Reward : [-5.26112973]\n",
      "--------------------------------------------------\n",
      "Step : 4\n",
      "Current State : [-0.7924619   0.60992146  4.3631487 ]\n",
      "Currrent Action : tensor([[-0.2872]])\n",
      "Next State : [[-0.7924619 ]\n",
      " [ 0.60992146]\n",
      " [ 4.3631487 ]]\n",
      "Reward : [-6.60917291]\n",
      "--------------------------------------------------\n",
      "Step : 5\n",
      "Current State : [-0.91533524  0.40269274  4.830094  ]\n",
      "Currrent Action : tensor([[0.0634]])\n",
      "Next State : [[-0.91533524]\n",
      " [ 0.40269274]\n",
      " [ 4.830094  ]]\n",
      "Reward : [-8.08207333]\n",
      "--------------------------------------------------\n",
      "Step : 6\n",
      "Current State : [-0.9886848   0.15000808  5.277606  ]\n",
      "Currrent Action : tensor([[0.9700]])\n",
      "Next State : [[-0.9886848 ]\n",
      " [ 0.15000808]\n",
      " [ 5.277606  ]]\n",
      "Reward : [-9.7711918]\n",
      "--------------------------------------------------\n",
      "Step : 7\n",
      "Current State : [-0.9917387  -0.12827481  5.584113  ]\n",
      "Currrent Action : tensor([[1.2933]])\n",
      "Next State : [[-0.9917387 ]\n",
      " [-0.12827481]\n",
      " [ 5.584113  ]]\n",
      "Reward : [-11.73316337]\n",
      "--------------------------------------------------\n",
      "Step : 8\n",
      "Current State : [-0.9194367 -0.393238   5.510429 ]\n",
      "Currrent Action : tensor([[0.1501]])\n",
      "Next State : [[-0.9194367]\n",
      " [-0.393238 ]\n",
      " [ 5.510429 ]]\n",
      "Reward : [-12.19620336]\n",
      "--------------------------------------------------\n",
      "Step : 9\n",
      "Current State : [-0.78264576 -0.62246734  5.3548136 ]\n",
      "Currrent Action : tensor([[0.9288]])\n",
      "Next State : [[-0.78264576]\n",
      " [-0.62246734]\n",
      " [ 5.3548136 ]]\n",
      "Reward : [-10.53093387]\n",
      "--------------------------------------------------\n",
      "Step : 10\n",
      "Current State : [-0.6128957 -0.7901638  4.7836967]\n",
      "Currrent Action : tensor([[-0.6951]])\n",
      "Next State : [[-0.6128957]\n",
      " [-0.7901638]\n",
      " [ 4.7836967]]\n",
      "Reward : [-8.9673107]\n",
      "--------------------------------------------------\n",
      "Step : 11\n",
      "Current State : [-0.44291303 -0.8965646   4.017499  ]\n",
      "Currrent Action : tensor([[-1.1572]])\n",
      "Next State : [[-0.44291303]\n",
      " [-0.8965646 ]\n",
      " [ 4.017499  ]]\n",
      "Reward : [-7.26491815]\n",
      "--------------------------------------------------\n",
      "Step : 12\n",
      "Current State : [-0.28574386 -0.958306    3.3812523 ]\n",
      "Currrent Action : tensor([[0.2412]])\n",
      "Next State : [[-0.28574386]\n",
      " [-0.958306  ]\n",
      " [ 3.3812523 ]]\n",
      "Reward : [-5.73353269]\n",
      "--------------------------------------------------\n",
      "Step : 13\n",
      "Current State : [-0.14295702 -0.98972887  2.9266818 ]\n",
      "Currrent Action : tensor([[1.7611]])\n",
      "Next State : [[-0.14295702]\n",
      " [-0.98972887]\n",
      " [ 2.9266818 ]]\n",
      "Reward : [-4.60814187]\n",
      "--------------------------------------------------\n",
      "Step : 14\n",
      "Current State : [-0.03834279 -0.99926466  2.1019258 ]\n",
      "Currrent Action : tensor([[-0.5497]])\n",
      "Next State : [[-0.03834279]\n",
      " [-0.99926466]\n",
      " [ 2.1019258 ]]\n",
      "Reward : [-3.79548408]\n",
      "--------------------------------------------------\n",
      "Step : 15\n",
      "Current State : [ 0.03244918 -0.9994734   1.4161413 ]\n",
      "Currrent Action : tensor([[0.4244]])\n",
      "Next State : [[ 0.03244918]\n",
      " [-0.9994734 ]\n",
      " [ 1.4161413 ]]\n",
      "Reward : [-3.03134831]\n",
      "--------------------------------------------------\n",
      "Step : 16\n",
      "Current State : [ 0.05991102 -0.9982037   0.54984075]\n",
      "Currrent Action : tensor([[-0.7780]])\n",
      "Next State : [[ 0.05991102]\n",
      " [-0.9982037 ]\n",
      " [ 0.54984075]]\n",
      "Reward : [-2.56764528]\n",
      "--------------------------------------------------\n",
      "Step : 17\n",
      "Current State : [ 0.05544323 -0.99846184 -0.08950473]\n",
      "Currrent Action : tensor([[0.7287]])\n",
      "Next State : [[ 0.05544323]\n",
      " [-0.99846184]\n",
      " [-0.08950473]]\n",
      "Reward : [-2.31342946]\n",
      "--------------------------------------------------\n",
      "Step : 18\n",
      "Current State : [ 0.00416514 -0.9999913  -1.0261306 ]\n",
      "Currrent Action : tensor([[-1.2519]])\n",
      "Next State : [[ 0.00416514]\n",
      " [-0.9999913 ]\n",
      " [-1.0261306 ]]\n",
      "Reward : [-2.29857706]\n",
      "--------------------------------------------------\n",
      "Step : 19\n",
      "Current State : [-0.0899802  -0.99594355 -1.8853444 ]\n",
      "Currrent Action : tensor([[-0.7281]])\n",
      "Next State : [[-0.0899802 ]\n",
      " [-0.99594355]\n",
      " [-1.8853444 ]]\n",
      "Reward : [-2.56015781]\n",
      "--------------------------------------------------\n",
      "Step : 20\n",
      "Current State : [-0.2210209 -0.9752691 -2.6551816]\n",
      "Currrent Action : tensor([[-0.1525]])\n",
      "Next State : [[-0.2210209]\n",
      " [-0.9752691]\n",
      " [-2.6551816]]\n",
      "Reward : [-3.11405906]\n",
      "--------------------------------------------------\n",
      "Step : 21\n",
      "Current State : [-0.3683121 -0.9297022 -3.0866334]\n",
      "Currrent Action : tensor([[2.0965]])\n",
      "Next State : [[-0.3683121]\n",
      " [-0.9297022]\n",
      " [-3.0866334]]\n",
      "Reward : [-3.92620604]\n",
      "--------------------------------------------------\n",
      "Step : 22\n",
      "Current State : [-0.534496   -0.84517103 -3.7343695 ]\n",
      "Currrent Action : tensor([[0.3303]])\n",
      "Next State : [[-0.534496  ]\n",
      " [-0.84517103]\n",
      " [-3.7343695 ]]\n",
      "Reward : [-4.74750133]\n",
      "--------------------------------------------------\n",
      "Step : 23\n",
      "Current State : [-0.69840753 -0.71570027 -4.1851754 ]\n",
      "Currrent Action : tensor([[1.2205]])\n",
      "Next State : [[-0.69840753]\n",
      " [-0.71570027]\n",
      " [-4.1851754 ]]\n",
      "Reward : [-5.95301765]\n",
      "--------------------------------------------------\n",
      "Step : 24\n",
      "Current State : [-0.84124434 -0.54065514 -4.5282125 ]\n",
      "Currrent Action : tensor([[1.2916]])\n",
      "Next State : [[-0.84124434]\n",
      " [-0.54065514]\n",
      " [-4.5282125 ]]\n",
      "Reward : [-7.24741572]\n",
      "--------------------------------------------------\n",
      "Step : 25\n",
      "Current State : [-0.943017   -0.33274454 -4.640064  ]\n",
      "Currrent Action : tensor([[1.9576]])\n",
      "Next State : [[-0.943017  ]\n",
      " [-0.33274454]\n",
      " [-4.640064  ]]\n",
      "Reward : [-8.6611408]\n",
      "--------------------------------------------------\n",
      "Step : 26\n",
      "Current State : [-0.994754   -0.10229602 -4.7347436 ]\n",
      "Currrent Action : tensor([[1.0325]])\n",
      "Next State : [[-0.994754  ]\n",
      " [-0.10229602]\n",
      " [-4.7347436 ]]\n",
      "Reward : [-10.00742021]\n",
      "--------------------------------------------------\n",
      "Step : 27\n",
      "Current State : [-0.98830336  0.15250064 -5.111466  ]\n",
      "Currrent Action : tensor([[-2.0311]])\n",
      "Next State : [[-0.98830336]\n",
      " [ 0.15250064]\n",
      " [-5.111466  ]]\n",
      "Reward : [-11.48201434]\n",
      "--------------------------------------------------\n",
      "Step : 28\n",
      "Current State : [-0.92159796  0.38814586 -4.910417  ]\n",
      "Currrent Action : tensor([[0.5778]])\n",
      "Next State : [[-0.92159796]\n",
      " [ 0.38814586]\n",
      " [-4.910417  ]]\n",
      "Reward : [-11.54414235]\n",
      "--------------------------------------------------\n",
      "Step : 29\n",
      "Current State : [-0.8093458  0.5873325 -4.582803 ]\n",
      "Currrent Action : tensor([[0.2434]])\n",
      "Next State : [[-0.8093458]\n",
      " [ 0.5873325]\n",
      " [-4.582803 ]]\n",
      "Reward : [-9.93518385]\n",
      "--------------------------------------------------\n",
      "Step : 30\n",
      "Current State : [-0.6684294   0.74377567 -4.2188454 ]\n",
      "Currrent Action : tensor([[-0.5103]])\n",
      "Next State : [[-0.6684294 ]\n",
      " [ 0.74377567]\n",
      " [-4.2188454 ]]\n",
      "Reward : [-8.41982812]\n",
      "--------------------------------------------------\n",
      "Step : 31\n",
      "Current State : [-0.52855307  0.84890026 -3.503997  ]\n",
      "Currrent Action : tensor([[1.0468]])\n",
      "Next State : [[-0.52855307]\n",
      " [ 0.84890026]\n",
      " [-3.503997  ]]\n",
      "Reward : [-7.08427024]\n",
      "--------------------------------------------------\n",
      "Step : 32\n",
      "Current State : [-0.40554395  0.91407555 -2.786427  ]\n",
      "Currrent Action : tensor([[0.5393]])\n",
      "Next State : [[-0.40554395]\n",
      " [ 0.91407555]\n",
      " [-2.786427  ]]\n",
      "Reward : [-5.75516158]\n",
      "--------------------------------------------------\n",
      "Step : 33\n",
      "Current State : [-0.31211466  0.95004445 -2.0031154 ]\n",
      "Currrent Action : tensor([[0.6517]])\n",
      "Next State : [[-0.31211466]\n",
      " [ 0.95004445]\n",
      " [-2.0031154 ]]\n",
      "Reward : [-4.73045823]\n",
      "--------------------------------------------------\n",
      "Step : 34\n",
      "Current State : [-0.24373832  0.96984106 -1.4239904 ]\n",
      "Currrent Action : tensor([[-0.8894]])\n",
      "Next State : [[-0.24373832]\n",
      " [ 0.96984106]\n",
      " [-1.4239904 ]]\n",
      "Reward : [-3.96739178]\n",
      "--------------------------------------------------\n",
      "Step : 35\n",
      "Current State : [-0.22445925  0.9744835  -0.39660954]\n",
      "Currrent Action : tensor([[2.1510]])\n",
      "Next State : [[-0.22445925]\n",
      " [ 0.9744835 ]\n",
      " [-0.39660954]]\n",
      "Reward : [-3.50831795]\n",
      "--------------------------------------------------\n",
      "Step : 36\n",
      "Current State : [-0.2424728  0.9701582  0.3705165]\n",
      "Currrent Action : tensor([[0.2418]])\n",
      "Next State : [[-0.2424728]\n",
      " [ 0.9701582]\n",
      " [ 0.3705165]]\n",
      "Reward : [-3.24566017]\n",
      "--------------------------------------------------\n",
      "Step : 37\n",
      "Current State : [-0.28967905  0.9571238   0.9795518 ]\n",
      "Currrent Action : tensor([[-0.7906]])\n",
      "Next State : [[-0.28967905]\n",
      " [ 0.9571238 ]\n",
      " [ 0.9795518 ]]\n",
      "Reward : [-3.31115689]\n",
      "--------------------------------------------------\n",
      "Step : 38\n",
      "Current State : [-0.38131946  0.9244433   1.9466336 ]\n",
      "Currrent Action : tensor([[1.6616]])\n",
      "Next State : [[-0.38131946]\n",
      " [ 0.9244433 ]\n",
      " [ 1.9466336 ]]\n",
      "Reward : [-3.57577374]\n",
      "--------------------------------------------------\n",
      "Step : 39\n",
      "Current State : [-0.49474922  0.8690358   2.5264614 ]\n",
      "Currrent Action : tensor([[-0.7567]])\n",
      "Next State : [[-0.49474922]\n",
      " [ 0.8690358 ]\n",
      " [ 2.5264614 ]]\n",
      "Reward : [-4.22903133]\n",
      "--------------------------------------------------\n",
      "Step : 40\n",
      "Current State : [-0.62542874  0.7802813   3.1626923 ]\n",
      "Currrent Action : tensor([[-0.1036]])\n",
      "Next State : [[-0.62542874]\n",
      " [ 0.7802813 ]\n",
      " [ 3.1626923 ]]\n",
      "Reward : [-4.99948611]\n",
      "--------------------------------------------------\n",
      "Step : 41\n",
      "Current State : [-0.7653344  0.6436328  3.917598 ]\n",
      "Currrent Action : tensor([[1.1313]])\n",
      "Next State : [[-0.7653344]\n",
      " [ 0.6436328]\n",
      " [ 3.917598 ]]\n",
      "Reward : [-6.04820176]\n",
      "--------------------------------------------------\n",
      "Step : 42\n",
      "Current State : [-0.88111717  0.47289798  4.133174  ]\n",
      "Currrent Action : tensor([[-1.7810]])\n",
      "Next State : [[-0.88111717]\n",
      " [ 0.47289798]\n",
      " [ 4.133174  ]]\n",
      "Reward : [-7.50303752]\n",
      "--------------------------------------------------\n",
      "Step : 43\n",
      "Current State : [-0.96553564  0.26027092  4.5854816 ]\n",
      "Currrent Action : tensor([[0.6509]])\n",
      "Next State : [[-0.96553564]\n",
      " [ 0.26027092]\n",
      " [ 4.5854816 ]]\n",
      "Reward : [-8.72602103]\n",
      "--------------------------------------------------\n",
      "Step : 44\n",
      "Current State : [-0.99992645  0.01212836  5.0234823 ]\n",
      "Currrent Action : tensor([[1.6186]])\n",
      "Next State : [[-0.99992645]\n",
      " [ 0.01212836]\n",
      " [ 5.0234823 ]]\n",
      "Reward : [-10.38983699]\n",
      "--------------------------------------------------\n",
      "Step : 45\n",
      "Current State : [-0.971573   -0.23674028  5.022761  ]\n",
      "Currrent Action : tensor([[-0.0655]])\n",
      "Next State : [[-0.971573  ]\n",
      " [-0.23674028]\n",
      " [ 5.022761  ]]\n",
      "Reward : [-12.3170867]\n",
      "--------------------------------------------------\n",
      "Step : 46\n",
      "Current State : [-0.8813176  -0.47252432  5.0628695 ]\n",
      "Currrent Action : tensor([[1.4511]])\n",
      "Next State : [[-0.8813176 ]\n",
      " [-0.47252432]\n",
      " [ 5.0628695 ]]\n",
      "Reward : [-10.94990784]\n",
      "--------------------------------------------------\n",
      "Step : 47\n",
      "Current State : [-0.7487159  -0.66289103  4.650413  ]\n",
      "Currrent Action : tensor([[-0.3871]])\n",
      "Next State : [[-0.7487159 ]\n",
      " [-0.66289103]\n",
      " [ 4.650413  ]]\n",
      "Reward : [-9.58294592]\n",
      "--------------------------------------------------\n",
      "Step : 48\n",
      "Current State : [-0.5978103 -0.8016376  4.10712  ]\n",
      "Currrent Action : tensor([[-0.3075]])\n",
      "Next State : [[-0.5978103]\n",
      " [-0.8016376]\n",
      " [ 4.10712  ]]\n",
      "Reward : [-8.00422699]\n",
      "--------------------------------------------------\n",
      "Step : 49\n",
      "Current State : [-0.45318994 -0.891414    3.408524  ]\n",
      "Currrent Action : tensor([[-0.6491]])\n",
      "Next State : [[-0.45318994]\n",
      " [-0.891414  ]\n",
      " [ 3.408524  ]]\n",
      "Reward : [-6.57827646]\n",
      "--------------------------------------------------\n",
      "Step : 50\n",
      "Current State : [-0.32992166 -0.9440083   2.6824005 ]\n",
      "Currrent Action : tensor([[-0.3838]])\n",
      "Next State : [[-0.32992166]\n",
      " [-0.9440083 ]\n",
      " [ 2.6824005 ]]\n",
      "Reward : [-5.3281908]\n",
      "--------------------------------------------------\n",
      "Step : 51\n",
      "Current State : [-0.23258829 -0.9725753   2.0296493 ]\n",
      "Currrent Action : tensor([[0.3684]])\n",
      "Next State : [[-0.23258829]\n",
      " [-0.9725753 ]\n",
      " [ 2.0296493 ]]\n",
      "Reward : [-4.35637644]\n",
      "--------------------------------------------------\n",
      "Step : 52\n",
      "Current State : [-0.16518037 -0.9862634   1.3759444 ]\n",
      "Currrent Action : tensor([[0.5048]])\n",
      "Next State : [[-0.16518037]\n",
      " [-0.9862634 ]\n",
      " [ 1.3759444 ]]\n",
      "Reward : [-3.67215711]\n",
      "--------------------------------------------------\n",
      "Step : 53\n",
      "Current State : [-0.14687394 -0.98915523  0.3706741 ]\n",
      "Currrent Action : tensor([[-1.7705]])\n",
      "Next State : [[-0.14687394]\n",
      " [-0.98915523]\n",
      " [ 0.3706741 ]]\n",
      "Reward : [-3.2087131]\n",
      "--------------------------------------------------\n",
      "Step : 54\n",
      "Current State : [-0.15780796 -0.9874698  -0.22126439]\n",
      "Currrent Action : tensor([[0.9995]])\n",
      "Next State : [[-0.15780796]\n",
      " [-0.9874698 ]\n",
      " [-0.22126439]]\n",
      "Reward : [-2.9669623]\n",
      "--------------------------------------------------\n",
      "Step : 55\n",
      "Current State : [-0.20715931 -0.97830725 -1.0039996 ]\n",
      "Currrent Action : tensor([[-0.2809]])\n",
      "Next State : [[-0.20715931]\n",
      " [-0.97830725]\n",
      " [-1.0039996 ]]\n",
      "Reward : [-2.99533814]\n",
      "--------------------------------------------------\n",
      "Step : 56\n",
      "Current State : [-0.28811324 -0.95759636 -1.671711  ]\n",
      "Currrent Action : tensor([[0.4401]])\n",
      "Next State : [[-0.28811324]\n",
      " [-0.95759636]\n",
      " [-1.671711  ]]\n",
      "Reward : [-3.26749701]\n",
      "--------------------------------------------------\n",
      "Step : 57\n",
      "Current State : [-0.40100738 -0.9160748  -2.4072068 ]\n",
      "Currrent Action : tensor([[-0.1153]])\n",
      "Next State : [[-0.40100738]\n",
      " [-0.9160748 ]\n",
      " [-2.4072068 ]]\n",
      "Reward : [-3.75043878]\n",
      "--------------------------------------------------\n",
      "Step : 58\n",
      "Current State : [-0.5246717  -0.85130465 -2.794263  ]\n",
      "Currrent Action : tensor([[2.1372]])\n",
      "Next State : [[-0.5246717 ]\n",
      " [-0.85130465]\n",
      " [-2.794263  ]]\n",
      "Reward : [-4.51738996]\n",
      "--------------------------------------------------\n",
      "Step : 59\n",
      "Current State : [-0.6651208 -0.7467358 -3.506523 ]\n",
      "Currrent Action : tensor([[-0.4919]])\n",
      "Next State : [[-0.6651208]\n",
      " [-0.7467358]\n",
      " [-3.506523 ]]\n",
      "Reward : [-5.28869533]\n",
      "--------------------------------------------------\n",
      "Step : 60\n",
      "Current State : [-0.79839075 -0.6021397  -3.9392471 ]\n",
      "Currrent Action : tensor([[0.8489]])\n",
      "Next State : [[-0.79839075]\n",
      " [-0.6021397 ]\n",
      " [-3.9392471 ]]\n",
      "Reward : [-6.51317192]\n",
      "--------------------------------------------------\n",
      "Step : 61\n",
      "Current State : [-0.9083012  -0.41831678 -4.2917404 ]\n",
      "Currrent Action : tensor([[0.6607]])\n",
      "Next State : [[-0.9083012 ]\n",
      " [-0.41831678]\n",
      " [-4.2917404 ]]\n",
      "Reward : [-7.77929555]\n",
      "--------------------------------------------------\n",
      "Step : 62\n",
      "Current State : [-0.97870415 -0.20527583 -4.4969172 ]\n",
      "Currrent Action : tensor([[0.7237]])\n",
      "Next State : [[-0.97870415]\n",
      " [-0.20527583]\n",
      " [-4.4969172 ]]\n",
      "Reward : [-9.18653435]\n",
      "--------------------------------------------------\n",
      "Step : 63\n",
      "Current State : [-0.99933344  0.03650576 -4.865188  ]\n",
      "Currrent Action : tensor([[-1.4288]])\n",
      "Next State : [[-0.99933344]\n",
      " [ 0.03650576]\n",
      " [-4.865188  ]]\n",
      "Reward : [-10.63759528]\n",
      "--------------------------------------------------\n",
      "Step : 64\n",
      "Current State : [-0.96192735  0.27330524 -4.806271  ]\n",
      "Currrent Action : tensor([[0.2103]])\n",
      "Next State : [[-0.96192735]\n",
      " [ 0.27330524]\n",
      " [-4.806271  ]]\n",
      "Reward : [-12.00856399]\n",
      "--------------------------------------------------\n",
      "Step : 65\n",
      "Current State : [-0.8780554   0.47855905 -4.44371   ]\n",
      "Currrent Action : tensor([[1.0505]])\n",
      "Next State : [[-0.8780554 ]\n",
      " [ 0.47855905]\n",
      " [-4.44371   ]]\n",
      "Reward : [-10.51800753]\n",
      "--------------------------------------------------\n",
      "Step : 66\n",
      "Current State : [-0.7723572  0.6351884 -3.7847905]\n",
      "Currrent Action : tensor([[2.7773]])\n",
      "Next State : [[-0.7723572]\n",
      " [ 0.6351884]\n",
      " [-3.7847905]]\n",
      "Reward : [-8.96188345]\n",
      "--------------------------------------------------\n",
      "Step : 67\n",
      "Current State : [-0.66577446  0.746153   -3.0802512 ]\n",
      "Currrent Action : tensor([[1.5210]])\n",
      "Next State : [[-0.66577446]\n",
      " [ 0.746153  ]\n",
      " [-3.0802512 ]]\n",
      "Reward : [-7.45365558]\n",
      "--------------------------------------------------\n",
      "Step : 68\n",
      "Current State : [-0.567053   0.8236813 -2.5121543]\n",
      "Currrent Action : tensor([[0.0565]])\n",
      "Next State : [[-0.567053 ]\n",
      " [ 0.8236813]\n",
      " [-2.5121543]]\n",
      "Reward : [-6.23570561]\n",
      "--------------------------------------------------\n",
      "Step : 69\n",
      "Current State : [-0.48685282  0.873484   -1.8888098 ]\n",
      "Currrent Action : tensor([[0.0372]])\n",
      "Next State : [[-0.48685282]\n",
      " [ 0.873484  ]\n",
      " [-1.8888098 ]]\n",
      "Reward : [-5.35615166]\n",
      "--------------------------------------------------\n",
      "Step : 70\n",
      "Current State : [-0.42391956  0.90569985 -1.4142901 ]\n",
      "Currrent Action : tensor([[-1.2040]])\n",
      "Next State : [[-0.42391956]\n",
      " [ 0.90569985]\n",
      " [-1.4142901 ]]\n",
      "Reward : [-4.68161276]\n",
      "--------------------------------------------------\n",
      "Step : 71\n",
      "Current State : [-0.38605604  0.92247534 -0.82832557]\n",
      "Currrent Action : tensor([[-0.6221]])\n",
      "Next State : [[-0.38605604]\n",
      " [ 0.92247534]\n",
      " [-0.82832557]]\n",
      "Reward : [-4.23474171]\n",
      "--------------------------------------------------\n",
      "Step : 72\n",
      "Current State : [-0.3879192   0.9216934   0.04041187]\n",
      "Currrent Action : tensor([[1.1792]])\n",
      "Next State : [[-0.3879192 ]\n",
      " [ 0.9216934 ]\n",
      " [ 0.04041187]]\n",
      "Reward : [-3.9396767]\n",
      "--------------------------------------------------\n",
      "Step : 73\n",
      "Current State : [-0.41938013  0.90781075  0.6877894 ]\n",
      "Currrent Action : tensor([[-0.2926]])\n",
      "Next State : [[-0.41938013]\n",
      " [ 0.90781075]\n",
      " [ 0.6877894 ]]\n",
      "Reward : [-3.87787649]\n",
      "--------------------------------------------------\n",
      "Step : 74\n",
      "Current State : [-0.4877429   0.87298733  1.5348002 ]\n",
      "Currrent Action : tensor([[1.1077]])\n",
      "Next State : [[-0.4877429 ]\n",
      " [ 0.87298733]\n",
      " [ 1.5348002 ]]\n",
      "Reward : [-4.06277994]\n",
      "--------------------------------------------------\n",
      "Step : 75\n",
      "Current State : [-0.5901267   0.80731064  2.4342678 ]\n",
      "Currrent Action : tensor([[1.6315]])\n",
      "Next State : [[-0.5901267 ]\n",
      " [ 0.80731064]\n",
      " [ 2.4342678 ]]\n",
      "Reward : [-4.56586579]\n",
      "--------------------------------------------------\n",
      "Step : 76\n",
      "Current State : [-0.70082253  0.71333563  2.906683  ]\n",
      "Currrent Action : tensor([[-0.8871]])\n",
      "Next State : [[-0.70082253]\n",
      " [ 0.71333563]\n",
      " [ 2.906683  ]]\n",
      "Reward : [-5.44221031]\n",
      "--------------------------------------------------\n",
      "Step : 77\n",
      "Current State : [-0.8212704  0.5705391  3.7416847]\n",
      "Currrent Action : tensor([[2.5140]])\n",
      "Next State : [[-0.8212704]\n",
      " [ 0.5705391]\n",
      " [ 3.7416847]]\n",
      "Reward : [-6.35891507]\n",
      "--------------------------------------------------\n",
      "Step : 78\n",
      "Current State : [-0.91763765  0.39741808  3.969214  ]\n",
      "Currrent Action : tensor([[-1.3358]])\n",
      "Next State : [[-0.91763765]\n",
      " [ 0.39741808]\n",
      " [ 3.969214  ]]\n",
      "Reward : [-7.82514286]\n",
      "--------------------------------------------------\n",
      "Step : 79\n",
      "Current State : [-0.98069435  0.19554678  4.23773   ]\n",
      "Currrent Action : tensor([[-0.1970]])\n",
      "Next State : [[-0.98069435]\n",
      " [ 0.19554678]\n",
      " [ 4.23773   ]]\n",
      "Reward : [-9.044199]\n",
      "--------------------------------------------------\n",
      "Step : 80\n",
      "Current State : [-0.99955624 -0.02978732  4.5321336 ]\n",
      "Currrent Action : tensor([[0.9850]])\n",
      "Next State : [[-0.99955624]\n",
      " [-0.02978732]\n",
      " [ 4.5321336 ]]\n",
      "Reward : [-10.46852119]\n",
      "--------------------------------------------------\n",
      "Step : 81\n",
      "Current State : [-0.9700727  -0.24281469  4.309492  ]\n",
      "Currrent Action : tensor([[-1.3353]])\n",
      "Next State : [[-0.9700727 ]\n",
      " [-0.24281469]\n",
      " [ 4.309492  ]]\n",
      "Reward : [-11.7391118]\n",
      "--------------------------------------------------\n",
      "Step : 82\n",
      "Current State : [-0.9001057  -0.43567157  4.110358  ]\n",
      "Currrent Action : tensor([[-0.1135]])\n",
      "Next State : [[-0.9001057 ]\n",
      " [-0.43567157]\n",
      " [ 4.110358  ]]\n",
      "Reward : [-10.24589123]\n",
      "--------------------------------------------------\n",
      "Step : 83\n",
      "Current State : [-0.8006172 -0.5991761  3.8337486]\n",
      "Currrent Action : tensor([[0.3343]])\n",
      "Next State : [[-0.8006172]\n",
      " [-0.5991761]\n",
      " [ 3.8337486]]\n",
      "Reward : [-8.93006618]\n",
      "--------------------------------------------------\n",
      "Step : 84\n",
      "Current State : [-0.6826908  -0.73070735  3.5377173 ]\n",
      "Currrent Action : tensor([[1.0223]])\n",
      "Next State : [[-0.6826908 ]\n",
      " [-0.73070735]\n",
      " [ 3.5377173 ]]\n",
      "Reward : [-7.71641367]\n",
      "--------------------------------------------------\n",
      "Step : 85\n",
      "Current State : [-0.57627547 -0.8172555   2.7454965 ]\n",
      "Currrent Action : tensor([[-1.6279]])\n",
      "Next State : [[-0.57627547]\n",
      " [-0.8172555 ]\n",
      " [ 2.7454965 ]]\n",
      "Reward : [-6.64697059]\n",
      "--------------------------------------------------\n",
      "Step : 86\n",
      "Current State : [-0.4899204  -0.87176716  2.0433106 ]\n",
      "Currrent Action : tensor([[-0.5950]])\n",
      "Next State : [[-0.4899204 ]\n",
      " [-0.87176716]\n",
      " [ 2.0433106 ]]\n",
      "Reward : [-5.52818062]\n",
      "--------------------------------------------------\n",
      "Step : 87\n",
      "Current State : [-0.43671098 -0.8996019   1.2011822 ]\n",
      "Currrent Action : tensor([[-1.2554]])\n",
      "Next State : [[-0.43671098]\n",
      " [-0.8996019 ]\n",
      " [ 1.2011822 ]]\n",
      "Reward : [-4.75712182]\n",
      "--------------------------------------------------\n",
      "Step : 88\n",
      "Current State : [-0.4126512 -0.9108891  0.531532 ]\n",
      "Currrent Action : tensor([[0.0337]])\n",
      "Next State : [[-0.4126512]\n",
      " [-0.9108891]\n",
      " [ 0.531532 ]]\n",
      "Reward : [-4.23574454]\n",
      "--------------------------------------------------\n",
      "Step : 89\n",
      "Current State : [-0.40671095 -0.9135569   0.13023634]\n",
      "Currrent Action : tensor([[1.8791]])\n",
      "Next State : [[-0.40671095]\n",
      " [-0.9135569 ]\n",
      " [ 0.13023634]]\n",
      "Reward : [-4.01643476]\n",
      "--------------------------------------------------\n",
      "Step : 90\n",
      "Current State : [-0.42908645 -0.90326345 -0.4926045 ]\n",
      "Currrent Action : tensor([[0.4155]])\n",
      "Next State : [[-0.42908645]\n",
      " [-0.90326345]\n",
      " [-0.4926045 ]]\n",
      "Reward : [-3.96056492]\n",
      "--------------------------------------------------\n",
      "Step : 91\n",
      "Current State : [-0.48495457 -0.8745394  -1.2566013 ]\n",
      "Currrent Action : tensor([[-0.5770]])\n",
      "Next State : [[-0.48495457]\n",
      " [-0.8745394 ]\n",
      " [-1.2566013 ]]\n",
      "Reward : [-4.08191253]\n",
      "--------------------------------------------------\n",
      "Step : 92\n",
      "Current State : [-0.5614308 -0.8275237 -1.7960522]\n",
      "Currrent Action : tensor([[0.7764]])\n",
      "Next State : [[-0.5614308]\n",
      " [-0.8275237]\n",
      " [-1.7960522]]\n",
      "Reward : [-4.47288312]\n",
      "--------------------------------------------------\n",
      "Step : 93\n",
      "Current State : [-0.6570042  -0.75388694 -2.4144845 ]\n",
      "Currrent Action : tensor([[0.0147]])\n",
      "Next State : [[-0.6570042 ]\n",
      " [-0.75388694]\n",
      " [-2.4144845 ]]\n",
      "Reward : [-5.01808012]\n",
      "--------------------------------------------------\n",
      "Step : 94\n",
      "Current State : [-0.7622204 -0.6473176 -2.9979668]\n",
      "Currrent Action : tensor([[-0.1204]])\n",
      "Next State : [[-0.7622204]\n",
      " [-0.6473176]\n",
      " [-2.9979668]]\n",
      "Reward : [-5.816259]\n",
      "--------------------------------------------------\n",
      "Step : 95\n",
      "Current State : [-0.85824597 -0.51323855 -3.3021188 ]\n",
      "Currrent Action : tensor([[1.2089]])\n",
      "Next State : [[-0.85824597]\n",
      " [-0.51323855]\n",
      " [-3.3021188 ]]\n",
      "Reward : [-6.84180762]\n",
      "--------------------------------------------------\n",
      "Step : 96\n",
      "Current State : [-0.9391077  -0.34362295 -3.7636387 ]\n",
      "Currrent Action : tensor([[-0.5106]])\n",
      "Next State : [[-0.9391077 ]\n",
      " [-0.34362295]\n",
      " [-3.7636387 ]]\n",
      "Reward : [-7.86438755]\n",
      "--------------------------------------------------\n",
      "Step : 97\n",
      "Current State : [-0.98796415 -0.15468304 -3.9093087 ]\n",
      "Currrent Action : tensor([[0.7470]])\n",
      "Next State : [[-0.98796415]\n",
      " [-0.15468304]\n",
      " [-3.9093087 ]]\n",
      "Reward : [-9.20573525]\n",
      "--------------------------------------------------\n",
      "Step : 98\n",
      "Current State : [-0.9988792   0.04733184 -4.0531235 ]\n",
      "Currrent Action : tensor([[-0.1853]])\n",
      "Next State : [[-0.9988792 ]\n",
      " [ 0.04733184]\n",
      " [-4.0531235 ]]\n",
      "Reward : [-10.44620805]\n",
      "--------------------------------------------------\n",
      "Step : 99\n",
      "Current State : [-0.96902347  0.24696878 -4.044027  ]\n",
      "Currrent Action : tensor([[-0.1760]])\n",
      "Next State : [[-0.96902347]\n",
      " [ 0.24696878]\n",
      " [-4.044027  ]]\n",
      "Reward : [-11.21715228]\n",
      "learning iteration : 18\n",
      "--------------------------------------------------\n",
      "Step : 0\n",
      "Current State : [ 0.9998651  -0.01642289 -0.33039305]\n",
      "Currrent Action : tensor([[-1.1049]])\n",
      "Next State : [[ 0.9998651 ]\n",
      " [-0.01642289]\n",
      " [-0.33039305]]\n",
      "Reward : [-0.00393432]\n",
      "--------------------------------------------------\n",
      "Step : 1\n",
      "Current State : [ 0.99909633 -0.04250282 -0.5218399 ]\n",
      "Currrent Action : tensor([[-1.1942]])\n",
      "Next State : [[ 0.99909633]\n",
      " [-0.04250282]\n",
      " [-0.5218399 ]]\n",
      "Reward : [-0.0126118]\n",
      "--------------------------------------------------\n",
      "Step : 2\n",
      "Current State : [ 0.9967355  -0.08073574 -0.7661616 ]\n",
      "Currrent Action : tensor([[-1.4163]])\n",
      "Next State : [[ 0.9967355 ]\n",
      " [-0.08073574]\n",
      " [-0.7661616 ]]\n",
      "Reward : [-0.03104517]\n",
      "--------------------------------------------------\n",
      "Step : 3\n",
      "Current State : [ 0.9929184  -0.11879862 -0.7651228 ]\n",
      "Currrent Action : tensor([[0.4106]])\n",
      "Next State : [[ 0.9929184 ]\n",
      " [-0.11879862]\n",
      " [-0.7651228 ]]\n",
      "Reward : [-0.06540143]\n",
      "--------------------------------------------------\n",
      "Step : 4\n",
      "Current State : [ 0.9869197  -0.16121247 -0.8567845 ]\n",
      "Currrent Action : tensor([[-0.0171]])\n",
      "Next State : [[ 0.9869197 ]\n",
      " [-0.16121247]\n",
      " [-0.8567845 ]]\n",
      "Reward : [-0.07272159]\n",
      "--------------------------------------------------\n",
      "Step : 5\n",
      "Current State : [ 0.9767173 -0.2145306 -1.085843 ]\n",
      "Currrent Action : tensor([[-0.7210]])\n",
      "Next State : [[ 0.9767173]\n",
      " [-0.2145306]\n",
      " [-1.085843 ]]\n",
      "Reward : [-0.10014558]\n",
      "--------------------------------------------------\n",
      "Step : 6\n",
      "Current State : [ 0.9600652  -0.27977633 -1.3469985 ]\n",
      "Currrent Action : tensor([[-0.6684]])\n",
      "Next State : [[ 0.9600652 ]\n",
      " [-0.27977633]\n",
      " [-1.3469985 ]]\n",
      "Reward : [-0.16509952]\n",
      "--------------------------------------------------\n",
      "Step : 7\n",
      "Current State : [ 0.93142265 -0.3639393  -1.7786517 ]\n",
      "Currrent Action : tensor([[-1.4788]])\n",
      "Next State : [[ 0.93142265]\n",
      " [-0.3639393 ]\n",
      " [-1.7786517 ]]\n",
      "Reward : [-0.26403427]\n",
      "--------------------------------------------------\n",
      "Step : 8\n",
      "Current State : [ 0.8875721  -0.46066883 -2.125098  ]\n",
      "Currrent Action : tensor([[-0.4899]])\n",
      "Next State : [[ 0.8875721 ]\n",
      " [-0.46066883]\n",
      " [-2.125098  ]]\n",
      "Reward : [-0.4553518]\n",
      "--------------------------------------------------\n",
      "Step : 9\n",
      "Current State : [ 0.82426333 -0.56620663 -2.4629562 ]\n",
      "Currrent Action : tensor([[0.0510]])\n",
      "Next State : [[ 0.82426333]\n",
      " [-0.56620663]\n",
      " [-2.4629562 ]]\n",
      "Reward : [-0.68080695]\n",
      "--------------------------------------------------\n",
      "Step : 10\n",
      "Current State : [ 0.731992  -0.6813132 -2.9531755]\n",
      "Currrent Action : tensor([[-0.4371]])\n",
      "Next State : [[ 0.731992 ]\n",
      " [-0.6813132]\n",
      " [-2.9531755]]\n",
      "Reward : [-0.96908568]\n",
      "--------------------------------------------------\n",
      "Step : 11\n",
      "Current State : [ 0.6082933 -0.7937124 -3.3466523]\n",
      "Currrent Action : tensor([[0.7834]])\n",
      "Next State : [[ 0.6082933]\n",
      " [-0.7937124]\n",
      " [-3.3466523]]\n",
      "Reward : [-1.43457132]\n",
      "--------------------------------------------------\n",
      "Step : 12\n",
      "Current State : [ 0.4545008  -0.89074636 -3.6419365 ]\n",
      "Currrent Action : tensor([[2.0817]])\n",
      "Next State : [[ 0.4545008 ]\n",
      " [-0.89074636]\n",
      " [-3.6419365 ]]\n",
      "Reward : [-1.96469138]\n",
      "--------------------------------------------------\n",
      "Step : 13\n",
      "Current State : [ 0.24532269 -0.9694415  -4.4791827 ]\n",
      "Currrent Action : tensor([[-1.1279]])\n",
      "Next State : [[ 0.24532269]\n",
      " [-0.9694415 ]\n",
      " [-4.4791827 ]]\n",
      "Reward : [-2.53540956]\n",
      "--------------------------------------------------\n",
      "Step : 14\n",
      "Current State : [-0.00567205 -0.9999839  -5.0704923 ]\n",
      "Currrent Action : tensor([[0.9051]])\n",
      "Next State : [[-0.00567205]\n",
      " [-0.9999839 ]\n",
      " [-5.0704923 ]]\n",
      "Reward : [-3.75730749]\n",
      "--------------------------------------------------\n",
      "Step : 15\n",
      "Current State : [-0.28739268 -0.95781285 -5.7166295 ]\n",
      "Currrent Action : tensor([[0.6923]])\n",
      "Next State : [[-0.28739268]\n",
      " [-0.95781285]\n",
      " [-5.7166295 ]]\n",
      "Reward : [-5.05672125]\n",
      "--------------------------------------------------\n",
      "Step : 16\n",
      "Current State : [-0.5724137 -0.819965  -6.358858 ]\n",
      "Currrent Action : tensor([[0.5075]])\n",
      "Next State : [[-0.5724137]\n",
      " [-0.819965 ]\n",
      " [-6.358858 ]]\n",
      "Reward : [-6.73640395]\n",
      "--------------------------------------------------\n",
      "Step : 17\n",
      "Current State : [-0.8094049  -0.58725095 -6.673832  ]\n",
      "Currrent Action : tensor([[2.2026]])\n",
      "Next State : [[-0.8094049 ]\n",
      " [-0.58725095]\n",
      " [-6.673832  ]]\n",
      "Reward : [-8.80096644]\n",
      "--------------------------------------------------\n",
      "Step : 18\n",
      "Current State : [-0.9603678 -0.2787359 -6.9036136]\n",
      "Currrent Action : tensor([[1.4044]])\n",
      "Next State : [[-0.9603678]\n",
      " [-0.2787359]\n",
      " [-6.9036136]]\n",
      "Reward : [-10.77584172]\n",
      "--------------------------------------------------\n",
      "Step : 19\n",
      "Current State : [-0.9974191   0.07179881 -7.0867643 ]\n",
      "Currrent Action : tensor([[0.1727]])\n",
      "Next State : [[-0.9974191 ]\n",
      " [ 0.07179881]\n",
      " [-7.0867643 ]]\n",
      "Reward : [-12.94055658]\n",
      "--------------------------------------------------\n",
      "Step : 20\n",
      "Current State : [-0.91209424  0.40998057 -7.011442  ]\n",
      "Currrent Action : tensor([[0.1432]])\n",
      "Next State : [[-0.91209424]\n",
      " [ 0.40998057]\n",
      " [-7.011442  ]]\n",
      "Reward : [-14.44549828]\n",
      "--------------------------------------------------\n",
      "Step : 21\n",
      "Current State : [-0.7320441   0.68125725 -6.5409203 ]\n",
      "Currrent Action : tensor([[1.0869]])\n",
      "Next State : [[-0.7320441 ]\n",
      " [ 0.68125725]\n",
      " [-6.5409203 ]]\n",
      "Reward : [-12.31104408]\n",
      "--------------------------------------------------\n",
      "Step : 22\n",
      "Current State : [-0.5096851   0.86036104 -5.729977  ]\n",
      "Currrent Action : tensor([[2.1916]])\n",
      "Next State : [[-0.5096851 ]\n",
      " [ 0.86036104]\n",
      " [-5.729977  ]]\n",
      "Reward : [-10.0045725]\n",
      "--------------------------------------------------\n",
      "Step : 23\n",
      "Current State : [-0.28199184  0.9594168  -4.978983  ]\n",
      "Currrent Action : tensor([[0.7048]])\n",
      "Next State : [[-0.28199184]\n",
      " [ 0.9594168 ]\n",
      " [-4.978983  ]]\n",
      "Reward : [-7.7173753]\n",
      "--------------------------------------------------\n",
      "Step : 24\n",
      "Current State : [-0.08189777  0.99664074 -4.0775995 ]\n",
      "Currrent Action : tensor([[1.2121]])\n",
      "Next State : [[-0.08189777]\n",
      " [ 0.99664074]\n",
      " [-4.0775995 ]]\n",
      "Reward : [-5.92770451]\n",
      "--------------------------------------------------\n",
      "Step : 25\n",
      "Current State : [ 0.08107041  0.9967084  -3.2629814 ]\n",
      "Currrent Action : tensor([[0.4476]])\n",
      "Next State : [[ 0.08107041]\n",
      " [ 0.9967084 ]\n",
      " [-3.2629814 ]]\n",
      "Reward : [-4.39458344]\n",
      "--------------------------------------------------\n",
      "Step : 26\n",
      "Current State : [ 0.19085947  0.9816174  -2.2175634 ]\n",
      "Currrent Action : tensor([[1.9859]])\n",
      "Next State : [[ 0.19085947]\n",
      " [ 0.9816174 ]\n",
      " [-2.2175634 ]]\n",
      "Reward : [-3.28766664]\n",
      "--------------------------------------------------\n",
      "Step : 27\n",
      "Current State : [ 0.2524173   0.96761847 -1.2627997 ]\n",
      "Currrent Action : tensor([[1.4570]])\n",
      "Next State : [[ 0.2524173 ]\n",
      " [ 0.96761847]\n",
      " [-1.2627997 ]]\n",
      "Reward : [-2.39485708]\n",
      "--------------------------------------------------\n",
      "Step : 28\n",
      "Current State : [ 0.27891612  0.96031547 -0.54975283]\n",
      "Currrent Action : tensor([[-0.0844]])\n",
      "Next State : [[ 0.27891612]\n",
      " [ 0.96031547]\n",
      " [-0.54975283]]\n",
      "Reward : [-1.89032601]\n",
      "--------------------------------------------------\n",
      "Step : 29\n",
      "Current State : [0.26870236 0.9632233  0.21239378]\n",
      "Currrent Action : tensor([[0.2794]])\n",
      "Next State : [[0.26870236]\n",
      " [0.9632233 ]\n",
      " [0.21239378]]\n",
      "Reward : [-1.68958251]\n",
      "--------------------------------------------------\n",
      "Step : 30\n",
      "Current State : [0.22259317 0.97491145 0.9514402 ]\n",
      "Currrent Action : tensor([[0.1109]])\n",
      "Next State : [[0.22259317]\n",
      " [0.97491145]\n",
      " [0.9514402 ]]\n",
      "Reward : [-1.69127691]\n",
      "--------------------------------------------------\n",
      "Step : 31\n",
      "Current State : [0.1489265  0.98884827 1.49982   ]\n",
      "Currrent Action : tensor([[-1.2187]])\n",
      "Next State : [[0.1489265 ]\n",
      " [0.98884827]\n",
      " [1.49982   ]]\n",
      "Reward : [-1.90459402]\n",
      "--------------------------------------------------\n",
      "Step : 32\n",
      "Current State : [0.0266134 0.9996458 2.4573205]\n",
      "Currrent Action : tensor([[1.4391]])\n",
      "Next State : [[0.0266134]\n",
      " [0.9996458]\n",
      " [2.4573205]]\n",
      "Reward : [-2.24714981]\n",
      "--------------------------------------------------\n",
      "Step : 33\n",
      "Current State : [-0.14348818  0.98965204  3.4120345 ]\n",
      "Currrent Action : tensor([[1.3665]])\n",
      "Next State : [[-0.14348818]\n",
      " [ 0.98965204]\n",
      " [ 3.4120345 ]]\n",
      "Reward : [-2.99020099]\n",
      "--------------------------------------------------\n",
      "Step : 34\n",
      "Current State : [-0.33988988  0.9404652   4.0562925 ]\n",
      "Currrent Action : tensor([[-0.6532]])\n",
      "Next State : [[-0.33988988]\n",
      " [ 0.9404652 ]\n",
      " [ 4.0562925 ]]\n",
      "Reward : [-4.10510033]\n",
      "--------------------------------------------------\n",
      "Step : 35\n",
      "Current State : [-0.54969805  0.8353634   4.704059  ]\n",
      "Currrent Action : tensor([[-0.3839]])\n",
      "Next State : [[-0.54969805]\n",
      " [ 0.8353634 ]\n",
      " [ 4.704059  ]]\n",
      "Reward : [-5.3226734]\n",
      "--------------------------------------------------\n",
      "Step : 36\n",
      "Current State : [-0.743899    0.66829205  5.1376615 ]\n",
      "Currrent Action : tensor([[-1.2861]])\n",
      "Next State : [[-0.743899  ]\n",
      " [ 0.66829205]\n",
      " [ 5.1376615 ]]\n",
      "Reward : [-6.84901521]\n",
      "--------------------------------------------------\n",
      "Step : 37\n",
      "Current State : [-0.9029232   0.42980185  5.752743  ]\n",
      "Currrent Action : tensor([[0.7591]])\n",
      "Next State : [[-0.9029232 ]\n",
      " [ 0.42980185]\n",
      " [ 5.752743  ]]\n",
      "Reward : [-8.44670118]\n",
      "--------------------------------------------------\n",
      "Step : 38\n",
      "Current State : [-0.9894484   0.14488542  5.9775233 ]\n",
      "Currrent Action : tensor([[-0.6505]])\n",
      "Next State : [[-0.9894484 ]\n",
      " [ 0.14488542]\n",
      " [ 5.9775233 ]]\n",
      "Reward : [-10.58535976]\n",
      "--------------------------------------------------\n",
      "Step : 39\n",
      "Current State : [-0.9867209  -0.16242485  6.170896  ]\n",
      "Currrent Action : tensor([[0.5647]])\n",
      "Next State : [[-0.9867209 ]\n",
      " [-0.16242485]\n",
      " [ 6.170896  ]]\n",
      "Reward : [-12.55058472]\n",
      "--------------------------------------------------\n",
      "Step : 40\n",
      "Current State : [-0.8985056  -0.43896198  5.8259087 ]\n",
      "Currrent Action : tensor([[-1.4878]])\n",
      "Next State : [[-0.8985056 ]\n",
      " [-0.43896198]\n",
      " [ 5.8259087 ]]\n",
      "Reward : [-12.68134396]\n",
      "--------------------------------------------------\n",
      "Step : 41\n",
      "Current State : [-0.7520901 -0.6590603  5.3025117]\n",
      "Currrent Action : tensor([[-1.2945]])\n",
      "Next State : [[-0.7520901]\n",
      " [-0.6590603]\n",
      " [ 5.3025117]]\n",
      "Reward : [-10.61656989]\n",
      "--------------------------------------------------\n",
      "Step : 42\n",
      "Current State : [-0.56118506 -0.82769036  5.1082163 ]\n",
      "Currrent Action : tensor([[2.1059]])\n",
      "Next State : [[-0.56118506]\n",
      " [-0.82769036]\n",
      " [ 5.1082163 ]]\n",
      "Reward : [-8.68186331]\n",
      "--------------------------------------------------\n",
      "Step : 43\n",
      "Current State : [-0.36476475 -0.9310997   4.4487343 ]\n",
      "Currrent Action : tensor([[-0.2581]])\n",
      "Next State : [[-0.36476475]\n",
      " [-0.9310997 ]\n",
      " [ 4.4487343 ]]\n",
      "Reward : [-7.30366683]\n",
      "--------------------------------------------------\n",
      "Step : 44\n",
      "Current State : [-0.17909816 -0.9838312   3.8662086 ]\n",
      "Currrent Action : tensor([[0.7720]])\n",
      "Next State : [[-0.17909816]\n",
      " [-0.9838312 ]\n",
      " [ 3.8662086 ]]\n",
      "Reward : [-5.7595417]\n",
      "--------------------------------------------------\n",
      "Step : 45\n",
      "Current State : [-0.02071519 -0.9997854   3.1870608 ]\n",
      "Currrent Action : tensor([[0.3915]])\n",
      "Next State : [[-0.02071519]\n",
      " [-0.9997854 ]\n",
      " [ 3.1870608 ]]\n",
      "Reward : [-4.56044193]\n",
      "--------------------------------------------------\n",
      "Step : 46\n",
      "Current State : [ 0.1090126  -0.99404037  2.5989268 ]\n",
      "Currrent Action : tensor([[1.0780]])\n",
      "Next State : [[ 0.1090126 ]\n",
      " [-0.99404037]\n",
      " [ 2.5989268 ]]\n",
      "Reward : [-3.54981142]\n",
      "--------------------------------------------------\n",
      "Step : 47\n",
      "Current State : [ 0.20374335 -0.97902435  1.919006  ]\n",
      "Currrent Action : tensor([[0.4374]])\n",
      "Next State : [[ 0.20374335]\n",
      " [-0.97902435]\n",
      " [ 1.919006  ]]\n",
      "Reward : [-2.81181048]\n",
      "--------------------------------------------------\n",
      "Step : 48\n",
      "Current State : [ 0.26531485 -0.9641618   1.2670096 ]\n",
      "Currrent Action : tensor([[0.5485]])\n",
      "Next State : [[ 0.26531485]\n",
      " [-0.9641618 ]\n",
      " [ 1.2670096 ]]\n",
      "Reward : [-2.23346727]\n",
      "--------------------------------------------------\n",
      "Step : 49\n",
      "Current State : [ 0.2943907  -0.95568514  0.6057492 ]\n",
      "Currrent Action : tensor([[0.4124]])\n",
      "Next State : [[ 0.2943907 ]\n",
      " [-0.95568514]\n",
      " [ 0.6057492 ]]\n",
      "Reward : [-1.85659783]\n",
      "--------------------------------------------------\n",
      "Step : 50\n",
      "Current State : [ 0.28201848 -0.959409   -0.25841135]\n",
      "Currrent Action : tensor([[-0.9826]])\n",
      "Next State : [[ 0.28201848]\n",
      " [-0.959409  ]\n",
      " [-0.25841135]]\n",
      "Reward : [-1.65558789]\n",
      "--------------------------------------------------\n",
      "Step : 51\n",
      "Current State : [ 0.2403175  -0.97069436 -0.8640886 ]\n",
      "Currrent Action : tensor([[0.7592]])\n",
      "Next State : [[ 0.2403175 ]\n",
      " [-0.97069436]\n",
      " [-0.8640886 ]]\n",
      "Reward : [-1.65821942]\n",
      "--------------------------------------------------\n",
      "Step : 52\n",
      "Current State : [ 0.17085478 -0.98529625 -1.4199154 ]\n",
      "Currrent Action : tensor([[1.1480]])\n",
      "Next State : [[ 0.17085478]\n",
      " [-0.98529625]\n",
      " [-1.4199154 ]]\n",
      "Reward : [-1.83984139]\n",
      "--------------------------------------------------\n",
      "Step : 53\n",
      "Current State : [ 0.06302781 -0.99801177 -2.1725504 ]\n",
      "Currrent Action : tensor([[-0.0911]])\n",
      "Next State : [[ 0.06302781]\n",
      " [-0.99801177]\n",
      " [-2.1725504 ]]\n",
      "Reward : [-2.15910282]\n",
      "--------------------------------------------------\n",
      "Step : 54\n",
      "Current State : [-0.06987414 -0.9975558  -2.6600149 ]\n",
      "Currrent Action : tensor([[1.7403]])\n",
      "Next State : [[-0.06987414]\n",
      " [-0.9975558 ]\n",
      " [-2.6600149 ]]\n",
      "Reward : [-2.748266]\n",
      "--------------------------------------------------\n",
      "Step : 55\n",
      "Current State : [-0.23275085 -0.9725364  -3.2994828 ]\n",
      "Currrent Action : tensor([[0.7247]])\n",
      "Next State : [[-0.23275085]\n",
      " [-0.9725364 ]\n",
      " [-3.2994828 ]]\n",
      "Reward : [-3.40007954]\n",
      "--------------------------------------------------\n",
      "Step : 56\n",
      "Current State : [-0.41407    -0.91024506 -3.8403125 ]\n",
      "Currrent Action : tensor([[1.2571]])\n",
      "Next State : [[-0.41407   ]\n",
      " [-0.91024506]\n",
      " [-3.8403125 ]]\n",
      "Reward : [-4.35079727]\n",
      "--------------------------------------------------\n",
      "Step : 57\n",
      "Current State : [-0.59939045 -0.80045676 -4.3163705 ]\n",
      "Currrent Action : tensor([[1.3775]])\n",
      "Next State : [[-0.59939045]\n",
      " [-0.80045676]\n",
      " [-4.3163705 ]]\n",
      "Reward : [-5.46757163]\n",
      "--------------------------------------------------\n",
      "Step : 58\n",
      "Current State : [-0.7705096  -0.63742834 -4.738017  ]\n",
      "Currrent Action : tensor([[1.1913]])\n",
      "Next State : [[-0.7705096 ]\n",
      " [-0.63742834]\n",
      " [-4.738017  ]]\n",
      "Reward : [-6.76426494]\n",
      "--------------------------------------------------\n",
      "Step : 59\n",
      "Current State : [-0.9086543  -0.41754934 -5.2081885 ]\n",
      "Currrent Action : tensor([[0.0527]])\n",
      "Next State : [[-0.9086543 ]\n",
      " [-0.41754934]\n",
      " [-5.2081885 ]]\n",
      "Reward : [-8.24952289]\n",
      "--------------------------------------------------\n",
      "Step : 60\n",
      "Current State : [-0.98563904 -0.16886607 -5.2213507 ]\n",
      "Currrent Action : tensor([[2.4337]])\n",
      "Next State : [[-0.98563904]\n",
      " [-0.16886607]\n",
      " [-5.2213507 ]]\n",
      "Reward : [-10.06520913]\n",
      "--------------------------------------------------\n",
      "Step : 61\n",
      "Current State : [-0.9951779   0.09808644 -5.35847   ]\n",
      "Currrent Action : tensor([[-0.0698]])\n",
      "Next State : [[-0.9951779 ]\n",
      " [ 0.09808644]\n",
      " [-5.35847   ]]\n",
      "Reward : [-11.55852542]\n",
      "--------------------------------------------------\n",
      "Step : 62\n",
      "Current State : [-0.9318754  0.3627785 -5.460068 ]\n",
      "Currrent Action : tensor([[-1.1678]])\n",
      "Next State : [[-0.9318754]\n",
      " [ 0.3627785]\n",
      " [-5.460068 ]]\n",
      "Reward : [-12.13465229]\n",
      "--------------------------------------------------\n",
      "Step : 63\n",
      "Current State : [-0.8110013  0.5850443 -5.0737376]\n",
      "Currrent Action : tensor([[0.7616]])\n",
      "Next State : [[-0.8110013]\n",
      " [ 0.5850443]\n",
      " [-5.0737376]]\n",
      "Reward : [-10.65662519]\n",
      "--------------------------------------------------\n",
      "Step : 64\n",
      "Current State : [-0.6516388  0.7585294 -4.7223744]\n",
      "Currrent Action : tensor([[-0.5828]])\n",
      "Next State : [[-0.6516388]\n",
      " [ 0.7585294]\n",
      " [-4.7223744]]\n",
      "Reward : [-8.90818809]\n",
      "--------------------------------------------------\n",
      "Step : 65\n",
      "Current State : [-0.47835833  0.8781647  -4.2191715 ]\n",
      "Currrent Action : tensor([[-0.4380]])\n",
      "Next State : [[-0.47835833]\n",
      " [ 0.8781647 ]\n",
      " [-4.2191715 ]]\n",
      "Reward : [-7.43113316]\n",
      "--------------------------------------------------\n",
      "Step : 66\n",
      "Current State : [-0.31557593  0.94890034 -3.554416  ]\n",
      "Currrent Action : tensor([[0.0409]])\n",
      "Next State : [[-0.31557593]\n",
      " [ 0.94890034]\n",
      " [-3.554416  ]]\n",
      "Reward : [-6.06330663]\n",
      "--------------------------------------------------\n",
      "Step : 67\n",
      "Current State : [-0.17656296  0.98428935 -2.8714015 ]\n",
      "Currrent Action : tensor([[-0.1911]])\n",
      "Next State : [[-0.17656296]\n",
      " [ 0.98428935]\n",
      " [-2.8714015 ]]\n",
      "Reward : [-4.8425575]\n",
      "--------------------------------------------------\n",
      "Step : 68\n",
      "Current State : [-0.08507055  0.9963749  -1.8463992 ]\n",
      "Currrent Action : tensor([[1.9119]])\n",
      "Next State : [[-0.08507055]\n",
      " [ 0.9963749 ]\n",
      " [-1.8463992 ]]\n",
      "Reward : [-3.88466719]\n",
      "--------------------------------------------------\n",
      "Step : 69\n",
      "Current State : [-0.04031928  0.9991869  -0.8968658 ]\n",
      "Currrent Action : tensor([[1.3483]])\n",
      "Next State : [[-0.04031928]\n",
      " [ 0.9991869 ]\n",
      " [-0.8968658 ]]\n",
      "Reward : [-3.08497309]\n",
      "--------------------------------------------------\n",
      "Step : 70\n",
      "Current State : [-0.03208225  0.99948525 -0.164849  ]\n",
      "Currrent Action : tensor([[-0.1158]])\n",
      "Next State : [[-0.03208225]\n",
      " [ 0.99948525]\n",
      " [-0.164849  ]]\n",
      "Reward : [-2.67617895]\n",
      "--------------------------------------------------\n",
      "Step : 71\n",
      "Current State : [-0.06501107  0.9978846   0.65938383]\n",
      "Currrent Action : tensor([[0.4975]])\n",
      "Next State : [[-0.06501107]\n",
      " [ 0.9978846 ]\n",
      " [ 0.65938383]]\n",
      "Reward : [-2.57220238]\n",
      "--------------------------------------------------\n",
      "Step : 72\n",
      "Current State : [-0.14319243  0.9896949   1.5725876 ]\n",
      "Currrent Action : tensor([[1.0986]])\n",
      "Next State : [[-0.14319243]\n",
      " [ 0.9896949 ]\n",
      " [ 1.5725876 ]]\n",
      "Reward : [-2.72070158]\n",
      "--------------------------------------------------\n",
      "Step : 73\n",
      "Current State : [-0.27065632  0.96267605  2.607768  ]\n",
      "Currrent Action : tensor([[1.9527]])\n",
      "Next State : [[-0.27065632]\n",
      " [ 0.96267605]\n",
      " [ 2.607768  ]]\n",
      "Reward : [-3.19056711]\n",
      "--------------------------------------------------\n",
      "Step : 74\n",
      "Current State : [-0.43041217  0.9026325   3.4174926 ]\n",
      "Currrent Action : tensor([[0.5848]])\n",
      "Next State : [[-0.43041217]\n",
      " [ 0.9026325 ]\n",
      " [ 3.4174926 ]]\n",
      "Reward : [-4.08393665]\n",
      "--------------------------------------------------\n",
      "Step : 75\n",
      "Current State : [-0.5976592  0.8017503  3.9125783]\n",
      "Currrent Action : tensor([[-1.2126]])\n",
      "Next State : [[-0.5976592]\n",
      " [ 0.8017503]\n",
      " [ 3.9125783]]\n",
      "Reward : [-5.23262668]\n",
      "--------------------------------------------------\n",
      "Step : 76\n",
      "Current State : [-0.76042295  0.6494282   4.467716  ]\n",
      "Currrent Action : tensor([[-0.3078]])\n",
      "Next State : [[-0.76042295]\n",
      " [ 0.6494282 ]\n",
      " [ 4.467716  ]]\n",
      "Reward : [-6.42109929]\n",
      "--------------------------------------------------\n",
      "Step : 77\n",
      "Current State : [-0.8916591   0.45270747  4.740656  ]\n",
      "Currrent Action : tensor([[-1.4275]])\n",
      "Next State : [[-0.8916591 ]\n",
      " [ 0.45270747]\n",
      " [ 4.740656  ]]\n",
      "Reward : [-7.92614479]\n",
      "--------------------------------------------------\n",
      "Step : 78\n",
      "Current State : [-0.9779634   0.20877655  5.189513  ]\n",
      "Currrent Action : tensor([[0.7288]])\n",
      "Next State : [[-0.9779634 ]\n",
      " [ 0.20877655]\n",
      " [ 5.189513  ]]\n",
      "Reward : [-9.38639196]\n",
      "--------------------------------------------------\n",
      "Step : 79\n",
      "Current State : [-0.9990786  -0.04291927  5.065125  ]\n",
      "Currrent Action : tensor([[-1.8731]])\n",
      "Next State : [[-0.9990786 ]\n",
      " [-0.04291927]\n",
      " [ 5.065125  ]]\n",
      "Reward : [-11.28895079]\n",
      "--------------------------------------------------\n",
      "Step : 80\n",
      "Current State : [-0.95995325 -0.2801602   4.82057   ]\n",
      "Currrent Action : tensor([[-1.4158]])\n",
      "Next State : [[-0.95995325]\n",
      " [-0.2801602 ]\n",
      " [ 4.82057   ]]\n",
      "Reward : [-12.16924835]\n",
      "--------------------------------------------------\n",
      "Step : 81\n",
      "Current State : [-0.86818826 -0.49623495  4.705915  ]\n",
      "Currrent Action : tensor([[0.6364]])\n",
      "Next State : [[-0.86818826]\n",
      " [-0.49623495]\n",
      " [ 4.705915  ]]\n",
      "Reward : [-10.49025344]\n",
      "--------------------------------------------------\n",
      "Step : 82\n",
      "Current State : [-0.7349894 -0.6780786  4.5177717]\n",
      "Currrent Action : tensor([[1.2269]])\n",
      "Next State : [[-0.7349894]\n",
      " [-0.6780786]\n",
      " [ 4.5177717]]\n",
      "Reward : [-9.09271458]\n",
      "--------------------------------------------------\n",
      "Step : 83\n",
      "Current State : [-0.59341854 -0.80489403  3.807032  ]\n",
      "Currrent Action : tensor([[-1.3479]])\n",
      "Next State : [[-0.59341854]\n",
      " [-0.80489403]\n",
      " [ 3.807032  ]]\n",
      "Reward : [-7.78580275]\n",
      "--------------------------------------------------\n",
      "Step : 84\n",
      "Current State : [-0.46515787 -0.88522774  3.0297298 ]\n",
      "Currrent Action : tensor([[-1.1575]])\n",
      "Next State : [[-0.46515787]\n",
      " [-0.88522774]\n",
      " [ 3.0297298 ]]\n",
      "Reward : [-6.31754774]\n",
      "--------------------------------------------------\n",
      "Step : 85\n",
      "Current State : [-0.35660875 -0.9342538   2.3835495 ]\n",
      "Currrent Action : tensor([[0.1183]])\n",
      "Next State : [[-0.35660875]\n",
      " [-0.9342538 ]\n",
      " [ 2.3835495 ]]\n",
      "Reward : [-5.13935958]\n",
      "--------------------------------------------------\n",
      "Step : 86\n",
      "Current State : [-0.27071527 -0.9626595   1.8099902 ]\n",
      "Currrent Action : tensor([[0.8475]])\n",
      "Next State : [[-0.27071527]\n",
      " [-0.9626595 ]\n",
      " [ 1.8099902 ]]\n",
      "Reward : [-4.31474533]\n",
      "--------------------------------------------------\n",
      "Step : 87\n",
      "Current State : [-0.22199671 -0.9750474   1.0054834 ]\n",
      "Currrent Action : tensor([[-0.5501]])\n",
      "Next State : [[-0.22199671]\n",
      " [-0.9750474 ]\n",
      " [ 1.0054834 ]]\n",
      "Reward : [-3.73168419]\n",
      "--------------------------------------------------\n",
      "Step : 88\n",
      "Current State : [-0.1997517  -0.97984654  0.4551458 ]\n",
      "Currrent Action : tensor([[1.2063]])\n",
      "Next State : [[-0.1997517 ]\n",
      " [-0.97984654]\n",
      " [ 0.4551458 ]]\n",
      "Reward : [-3.32335266]\n",
      "--------------------------------------------------\n",
      "Step : 89\n",
      "Current State : [-0.21669632 -0.9762391  -0.3464919 ]\n",
      "Currrent Action : tensor([[-0.4450]])\n",
      "Next State : [[-0.21669632]\n",
      " [-0.9762391 ]\n",
      " [-0.3464919 ]]\n",
      "Reward : [-3.16054637]\n",
      "--------------------------------------------------\n",
      "Step : 90\n",
      "Current State : [-0.2618449  -0.96510994 -0.9300839 ]\n",
      "Currrent Action : tensor([[0.9906]])\n",
      "Next State : [[-0.2618449 ]\n",
      " [-0.96510994]\n",
      " [-0.9300839 ]]\n",
      "Reward : [-3.21431454]\n",
      "--------------------------------------------------\n",
      "Step : 91\n",
      "Current State : [-0.33217576 -0.9432175  -1.4735217 ]\n",
      "Currrent Action : tensor([[1.2026]])\n",
      "Next State : [[-0.33217576]\n",
      " [-0.9432175 ]\n",
      " [-1.4735217 ]]\n",
      "Reward : [-3.45785516]\n",
      "--------------------------------------------------\n",
      "Step : 92\n",
      "Current State : [-0.43784523 -0.89905035 -2.2918239 ]\n",
      "Currrent Action : tensor([[-0.7393]])\n",
      "Next State : [[-0.43784523]\n",
      " [-0.89905035]\n",
      " [-2.2918239 ]]\n",
      "Reward : [-3.86350326]\n",
      "--------------------------------------------------\n",
      "Step : 93\n",
      "Current State : [-0.55595577 -0.83121186 -2.7262366 ]\n",
      "Currrent Action : tensor([[1.5992]])\n",
      "Next State : [[-0.55595577]\n",
      " [-0.83121186]\n",
      " [-2.7262366 ]]\n",
      "Reward : [-4.62436636]\n",
      "--------------------------------------------------\n",
      "Step : 94\n",
      "Current State : [-0.68467677 -0.72884685 -3.2929542 ]\n",
      "Currrent Action : tensor([[0.3779]])\n",
      "Next State : [[-0.68467677]\n",
      " [-0.72884685]\n",
      " [-3.2929542 ]]\n",
      "Reward : [-5.41031322]\n",
      "--------------------------------------------------\n",
      "Step : 95\n",
      "Current State : [-0.8027475 -0.5963191 -3.554567 ]\n",
      "Currrent Action : tensor([[1.9001]])\n",
      "Next State : [[-0.8027475]\n",
      " [-0.5963191]\n",
      " [-3.554567 ]]\n",
      "Reward : [-6.4933877]\n",
      "--------------------------------------------------\n",
      "Step : 96\n",
      "Current State : [-0.9053757  -0.42461142 -4.0075045 ]\n",
      "Currrent Action : tensor([[-0.0380]])\n",
      "Next State : [[-0.9053757 ]\n",
      " [-0.42461142]\n",
      " [-4.0075045 ]]\n",
      "Reward : [-7.52692726]\n",
      "--------------------------------------------------\n",
      "Step : 97\n",
      "Current State : [-0.97605455 -0.21752599 -4.385072  ]\n",
      "Currrent Action : tensor([[-0.3941]])\n",
      "Next State : [[-0.97605455]\n",
      " [-0.21752599]\n",
      " [-4.385072  ]]\n",
      "Reward : [-8.91269771]\n",
      "--------------------------------------------------\n",
      "Step : 98\n",
      "Current State : [-0.9997594  0.021935  -4.824316 ]\n",
      "Currrent Action : tensor([[-1.8407]])\n",
      "Next State : [[-0.9997594]\n",
      " [ 0.021935 ]\n",
      " [-4.824316 ]]\n",
      "Reward : [-10.46619059]\n",
      "--------------------------------------------------\n",
      "Step : 99\n",
      "Current State : [-0.96808386  0.25062653 -4.627813  ]\n",
      "Currrent Action : tensor([[1.2003]])\n",
      "Next State : [[-0.96808386]\n",
      " [ 0.25062653]\n",
      " [-4.627813  ]]\n",
      "Reward : [-12.0610963]\n",
      "learning iteration : 19\n",
      "--------------------------------------------------\n",
      "Step : 0\n",
      "Current State : [-0.5597204  0.8286815  1.2427179]\n",
      "Currrent Action : tensor([[-0.7313]])\n",
      "Next State : [[-0.5597204]\n",
      " [ 0.8286815]\n",
      " [ 1.2427179]]\n",
      "Reward : [-4.4717664]\n",
      "--------------------------------------------------\n",
      "Step : 1\n",
      "Current State : [-0.6227557   0.78241634  1.564229  ]\n",
      "Currrent Action : tensor([[-2.1360]])\n",
      "Next State : [[-0.6227557 ]\n",
      " [ 0.78241634]\n",
      " [ 1.564229  ]]\n",
      "Reward : [-4.84498731]\n",
      "--------------------------------------------------\n",
      "Step : 2\n",
      "Current State : [-0.7115195  0.7026664  2.3879693]\n",
      "Currrent Action : tensor([[1.5795]])\n",
      "Next State : [[-0.7115195]\n",
      " [ 0.7026664]\n",
      " [ 2.3879693]]\n",
      "Reward : [-5.27847698]\n",
      "--------------------------------------------------\n",
      "Step : 3\n",
      "Current State : [-0.8028349   0.59620136  2.8075361 ]\n",
      "Currrent Action : tensor([[-0.7162]])\n",
      "Next State : [[-0.8028349 ]\n",
      " [ 0.59620136]\n",
      " [ 2.8075361 ]]\n",
      "Reward : [-6.1519444]\n",
      "--------------------------------------------------\n",
      "Step : 4\n",
      "Current State : [-0.8892124  0.4574946  3.2717128]\n",
      "Currrent Action : tensor([[0.1135]])\n",
      "Next State : [[-0.8892124]\n",
      " [ 0.4574946]\n",
      " [ 3.2717128]]\n",
      "Reward : [-7.0524039]\n",
      "--------------------------------------------------\n",
      "Step : 5\n",
      "Current State : [-0.95560604  0.29464737  3.5217822 ]\n",
      "Currrent Action : tensor([[-0.6203]])\n",
      "Next State : [[-0.95560604]\n",
      " [ 0.29464737]\n",
      " [ 3.5217822 ]]\n",
      "Reward : [-8.18057525]\n",
      "--------------------------------------------------\n",
      "Step : 6\n",
      "Current State : [-0.9942015   0.10753325  3.8268983 ]\n",
      "Currrent Action : tensor([[0.5609]])\n",
      "Next State : [[-0.9942015 ]\n",
      " [ 0.10753325]\n",
      " [ 3.8268983 ]]\n",
      "Reward : [-9.32045088]\n",
      "--------------------------------------------------\n",
      "Step : 7\n",
      "Current State : [-0.9955815  -0.09390116  4.0356255 ]\n",
      "Currrent Action : tensor([[0.8538]])\n",
      "Next State : [[-0.9955815 ]\n",
      " [-0.09390116]\n",
      " [ 4.0356255 ]]\n",
      "Reward : [-10.66949654]\n",
      "--------------------------------------------------\n",
      "Step : 8\n",
      "Current State : [-0.955861  -0.2938193  4.083607 ]\n",
      "Currrent Action : tensor([[0.7894]])\n",
      "Next State : [[-0.955861 ]\n",
      " [-0.2938193]\n",
      " [ 4.083607 ]]\n",
      "Reward : [-10.91682947]\n",
      "--------------------------------------------------\n",
      "Step : 9\n",
      "Current State : [-0.8763029  -0.48176056  4.0888524 ]\n",
      "Currrent Action : tensor([[1.5041]])\n",
      "Next State : [[-0.8763029 ]\n",
      " [-0.48176056]\n",
      " [ 4.0888524 ]]\n",
      "Reward : [-9.75461455]\n",
      "--------------------------------------------------\n",
      "Step : 10\n",
      "Current State : [-0.77562445 -0.63119465  3.6085963 ]\n",
      "Currrent Action : tensor([[-0.7929]])\n",
      "Next State : [[-0.77562445]\n",
      " [-0.63119465]\n",
      " [ 3.6085963 ]]\n",
      "Reward : [-8.63645151]\n",
      "--------------------------------------------------\n",
      "Step : 11\n",
      "Current State : [-0.6786653 -0.7344477  2.8352005]\n",
      "Currrent Action : tensor([[-2.2167]])\n",
      "Next State : [[-0.6786653]\n",
      " [-0.7344477]\n",
      " [ 2.8352005]]\n",
      "Reward : [-7.3504198]\n",
      "--------------------------------------------------\n",
      "Step : 12\n",
      "Current State : [-0.58423114 -0.81158733  2.4402282 ]\n",
      "Currrent Action : tensor([[1.0391]])\n",
      "Next State : [[-0.58423114]\n",
      " [-0.81158733]\n",
      " [ 2.4402282 ]]\n",
      "Reward : [-6.17220068]\n",
      "--------------------------------------------------\n",
      "Step : 13\n",
      "Current State : [-0.5138194  -0.85789835  1.6860288 ]\n",
      "Currrent Action : tensor([[-0.9701]])\n",
      "Next State : [[-0.5138194 ]\n",
      " [-0.85789835]\n",
      " [ 1.6860288 ]]\n",
      "Reward : [-5.41324652]\n",
      "--------------------------------------------------\n",
      "Step : 14\n",
      "Current State : [-0.46430567 -0.885675    1.1356094 ]\n",
      "Currrent Action : tensor([[0.6200]])\n",
      "Next State : [[-0.46430567]\n",
      " [-0.885675  ]\n",
      " [ 1.1356094 ]]\n",
      "Reward : [-4.73855705]\n",
      "--------------------------------------------------\n",
      "Step : 15\n",
      "Current State : [-0.45670056 -0.8896205   0.17135315]\n",
      "Currrent Action : tensor([[-2.0838]])\n",
      "Next State : [[-0.45670056]\n",
      " [-0.8896205 ]\n",
      " [ 0.17135315]]\n",
      "Reward : [-4.35042608]\n",
      "--------------------------------------------------\n",
      "Step : 16\n",
      "Current State : [-0.4837481 -0.8752073 -0.6129875]\n",
      "Currrent Action : tensor([[-0.7808]])\n",
      "Next State : [[-0.4837481]\n",
      " [-0.8752073]\n",
      " [-0.6129875]]\n",
      "Reward : [-4.18589462]\n",
      "--------------------------------------------------\n",
      "Step : 17\n",
      "Current State : [-0.5355892 -0.8444786 -1.2054629]\n",
      "Currrent Action : tensor([[0.4262]])\n",
      "Next State : [[-0.5355892]\n",
      " [-0.8444786]\n",
      " [-1.2054629]]\n",
      "Reward : [-4.34640592]\n",
      "--------------------------------------------------\n",
      "Step : 18\n",
      "Current State : [-0.608231   -0.79376006 -1.7724938 ]\n",
      "Currrent Action : tensor([[0.4422]])\n",
      "Next State : [[-0.608231  ]\n",
      " [-0.79376006]\n",
      " [-1.7724938 ]]\n",
      "Reward : [-4.70801278]\n",
      "--------------------------------------------------\n",
      "Step : 19\n",
      "Current State : [-0.6938956 -0.7200756 -2.2611003]\n",
      "Currrent Action : tensor([[0.7114]])\n",
      "Next State : [[-0.6938956]\n",
      " [-0.7200756]\n",
      " [-2.2611003]]\n",
      "Reward : [-5.26364201]\n",
      "--------------------------------------------------\n",
      "Step : 20\n",
      "Current State : [-0.77829313 -0.6279011  -2.5011568 ]\n",
      "Currrent Action : tensor([[2.2403]])\n",
      "Next State : [[-0.77829313]\n",
      " [-0.6279011 ]\n",
      " [-2.5011568 ]]\n",
      "Reward : [-5.98001162]\n",
      "--------------------------------------------------\n",
      "Step : 21\n",
      "Current State : [-0.8623392 -0.506331  -2.9585736]\n",
      "Currrent Action : tensor([[0.0901]])\n",
      "Next State : [[-0.8623392]\n",
      " [-0.506331 ]\n",
      " [-2.9585736]]\n",
      "Reward : [-6.69067117]\n",
      "--------------------------------------------------\n",
      "Step : 22\n",
      "Current State : [-0.93267614 -0.36071485 -3.2378106 ]\n",
      "Currrent Action : tensor([[0.6701]])\n",
      "Next State : [[-0.93267614]\n",
      " [-0.36071485]\n",
      " [-3.2378106 ]]\n",
      "Reward : [-7.69135166]\n",
      "--------------------------------------------------\n",
      "Step : 23\n",
      "Current State : [-0.98088825 -0.19457184 -3.464266  ]\n",
      "Currrent Action : tensor([[0.2939]])\n",
      "Next State : [[-0.98088825]\n",
      " [-0.19457184]\n",
      " [-3.464266  ]]\n",
      "Reward : [-8.73550839]\n",
      "--------------------------------------------------\n",
      "Step : 24\n",
      "Current State : [-0.9995792  -0.02900802 -3.3361769 ]\n",
      "Currrent Action : tensor([[1.8268]])\n",
      "Next State : [[-0.9995792 ]\n",
      " [-0.02900802]\n",
      " [-3.3361769 ]]\n",
      "Reward : [-9.88102209]\n",
      "--------------------------------------------------\n",
      "Step : 25\n",
      "Current State : [-0.9923361   0.12356791 -3.0579329 ]\n",
      "Currrent Action : tensor([[2.2322]])\n",
      "Next State : [[-0.9923361 ]\n",
      " [ 0.12356791]\n",
      " [-3.0579329 ]]\n",
      "Reward : [-10.80516533]\n",
      "--------------------------------------------------\n",
      "Step : 26\n",
      "Current State : [-0.9642404   0.26502925 -2.8869944 ]\n",
      "Currrent Action : tensor([[0.5218]])\n",
      "Next State : [[-0.9642404 ]\n",
      " [ 0.26502925]\n",
      " [-2.8869944 ]]\n",
      "Reward : [-10.04192969]\n",
      "--------------------------------------------------\n",
      "Step : 27\n",
      "Current State : [-0.9257789   0.37806538 -2.3894281 ]\n",
      "Currrent Action : tensor([[1.9920]])\n",
      "Next State : [[-0.9257789 ]\n",
      " [ 0.37806538]\n",
      " [-2.3894281 ]]\n",
      "Reward : [-9.09362993]\n",
      "--------------------------------------------------\n",
      "Step : 28\n",
      "Current State : [-0.8831808   0.46903265 -2.0097897 ]\n",
      "Currrent Action : tensor([[0.6406]])\n",
      "Next State : [[-0.8831808 ]\n",
      " [ 0.46903265]\n",
      " [-2.0097897 ]]\n",
      "Reward : [-8.15524055]\n",
      "--------------------------------------------------\n",
      "Step : 29\n",
      "Current State : [-0.8447622  0.5351419 -1.5296109]\n",
      "Currrent Action : tensor([[0.8560]])\n",
      "Next State : [[-0.8447622]\n",
      " [ 0.5351419]\n",
      " [-1.5296109]]\n",
      "Reward : [-7.44517652]\n",
      "--------------------------------------------------\n",
      "Step : 30\n",
      "Current State : [-0.8157957  0.5783402 -1.0403388]\n",
      "Currrent Action : tensor([[0.5861]])\n",
      "Next State : [[-0.8157957]\n",
      " [ 0.5783402]\n",
      " [-1.0403388]]\n",
      "Reward : [-6.87481541]\n",
      "--------------------------------------------------\n",
      "Step : 31\n",
      "Current State : [-0.79985774  0.6001896  -0.5409104 ]\n",
      "Currrent Action : tensor([[0.4378]])\n",
      "Next State : [[-0.79985774]\n",
      " [ 0.6001896 ]\n",
      " [-0.5409104 ]]\n",
      "Reward : [-6.48354221]\n",
      "--------------------------------------------------\n",
      "Step : 32\n",
      "Current State : [-0.79718745  0.6037319  -0.08872034]\n",
      "Currrent Action : tensor([[0.0137]])\n",
      "Next State : [[-0.79718745]\n",
      " [ 0.6037319 ]\n",
      " [-0.08872034]]\n",
      "Reward : [-6.26853564]\n",
      "--------------------------------------------------\n",
      "Step : 33\n",
      "Current State : [-0.8088604   0.5880007   0.39178663]\n",
      "Currrent Action : tensor([[0.1847]])\n",
      "Next State : [[-0.8088604 ]\n",
      " [ 0.5880007 ]\n",
      " [ 0.39178663]]\n",
      "Reward : [-6.21795692]\n",
      "--------------------------------------------------\n",
      "Step : 34\n",
      "Current State : [-0.8353987   0.54964453  0.932924  ]\n",
      "Currrent Action : tensor([[0.6676]])\n",
      "Next State : [[-0.8353987 ]\n",
      " [ 0.54964453]\n",
      " [ 0.932924  ]]\n",
      "Reward : [-6.33100355]\n",
      "--------------------------------------------------\n",
      "Step : 35\n",
      "Current State : [-0.86620814  0.49968335  1.1741093 ]\n",
      "Currrent Action : tensor([[-1.1403]])\n",
      "Next State : [[-0.86620814]\n",
      " [ 0.49968335]\n",
      " [ 1.1741093 ]]\n",
      "Reward : [-6.64016366]\n",
      "--------------------------------------------------\n",
      "Step : 36\n",
      "Current State : [-0.90392196  0.42769748  1.625784  ]\n",
      "Currrent Action : tensor([[0.5127]])\n",
      "Next State : [[-0.90392196]\n",
      " [ 0.42769748]\n",
      " [ 1.625784  ]]\n",
      "Reward : [-6.99392247]\n",
      "--------------------------------------------------\n",
      "Step : 37\n",
      "Current State : [-0.93836665  0.3456414   1.7804352 ]\n",
      "Currrent Action : tensor([[-1.1075]])\n",
      "Next State : [[-0.93836665]\n",
      " [ 0.3456414 ]\n",
      " [ 1.7804352 ]]\n",
      "Reward : [-7.55364679]\n",
      "--------------------------------------------------\n",
      "Step : 38\n",
      "Current State : [-0.9684775   0.24910103  2.0234063 ]\n",
      "Currrent Action : tensor([[-0.1084]])\n",
      "Next State : [[-0.9684775 ]\n",
      " [ 0.24910103]\n",
      " [ 2.0234063 ]]\n",
      "Reward : [-8.09368945]\n",
      "--------------------------------------------------\n",
      "Step : 39\n",
      "Current State : [-0.99204224  0.12590526  2.5102322 ]\n",
      "Currrent Action : tensor([[2.1749]])\n",
      "Next State : [[-0.99204224]\n",
      " [ 0.12590526]\n",
      " [ 2.5102322 ]]\n",
      "Reward : [-8.76459687]\n",
      "--------------------------------------------------\n",
      "Step : 40\n",
      "Current State : [-9.9999934e-01 -1.1361387e-03  2.5475287e+00]\n",
      "Currrent Action : tensor([[-0.3809]])\n",
      "Next State : [[-9.9999934e-01]\n",
      " [-1.1361387e-03]\n",
      " [ 2.5475287e+00]]\n",
      "Reward : [-9.72262142]\n",
      "--------------------------------------------------\n",
      "Step : 41\n",
      "Current State : [-0.9912611 -0.1319144  2.6232774]\n",
      "Currrent Action : tensor([[0.5107]])\n",
      "Next State : [[-0.9912611]\n",
      " [-0.1319144]\n",
      " [ 2.6232774]]\n",
      "Reward : [-10.51171821]\n",
      "--------------------------------------------------\n",
      "Step : 42\n",
      "Current State : [-0.96787405 -0.25143546  2.437261  ]\n",
      "Currrent Action : tensor([[-0.5805]])\n",
      "Next State : [[-0.96787405]\n",
      " [-0.25143546]\n",
      " [ 2.437261  ]]\n",
      "Reward : [-9.7443377]\n",
      "--------------------------------------------------\n",
      "Step : 43\n",
      "Current State : [-0.9294789  -0.36887538  2.4727142 ]\n",
      "Currrent Action : tensor([[1.4935]])\n",
      "Next State : [[-0.9294789 ]\n",
      " [-0.36887538]\n",
      " [ 2.4727142 ]]\n",
      "Reward : [-8.93350443]\n",
      "--------------------------------------------------\n",
      "Step : 44\n",
      "Current State : [-0.87902457 -0.47677648  2.3837023 ]\n",
      "Currrent Action : tensor([[1.2510]])\n",
      "Next State : [[-0.87902457]\n",
      " [-0.47677648]\n",
      " [ 2.3837023 ]]\n",
      "Reward : [-8.251553]\n",
      "--------------------------------------------------\n",
      "Step : 45\n",
      "Current State : [-0.83088005 -0.55645156  1.8625015 ]\n",
      "Currrent Action : tensor([[-1.0908]])\n",
      "Next State : [[-0.83088005]\n",
      " [-0.55645156]\n",
      " [ 1.8625015 ]]\n",
      "Reward : [-7.56334897]\n",
      "--------------------------------------------------\n",
      "Step : 46\n",
      "Current State : [-0.77983934 -0.62597966  1.7255641 ]\n",
      "Currrent Action : tensor([[1.8693]])\n",
      "Next State : [[-0.77983934]\n",
      " [-0.62597966]\n",
      " [ 1.7255641 ]]\n",
      "Reward : [-6.86045461]\n",
      "--------------------------------------------------\n",
      "Step : 47\n",
      "Current State : [-0.7369504  -0.67594683  1.317231  ]\n",
      "Currrent Action : tensor([[0.4077]])\n",
      "Next State : [[-0.7369504 ]\n",
      " [-0.67594683]\n",
      " [ 1.317231  ]]\n",
      "Reward : [-6.37516142]\n",
      "--------------------------------------------------\n",
      "Step : 48\n",
      "Current State : [-0.70353675 -0.7106589   0.96371144]\n",
      "Currrent Action : tensor([[1.0229]])\n",
      "Next State : [[-0.70353675]\n",
      " [-0.7106589 ]\n",
      " [ 0.96371144]]\n",
      "Reward : [-5.93140745]\n",
      "--------------------------------------------------\n",
      "Step : 49\n",
      "Current State : [-0.69349277 -0.7204636   0.28072557]\n",
      "Currrent Action : tensor([[-0.9999]])\n",
      "Next State : [[-0.69349277]\n",
      " [-0.7204636 ]\n",
      " [ 0.28072557]]\n",
      "Reward : [-5.62181946]\n",
      "--------------------------------------------------\n",
      "Step : 50\n",
      "Current State : [-0.70005417 -0.71408975 -0.18295245]\n",
      "Currrent Action : tensor([[0.5111]])\n",
      "Next State : [[-0.70005417]\n",
      " [-0.71408975]\n",
      " [-0.18295245]]\n",
      "Reward : [-5.47028153]\n",
      "--------------------------------------------------\n",
      "Step : 51\n",
      "Current State : [-0.7250027  -0.68874604 -0.71129894]\n",
      "Currrent Action : tensor([[0.0481]])\n",
      "Next State : [[-0.7250027 ]\n",
      " [-0.68874604]\n",
      " [-0.71129894]]\n",
      "Reward : [-5.50833097]\n",
      "--------------------------------------------------\n",
      "Step : 52\n",
      "Current State : [-0.75790066 -0.65237    -0.9810152 ]\n",
      "Currrent Action : tensor([[1.6456]])\n",
      "Next State : [[-0.75790066]\n",
      " [-0.65237   ]\n",
      " [-0.9810152 ]]\n",
      "Reward : [-5.72643897]\n",
      "--------------------------------------------------\n",
      "Step : 53\n",
      "Current State : [-0.8115776 -0.5842446 -1.7351661]\n",
      "Currrent Action : tensor([[-1.7658]])\n",
      "Next State : [[-0.8115776]\n",
      " [-0.5842446]\n",
      " [-1.7351661]]\n",
      "Reward : [-6.00856108]\n",
      "--------------------------------------------------\n",
      "Step : 54\n",
      "Current State : [-0.86916584 -0.49452066 -2.1333134 ]\n",
      "Currrent Action : tensor([[0.2669]])\n",
      "Next State : [[-0.86916584]\n",
      " [-0.49452066]\n",
      " [-2.1333134 ]]\n",
      "Reward : [-6.63968124]\n",
      "--------------------------------------------------\n",
      "Step : 55\n",
      "Current State : [-0.9182836  -0.39592326 -2.204204  ]\n",
      "Currrent Action : tensor([[2.0483]])\n",
      "Next State : [[-0.9182836 ]\n",
      " [-0.39592326]\n",
      " [-2.204204  ]]\n",
      "Reward : [-7.34610241]\n",
      "--------------------------------------------------\n",
      "Step : 56\n",
      "Current State : [-0.9569955 -0.2901028 -2.2547758]\n",
      "Currrent Action : tensor([[1.6425]])\n",
      "Next State : [[-0.9569955]\n",
      " [-0.2901028]\n",
      " [-2.2547758]]\n",
      "Reward : [-7.96614663]\n",
      "--------------------------------------------------\n",
      "Step : 57\n",
      "Current State : [-0.9850977 -0.1719958 -2.4295793]\n",
      "Currrent Action : tensor([[0.2852]])\n",
      "Next State : [[-0.9850977]\n",
      " [-0.1719958]\n",
      " [-2.4295793]]\n",
      "Reward : [-8.61536292]\n",
      "--------------------------------------------------\n",
      "Step : 58\n",
      "Current State : [-0.9989685  -0.04540914 -2.548611  ]\n",
      "Currrent Action : tensor([[0.0664]])\n",
      "Next State : [[-0.9989685 ]\n",
      " [-0.04540914]\n",
      " [-2.548611  ]]\n",
      "Reward : [-9.40369139]\n",
      "--------------------------------------------------\n",
      "Step : 59\n",
      "Current State : [-0.99703425  0.07695928 -2.4492042 ]\n",
      "Currrent Action : tensor([[0.8898]])\n",
      "Next State : [[-0.99703425]\n",
      " [ 0.07695928]\n",
      " [-2.4492042 ]]\n",
      "Reward : [-10.23658909]\n",
      "--------------------------------------------------\n",
      "Step : 60\n",
      "Current State : [-0.9803532   0.19725016 -2.4303346 ]\n",
      "Currrent Action : tensor([[-0.2590]])\n",
      "Next State : [[-0.9803532 ]\n",
      " [ 0.19725016]\n",
      " [-2.4303346 ]]\n",
      "Reward : [-9.99143807]\n",
      "--------------------------------------------------\n",
      "Step : 61\n",
      "Current State : [-0.9539816   0.29986513 -2.119982  ]\n",
      "Currrent Action : tensor([[1.0828]])\n",
      "Next State : [[-0.9539816 ]\n",
      " [ 0.29986513]\n",
      " [-2.119982  ]]\n",
      "Reward : [-9.25331223]\n",
      "--------------------------------------------------\n",
      "Step : 62\n",
      "Current State : [-0.92507803  0.379777   -1.7000794 ]\n",
      "Currrent Action : tensor([[1.3000]])\n",
      "Next State : [[-0.92507803]\n",
      " [ 0.379777  ]\n",
      " [-1.7000794 ]]\n",
      "Reward : [-8.4999262]\n",
      "--------------------------------------------------\n",
      "Step : 63\n",
      "Current State : [-0.9009888   0.43384236 -1.1839554 ]\n",
      "Currrent Action : tensor([[1.5419]])\n",
      "Next State : [[-0.9009888 ]\n",
      " [ 0.43384236]\n",
      " [-1.1839554 ]]\n",
      "Reward : [-7.86511444]\n",
      "--------------------------------------------------\n",
      "Step : 64\n",
      "Current State : [-0.8843127  0.4668952 -0.7404699]\n",
      "Currrent Action : tensor([[0.7874]])\n",
      "Next State : [[-0.8843127]\n",
      " [ 0.4668952]\n",
      " [-0.7404699]]\n",
      "Reward : [-7.39218025]\n",
      "--------------------------------------------------\n",
      "Step : 65\n",
      "Current State : [-0.88219565  0.470883   -0.09029848]\n",
      "Currrent Action : tensor([[2.4679]])\n",
      "Next State : [[-0.88219565]\n",
      " [ 0.470883  ]\n",
      " [-0.09029848]]\n",
      "Reward : [-7.11218892]\n",
      "--------------------------------------------------\n",
      "Step : 66\n",
      "Current State : [-0.88821477  0.45942843  0.25879708]\n",
      "Currrent Action : tensor([[-0.0271]])\n",
      "Next State : [[-0.88821477]\n",
      " [ 0.45942843]\n",
      " [ 0.25879708]]\n",
      "Reward : [-7.03021424]\n",
      "--------------------------------------------------\n",
      "Step : 67\n",
      "Current State : [-0.9025309   0.4306251   0.64332575]\n",
      "Currrent Action : tensor([[0.2664]])\n",
      "Next State : [[-0.9025309 ]\n",
      " [ 0.4306251 ]\n",
      " [ 0.64332575]]\n",
      "Reward : [-7.10494902]\n",
      "--------------------------------------------------\n",
      "Step : 68\n",
      "Current State : [-0.9212284   0.38902217  0.91230786]\n",
      "Currrent Action : tensor([[-0.3599]])\n",
      "Next State : [[-0.9212284 ]\n",
      " [ 0.38902217]\n",
      " [ 0.91230786]]\n",
      "Reward : [-7.31212896]\n",
      "--------------------------------------------------\n",
      "Step : 69\n",
      "Current State : [-0.94481355  0.32760856  1.3159714 ]\n",
      "Currrent Action : tensor([[0.7460]])\n",
      "Next State : [[-0.94481355]\n",
      " [ 0.32760856]\n",
      " [ 1.3159714 ]]\n",
      "Reward : [-7.60247581]\n",
      "--------------------------------------------------\n",
      "Step : 70\n",
      "Current State : [-0.9687894   0.24788532  1.6654902 ]\n",
      "Currrent Action : tensor([[0.6921]])\n",
      "Next State : [[-0.9687894 ]\n",
      " [ 0.24788532]\n",
      " [ 1.6654902 ]]\n",
      "Reward : [-8.0575176]\n",
      "--------------------------------------------------\n",
      "Step : 71\n",
      "Current State : [-0.98779213  0.1557779   1.8816383 ]\n",
      "Currrent Action : tensor([[0.2016]])\n",
      "Next State : [[-0.98779213]\n",
      " [ 0.1557779 ]\n",
      " [ 1.8816383 ]]\n",
      "Reward : [-8.63586147]\n",
      "--------------------------------------------------\n",
      "Step : 72\n",
      "Current State : [-0.9984772   0.05516522  2.0244336 ]\n",
      "Currrent Action : tensor([[0.1731]])\n",
      "Next State : [[-0.9984772 ]\n",
      " [ 0.05516522]\n",
      " [ 2.0244336 ]]\n",
      "Reward : [-9.26537234]\n",
      "--------------------------------------------------\n",
      "Step : 73\n",
      "Current State : [-0.99837154 -0.05704626  2.2454095 ]\n",
      "Currrent Action : tensor([[1.1973]])\n",
      "Next State : [[-0.99837154]\n",
      " [-0.05704626]\n",
      " [ 2.2454095 ]]\n",
      "Reward : [-9.93712816]\n",
      "--------------------------------------------------\n",
      "Step : 74\n",
      "Current State : [-0.98738945 -0.15830995  2.0380309 ]\n",
      "Currrent Action : tensor([[-1.0973]])\n",
      "Next State : [[-0.98738945]\n",
      " [-0.15830995]\n",
      " [ 2.0380309 ]]\n",
      "Reward : [-10.01962576]\n",
      "--------------------------------------------------\n",
      "Step : 75\n",
      "Current State : [-0.96507853 -0.26196072  2.1214905 ]\n",
      "Currrent Action : tensor([[1.3479]])\n",
      "Next State : [[-0.96507853]\n",
      " [-0.26196072]\n",
      " [ 2.1214905 ]]\n",
      "Reward : [-9.31315943]\n",
      "--------------------------------------------------\n",
      "Step : 76\n",
      "Current State : [-0.9369128 -0.3495633  1.8410331]\n",
      "Currrent Action : tensor([[-0.5599]])\n",
      "Next State : [[-0.9369128]\n",
      " [-0.3495633]\n",
      " [ 1.8410331]]\n",
      "Reward : [-8.7248643]\n",
      "--------------------------------------------------\n",
      "Step : 77\n",
      "Current State : [-0.90375644 -0.42804706  1.7045156 ]\n",
      "Currrent Action : tensor([[0.8377]])\n",
      "Next State : [[-0.90375644]\n",
      " [-0.42804706]\n",
      " [ 1.7045156 ]]\n",
      "Reward : [-8.09301372]\n",
      "--------------------------------------------------\n",
      "Step : 78\n",
      "Current State : [-0.87568104 -0.48288998  1.2324249 ]\n",
      "Currrent Action : tensor([[-1.0070]])\n",
      "Next State : [[-0.87568104]\n",
      " [-0.48288998]\n",
      " [ 1.2324249 ]]\n",
      "Reward : [-7.57756631]\n",
      "--------------------------------------------------\n",
      "Step : 79\n",
      "Current State : [-0.85175884 -0.52393407  0.95022327]\n",
      "Currrent Action : tensor([[0.5331]])\n",
      "Next State : [[-0.85175884]\n",
      " [-0.52393407]\n",
      " [ 0.95022327]]\n",
      "Reward : [-7.10931958]\n",
      "--------------------------------------------------\n",
      "Step : 80\n",
      "Current State : [-0.8334718  -0.55256206  0.6794386 ]\n",
      "Currrent Action : tensor([[0.8144]])\n",
      "Next State : [[-0.8334718 ]\n",
      " [-0.55256206]\n",
      " [ 0.6794386 ]]\n",
      "Reward : [-6.79972658]\n",
      "--------------------------------------------------\n",
      "Step : 81\n",
      "Current State : [-0.82859504 -0.5598484   0.1753558 ]\n",
      "Currrent Action : tensor([[-0.5977]])\n",
      "Next State : [[-0.82859504]\n",
      " [-0.5598484 ]\n",
      " [ 0.1753558 ]]\n",
      "Reward : [-6.5804625]\n",
      "--------------------------------------------------\n",
      "Step : 82\n",
      "Current State : [-0.828755   -0.55961156 -0.00571735]\n",
      "Currrent Action : tensor([[1.5921]])\n",
      "Next State : [[-0.828755  ]\n",
      " [-0.55961156]\n",
      " [-0.00571735]]\n",
      "Reward : [-6.4948044]\n",
      "--------------------------------------------------\n",
      "Step : 83\n",
      "Current State : [-0.8322482  -0.5544032  -0.12542601]\n",
      "Currrent Action : tensor([[2.0350]])\n",
      "Next State : [[-0.8322482 ]\n",
      " [-0.5544032 ]\n",
      " [-0.12542601]]\n",
      "Reward : [-6.49465447]\n",
      "--------------------------------------------------\n",
      "Step : 84\n",
      "Current State : [-0.84731156 -0.53109616 -0.5550388 ]\n",
      "Currrent Action : tensor([[-0.0921]])\n",
      "Next State : [[-0.84731156]\n",
      " [-0.53109616]\n",
      " [-0.5550388 ]]\n",
      "Reward : [-6.52422666]\n",
      "--------------------------------------------------\n",
      "Step : 85\n",
      "Current State : [-0.8705212  -0.49213096 -0.9071559 ]\n",
      "Currrent Action : tensor([[0.3080]])\n",
      "Next State : [[-0.8705212 ]\n",
      " [-0.49213096]\n",
      " [-0.9071559 ]]\n",
      "Reward : [-6.69607085]\n",
      "--------------------------------------------------\n",
      "Step : 86\n",
      "Current State : [-0.9004938  -0.43486887 -1.2928663 ]\n",
      "Currrent Action : tensor([[-0.1107]])\n",
      "Next State : [[-0.9004938 ]\n",
      " [-0.43486887]\n",
      " [-1.2928663 ]]\n",
      "Reward : [-6.98373225]\n",
      "--------------------------------------------------\n",
      "Step : 87\n",
      "Current State : [-0.929443   -0.36896577 -1.4399326 ]\n",
      "Currrent Action : tensor([[1.1939]])\n",
      "Next State : [[-0.929443  ]\n",
      " [-0.36896577]\n",
      " [-1.4399326 ]]\n",
      "Reward : [-7.41382463]\n",
      "--------------------------------------------------\n",
      "Step : 88\n",
      "Current State : [-0.95685256 -0.29057384 -1.6613905 ]\n",
      "Currrent Action : tensor([[0.3684]])\n",
      "Next State : [[-0.95685256]\n",
      " [-0.29057384]\n",
      " [-1.6613905 ]]\n",
      "Reward : [-7.84549538]\n",
      "--------------------------------------------------\n",
      "Step : 89\n",
      "Current State : [-0.9805524  -0.19625767 -1.9457321 ]\n",
      "Currrent Action : tensor([[-0.4427]])\n",
      "Next State : [[-0.9805524 ]\n",
      " [-0.19625767]\n",
      " [-1.9457321 ]]\n",
      "Reward : [-8.38029539]\n",
      "--------------------------------------------------\n",
      "Step : 90\n",
      "Current State : [-0.99418515 -0.10768442 -1.7929254 ]\n",
      "Currrent Action : tensor([[2.0188]])\n",
      "Next State : [[-0.99418515]\n",
      " [-0.10768442]\n",
      " [-1.7929254 ]]\n",
      "Reward : [-9.050034]\n",
      "--------------------------------------------------\n",
      "Step : 91\n",
      "Current State : [-0.9996653  -0.02586917 -1.6404314 ]\n",
      "Currrent Action : tensor([[1.5550]])\n",
      "Next State : [[-0.9996653 ]\n",
      " [-0.02586917]\n",
      " [-1.6404314 ]]\n",
      "Reward : [-9.5272061]\n",
      "--------------------------------------------------\n",
      "Step : 92\n",
      "Current State : [-0.9989554   0.04569516 -1.4316626 ]\n",
      "Currrent Action : tensor([[1.5211]])\n",
      "Next State : [[-0.9989554 ]\n",
      " [ 0.04569516]\n",
      " [-1.4316626 ]]\n",
      "Reward : [-9.97913023]\n",
      "--------------------------------------------------\n",
      "Step : 93\n",
      "Current State : [-0.99388576  0.11041324 -1.2985549 ]\n",
      "Currrent Action : tensor([[0.6589]])\n",
      "Next State : [[-0.99388576]\n",
      " [ 0.11041324]\n",
      " [-1.2985549 ]]\n",
      "Reward : [-9.78988269]\n",
      "--------------------------------------------------\n",
      "Step : 94\n",
      "Current State : [-0.98449004  0.17544058 -1.3142891 ]\n",
      "Currrent Action : tensor([[-0.6570]])\n",
      "Next State : [[-0.98449004]\n",
      " [ 0.17544058]\n",
      " [-1.3142891 ]]\n",
      "Reward : [-9.35573722]\n",
      "--------------------------------------------------\n",
      "Step : 95\n",
      "Current State : [-0.97256106  0.23264788 -1.1689222 ]\n",
      "Currrent Action : tensor([[0.0919]])\n",
      "Next State : [[-0.97256106]\n",
      " [ 0.23264788]\n",
      " [-1.1689222 ]]\n",
      "Reward : [-8.9653886]\n",
      "--------------------------------------------------\n",
      "Step : 96\n",
      "Current State : [-0.95876116  0.284213   -1.0677223 ]\n",
      "Currrent Action : tensor([[-0.4886]])\n",
      "Next State : [[-0.95876116]\n",
      " [ 0.284213  ]\n",
      " [-1.0677223 ]]\n",
      "Reward : [-8.58632373]\n",
      "--------------------------------------------------\n",
      "Step : 97\n",
      "Current State : [-0.9454794   0.32568184 -0.87094617]\n",
      "Currrent Action : tensor([[-0.1092]])\n",
      "Next State : [[-0.9454794 ]\n",
      " [ 0.32568184]\n",
      " [-0.87094617]]\n",
      "Reward : [-8.25594742]\n",
      "--------------------------------------------------\n",
      "Step : 98\n",
      "Current State : [-0.93360496  0.35830396 -0.6943562 ]\n",
      "Currrent Action : tensor([[-0.4511]])\n",
      "Next State : [[-0.93360496]\n",
      " [ 0.35830396]\n",
      " [-0.6943562 ]]\n",
      "Reward : [-7.97137061]\n",
      "--------------------------------------------------\n",
      "Step : 99\n",
      "Current State : [-0.9236575   0.38321912 -0.5365675 ]\n",
      "Currrent Action : tensor([[-0.7396]])\n",
      "Next State : [[-0.9236575 ]\n",
      " [ 0.38321912]\n",
      " [-0.5365675 ]]\n",
      "Reward : [-7.75017338]\n",
      "learning iteration : 20\n",
      "--------------------------------------------------\n",
      "Step : 0\n",
      "Current State : [ 0.98444617  0.17568657 -0.5363977 ]\n",
      "Currrent Action : tensor([[-0.6177]])\n",
      "Next State : [[ 0.98444617]\n",
      " [ 0.17568657]\n",
      " [-0.5363977 ]]\n",
      "Reward : [-0.07719604]\n",
      "--------------------------------------------------\n",
      "Step : 1\n",
      "Current State : [ 0.9883902   0.15193692 -0.48150972]\n",
      "Currrent Action : tensor([[-0.5125]])\n",
      "Next State : [[ 0.9883902 ]\n",
      " [ 0.15193692]\n",
      " [-0.48150972]]\n",
      "Reward : [-0.06022359]\n",
      "--------------------------------------------------\n",
      "Step : 2\n",
      "Current State : [ 0.9904522   0.13785659 -0.28461275]\n",
      "Currrent Action : tensor([[0.5530]])\n",
      "Next State : [[ 0.9904522 ]\n",
      " [ 0.13785659]\n",
      " [-0.28461275]]\n",
      "Reward : [-0.04675561]\n",
      "--------------------------------------------------\n",
      "Step : 3\n",
      "Current State : [ 0.99145204  0.13047178 -0.14904416]\n",
      "Currrent Action : tensor([[0.2145]])\n",
      "Next State : [[ 0.99145204]\n",
      " [ 0.13047178]\n",
      " [-0.14904416]]\n",
      "Reward : [-0.02727252]\n",
      "--------------------------------------------------\n",
      "Step : 4\n",
      "Current State : [ 0.9919306   0.12678197 -0.07441416]\n",
      "Currrent Action : tensor([[-0.1548]])\n",
      "Next State : [[ 0.9919306 ]\n",
      " [ 0.12678197]\n",
      " [-0.07441416]]\n",
      "Reward : [-0.01936575]\n",
      "--------------------------------------------------\n",
      "Step : 5\n",
      "Current State : [ 0.9926305   0.12118058 -0.11289915]\n",
      "Currrent Action : tensor([[-0.8905]])\n",
      "Next State : [[ 0.9926305 ]\n",
      " [ 0.12118058]\n",
      " [-0.11289915]]\n",
      "Reward : [-0.01750723]\n",
      "--------------------------------------------------\n",
      "Step : 6\n",
      "Current State : [ 0.99405813  0.10885041 -0.24825254]\n",
      "Currrent Action : tensor([[-1.5083]])\n",
      "Next State : [[ 0.99405813]\n",
      " [ 0.10885041]\n",
      " [-0.24825254]]\n",
      "Reward : [-0.01830665]\n",
      "--------------------------------------------------\n",
      "Step : 7\n",
      "Current State : [ 0.9943541   0.10611226 -0.05508222]\n",
      "Currrent Action : tensor([[0.7436]])\n",
      "Next State : [[ 0.9943541 ]\n",
      " [ 0.10611226]\n",
      " [-0.05508222]]\n",
      "Reward : [-0.0186113]\n",
      "--------------------------------------------------\n",
      "Step : 8\n",
      "Current State : [0.99346596 0.11412879 0.16131224]\n",
      "Currrent Action : tensor([[0.9121]])\n",
      "Next State : [[0.99346596]\n",
      " [0.11412879]\n",
      " [0.16131224]]\n",
      "Reward : [-0.0124376]\n",
      "--------------------------------------------------\n",
      "Step : 9\n",
      "Current State : [0.99056    0.13707992 0.46269783]\n",
      "Currrent Action : tensor([[1.4386]])\n",
      "Next State : [[0.99056   ]\n",
      " [0.13707992]\n",
      " [0.46269783]]\n",
      "Reward : [-0.01775404]\n",
      "--------------------------------------------------\n",
      "Step : 10\n",
      "Current State : [0.9880929  0.15385851 0.33918408]\n",
      "Currrent Action : tensor([[-1.5088]])\n",
      "Next State : [[0.9880929 ]\n",
      " [0.15385851]\n",
      " [0.33918408]]\n",
      "Reward : [-0.04259528]\n",
      "--------------------------------------------------\n",
      "Step : 11\n",
      "Current State : [0.98472476 0.17411816 0.41076133]\n",
      "Currrent Action : tensor([[-0.2921]])\n",
      "Next State : [[0.98472476]\n",
      " [0.17411816]\n",
      " [0.41076133]]\n",
      "Reward : [-0.03545155]\n",
      "--------------------------------------------------\n",
      "Step : 12\n",
      "Current State : [0.97921693 0.20281577 0.5844484 ]\n",
      "Currrent Action : tensor([[0.2873]])\n",
      "Next State : [[0.97921693]\n",
      " [0.20281577]\n",
      " [0.5844484 ]]\n",
      "Reward : [-0.0475836]\n",
      "--------------------------------------------------\n",
      "Step : 13\n",
      "Current State : [0.9712379  0.23811114 0.7237597 ]\n",
      "Currrent Action : tensor([[-0.0853]])\n",
      "Next State : [[0.9712379 ]\n",
      " [0.23811114]\n",
      " [0.7237597 ]]\n",
      "Reward : [-0.07587623]\n",
      "--------------------------------------------------\n",
      "Step : 14\n",
      "Current State : [0.9614822  0.27486715 0.7606189 ]\n",
      "Currrent Action : tensor([[-0.9448]])\n",
      "Next State : [[0.9614822 ]\n",
      " [0.27486715]\n",
      " [0.7606189 ]]\n",
      "Reward : [-0.11107756]\n",
      "--------------------------------------------------\n",
      "Step : 15\n",
      "Current State : [0.95008105 0.3120032  0.77698463]\n",
      "Currrent Action : tensor([[-1.2652]])\n",
      "Next State : [[0.95008105]\n",
      " [0.3120032 ]\n",
      " [0.77698463]]\n",
      "Reward : [-0.13699018]\n",
      "--------------------------------------------------\n",
      "Step : 16\n",
      "Current State : [0.9383916  0.34557372 0.71098703]\n",
      "Currrent Action : tensor([[-2.0071]])\n",
      "Next State : [[0.9383916 ]\n",
      " [0.34557372]\n",
      " [0.71098703]]\n",
      "Reward : [-0.16505028]\n",
      "--------------------------------------------------\n",
      "Step : 17\n",
      "Current State : [0.9213735  0.38867846 0.9269348 ]\n",
      "Currrent Action : tensor([[-0.2882]])\n",
      "Next State : [[0.9213735 ]\n",
      " [0.38867846]\n",
      " [0.9269348 ]]\n",
      "Reward : [-0.17513652]\n",
      "--------------------------------------------------\n",
      "Step : 18\n",
      "Current State : [0.8943884 0.4472912 1.2907501]\n",
      "Currrent Action : tensor([[0.4820]])\n",
      "Next State : [[0.8943884]\n",
      " [0.4472912]\n",
      " [1.2907501]]\n",
      "Reward : [-0.2455113]\n",
      "--------------------------------------------------\n",
      "Step : 19\n",
      "Current State : [0.8491022  0.52822864 1.8555735 ]\n",
      "Currrent Action : tensor([[1.5290]])\n",
      "Next State : [[0.8491022 ]\n",
      " [0.52822864]\n",
      " [1.8555735 ]]\n",
      "Reward : [-0.3839911]\n",
      "--------------------------------------------------\n",
      "Step : 20\n",
      "Current State : [0.7856976 0.6186108 2.2092066]\n",
      "Currrent Action : tensor([[-0.2836]])\n",
      "Next State : [[0.7856976]\n",
      " [0.6186108]\n",
      " [2.2092066]]\n",
      "Reward : [-0.65410251]\n",
      "--------------------------------------------------\n",
      "Step : 21\n",
      "Current State : [0.6919688 0.7219274 2.792206 ]\n",
      "Currrent Action : tensor([[0.7936]])\n",
      "Next State : [[0.6919688]\n",
      " [0.7219274]\n",
      " [2.792206 ]]\n",
      "Reward : [-0.93354265]\n",
      "--------------------------------------------------\n",
      "Step : 22\n",
      "Current State : [0.55656284 0.8308055  3.4793947 ]\n",
      "Currrent Action : tensor([[0.9716]])\n",
      "Next State : [[0.55656284]\n",
      " [0.8308055 ]\n",
      " [3.4793947 ]]\n",
      "Reward : [-1.43116276]\n",
      "--------------------------------------------------\n",
      "Step : 23\n",
      "Current State : [0.3709377  0.92865777 4.2044873 ]\n",
      "Currrent Action : tensor([[0.6799]])\n",
      "Next State : [[0.3709377 ]\n",
      " [0.92865777]\n",
      " [4.2044873 ]]\n",
      "Reward : [-2.17256599]\n",
      "--------------------------------------------------\n",
      "Step : 24\n",
      "Current State : [0.1418422  0.98988926 4.753927  ]\n",
      "Currrent Action : tensor([[-0.9804]])\n",
      "Next State : [[0.1418422 ]\n",
      " [0.98988926]\n",
      " [4.753927  ]]\n",
      "Reward : [-3.1866841]\n",
      "--------------------------------------------------\n",
      "Step : 25\n",
      "Current State : [-0.14331432  0.98967725  5.7226334 ]\n",
      "Currrent Action : tensor([[1.5086]])\n",
      "Next State : [[-0.14331432]\n",
      " [ 0.98967725]\n",
      " [ 5.7226334 ]]\n",
      "Reward : [-4.30279679]\n",
      "--------------------------------------------------\n",
      "Step : 26\n",
      "Current State : [-0.45349896  0.8912568   6.5375566 ]\n",
      "Currrent Action : tensor([[0.4844]])\n",
      "Next State : [[-0.45349896]\n",
      " [ 0.8912568 ]\n",
      " [ 6.5375566 ]]\n",
      "Reward : [-6.2149613]\n",
      "--------------------------------------------------\n",
      "Step : 27\n",
      "Current State : [-0.7448334   0.66725045  7.391957  ]\n",
      "Currrent Action : tensor([[1.2397]])\n",
      "Next State : [[-0.7448334 ]\n",
      " [ 0.66725045]\n",
      " [ 7.391957  ]]\n",
      "Reward : [-8.44315678]\n",
      "--------------------------------------------------\n",
      "Step : 28\n",
      "Current State : [-0.9436628  0.3309086  7.8648987]\n",
      "Currrent Action : tensor([[-0.1833]])\n",
      "Next State : [[-0.9436628]\n",
      " [ 0.3309086]\n",
      " [ 7.8648987]]\n",
      "Reward : [-11.27744966]\n",
      "--------------------------------------------------\n",
      "Step : 29\n",
      "Current State : [-0.99803287 -0.06269262  8.        ]\n",
      "Currrent Action : tensor([[-0.3636]])\n",
      "Next State : [[-0.99803287]\n",
      " [-0.06269262]\n",
      " [ 8.        ]]\n",
      "Reward : [-14.05004231]\n",
      "--------------------------------------------------\n",
      "Step : 30\n",
      "Current State : [-0.8978714 -0.4402579  7.863043 ]\n",
      "Currrent Action : tensor([[-0.5996]])\n",
      "Next State : [[-0.8978714]\n",
      " [-0.4402579]\n",
      " [ 7.863043 ]]\n",
      "Reward : [-15.8797316]\n",
      "--------------------------------------------------\n",
      "Step : 31\n",
      "Current State : [-0.6687245 -0.7435103  7.6483707]\n",
      "Currrent Action : tensor([[0.7701]])\n",
      "Next State : [[-0.6687245]\n",
      " [-0.7435103]\n",
      " [ 7.6483707]]\n",
      "Reward : [-13.39635792]\n",
      "--------------------------------------------------\n",
      "Step : 32\n",
      "Current State : [-0.3682389 -0.9297312  7.107556 ]\n",
      "Currrent Action : tensor([[0.1121]])\n",
      "Next State : [[-0.3682389]\n",
      " [-0.9297312]\n",
      " [ 7.107556 ]]\n",
      "Reward : [-11.15490652]\n",
      "--------------------------------------------------\n",
      "Step : 33\n",
      "Current State : [-0.05642666 -0.99840677  6.4131494 ]\n",
      "Currrent Action : tensor([[0.0193]])\n",
      "Next State : [[-0.05642666]\n",
      " [-0.99840677]\n",
      " [ 6.4131494 ]]\n",
      "Reward : [-8.84609041]\n",
      "--------------------------------------------------\n",
      "Step : 34\n",
      "Current State : [ 0.22020993 -0.9754525   5.5697265 ]\n",
      "Currrent Action : tensor([[-0.6308]])\n",
      "Next State : [[ 0.22020993]\n",
      " [-0.9754525 ]\n",
      " [ 5.5697265 ]]\n",
      "Reward : [-6.76119865]\n",
      "--------------------------------------------------\n",
      "Step : 35\n",
      "Current State : [ 0.43400767 -0.9009092   4.538137  ]\n",
      "Currrent Action : tensor([[-2.4479]])\n",
      "Next State : [[ 0.43400767]\n",
      " [-0.9009092 ]\n",
      " [ 4.538137  ]]\n",
      "Reward : [-4.92535657]\n",
      "--------------------------------------------------\n",
      "Step : 36\n",
      "Current State : [ 0.5907965  -0.80682063  3.6621828 ]\n",
      "Currrent Action : tensor([[-1.3351]])\n",
      "Next State : [[ 0.5907965 ]\n",
      " [-0.80682063]\n",
      " [ 3.6621828 ]]\n",
      "Reward : [-3.31982071]\n",
      "--------------------------------------------------\n",
      "Step : 37\n",
      "Current State : [ 0.7126842 -0.701485   3.2254336]\n",
      "Currrent Action : tensor([[1.1224]])\n",
      "Next State : [[ 0.7126842]\n",
      " [-0.701485 ]\n",
      " [ 3.2254336]]\n",
      "Reward : [-2.22367094]\n",
      "--------------------------------------------------\n",
      "Step : 38\n",
      "Current State : [ 0.8024772 -0.5966828  2.7623599]\n",
      "Currrent Action : tensor([[0.4203]])\n",
      "Next State : [[ 0.8024772]\n",
      " [-0.5966828]\n",
      " [ 2.7623599]]\n",
      "Reward : [-1.64499237]\n",
      "--------------------------------------------------\n",
      "Step : 39\n",
      "Current State : [ 0.8673151  -0.49775952  2.366948  ]\n",
      "Currrent Action : tensor([[0.3473]])\n",
      "Next State : [[ 0.8673151 ]\n",
      " [-0.49775952]\n",
      " [ 2.366948  ]]\n",
      "Reward : [-1.17196634]\n",
      "--------------------------------------------------\n",
      "Step : 40\n",
      "Current State : [ 0.9107802 -0.4128916  1.9077395]\n",
      "Currrent Action : tensor([[-0.5726]])\n",
      "Next State : [[ 0.9107802]\n",
      " [-0.4128916]\n",
      " [ 1.9077395]]\n",
      "Reward : [-0.8320273]\n",
      "--------------------------------------------------\n",
      "Step : 41\n",
      "Current State : [ 0.93735605 -0.34837288  1.3958391 ]\n",
      "Currrent Action : tensor([[-1.3482]])\n",
      "Next State : [[ 0.93735605]\n",
      " [-0.34837288]\n",
      " [ 1.3958391 ]]\n",
      "Reward : [-0.54692272]\n",
      "--------------------------------------------------\n",
      "Step : 42\n",
      "Current State : [ 0.95945656 -0.28185657  1.4021223 ]\n",
      "Currrent Action : tensor([[1.7838]])\n",
      "Next State : [[ 0.95945656]\n",
      " [-0.28185657]\n",
      " [ 1.4021223 ]]\n",
      "Reward : [-0.32463677]\n",
      "--------------------------------------------------\n",
      "Step : 43\n",
      "Current State : [ 0.9731239  -0.23028219  1.0672183 ]\n",
      "Currrent Action : tensor([[-0.8234]])\n",
      "Next State : [[ 0.9731239 ]\n",
      " [-0.23028219]\n",
      " [ 1.0672183 ]]\n",
      "Reward : [-0.27891349]\n",
      "--------------------------------------------------\n",
      "Step : 44\n",
      "Current State : [ 0.97979814 -0.19998915  0.62041605]\n",
      "Currrent Action : tensor([[-1.8273]])\n",
      "Next State : [[ 0.97979814]\n",
      " [-0.19998915]\n",
      " [ 0.62041605]]\n",
      "Reward : [-0.17122914]\n",
      "--------------------------------------------------\n",
      "Step : 45\n",
      "Current State : [ 0.9826747  -0.18533869  0.29860687]\n",
      "Currrent Action : tensor([[-1.1454]])\n",
      "Next State : [[ 0.9826747 ]\n",
      " [-0.18533869]\n",
      " [ 0.29860687]]\n",
      "Reward : [-0.08034421]\n",
      "--------------------------------------------------\n",
      "Step : 46\n",
      "Current State : [ 0.98298734 -0.18367341  0.03388723]\n",
      "Currrent Action : tensor([[-0.8381]])\n",
      "Next State : [[ 0.98298734]\n",
      " [-0.18367341]\n",
      " [ 0.03388723]]\n",
      "Reward : [-0.04437014]\n",
      "--------------------------------------------------\n",
      "Step : 47\n",
      "Current State : [ 0.98198026 -0.18898356 -0.10809626]\n",
      "Currrent Action : tensor([[-0.0282]])\n",
      "Next State : [[ 0.98198026]\n",
      " [-0.18898356]\n",
      " [-0.10809626]]\n",
      "Reward : [-0.0342379]\n",
      "--------------------------------------------------\n",
      "Step : 48\n",
      "Current State : [ 0.9790695  -0.20352648 -0.29662985]\n",
      "Currrent Action : tensor([[-0.3120]])\n",
      "Next State : [[ 0.9790695 ]\n",
      " [-0.20352648]\n",
      " [-0.29662985]]\n",
      "Reward : [-0.03741407]\n",
      "--------------------------------------------------\n",
      "Step : 49\n",
      "Current State : [ 0.97352946 -0.22856146 -0.5128264 ]\n",
      "Currrent Action : tensor([[-0.4237]])\n",
      "Next State : [[ 0.97352946]\n",
      " [-0.22856146]\n",
      " [-0.5128264 ]]\n",
      "Reward : [-0.0509864]\n",
      "--------------------------------------------------\n",
      "Step : 50\n",
      "Current State : [ 0.96456724 -0.2638372  -0.7279689 ]\n",
      "Currrent Action : tensor([[-0.2915]])\n",
      "Next State : [[ 0.96456724]\n",
      " [-0.2638372 ]\n",
      " [-0.7279689 ]]\n",
      "Reward : [-0.0795603]\n",
      "--------------------------------------------------\n",
      "Step : 51\n",
      "Current State : [ 0.94857997 -0.31653762 -1.1015794 ]\n",
      "Currrent Action : tensor([[-1.1716]])\n",
      "Next State : [[ 0.94857997]\n",
      " [-0.31653762]\n",
      " [-1.1015794 ]]\n",
      "Reward : [-0.12565444]\n",
      "--------------------------------------------------\n",
      "Step : 52\n",
      "Current State : [ 0.9205723 -0.3905722 -1.5835176]\n",
      "Currrent Action : tensor([[-1.6302]])\n",
      "Next State : [[ 0.9205723]\n",
      " [-0.3905722]\n",
      " [-1.5835176]]\n",
      "Reward : [-0.22773909]\n",
      "--------------------------------------------------\n",
      "Step : 53\n",
      "Current State : [ 0.87750274 -0.47957167 -1.9782695 ]\n",
      "Currrent Action : tensor([[-0.6788]])\n",
      "Next State : [[ 0.87750274]\n",
      " [-0.47957167]\n",
      " [-1.9782695 ]]\n",
      "Reward : [-0.41221758]\n",
      "--------------------------------------------------\n",
      "Step : 54\n",
      "Current State : [ 0.8205712  -0.57154435 -2.1644034 ]\n",
      "Currrent Action : tensor([[1.1570]])\n",
      "Next State : [[ 0.8205712 ]\n",
      " [-0.57154435]\n",
      " [-2.1644034 ]]\n",
      "Reward : [-0.64286011]\n",
      "--------------------------------------------------\n",
      "Step : 55\n",
      "Current State : [ 0.7389731  -0.67373496 -2.6172962 ]\n",
      "Currrent Action : tensor([[-0.1616]])\n",
      "Next State : [[ 0.7389731 ]\n",
      " [-0.67373496]\n",
      " [-2.6172962 ]]\n",
      "Reward : [-0.83862465]\n",
      "--------------------------------------------------\n",
      "Step : 56\n",
      "Current State : [ 0.6311058  -0.77569675 -2.9713373 ]\n",
      "Currrent Action : tensor([[1.0084]])\n",
      "Next State : [[ 0.6311058 ]\n",
      " [-0.77569675]\n",
      " [-2.9713373 ]]\n",
      "Reward : [-1.23253361]\n",
      "--------------------------------------------------\n",
      "Step : 57\n",
      "Current State : [ 0.49715996 -0.8676589  -3.25311   ]\n",
      "Currrent Action : tensor([[2.1413]])\n",
      "Next State : [[ 0.49715996]\n",
      " [-0.8676589 ]\n",
      " [-3.25311   ]]\n",
      "Reward : [-1.67510604]\n",
      "--------------------------------------------------\n",
      "Step : 58\n",
      "Current State : [ 0.32398826 -0.9460611  -3.8076067 ]\n",
      "Currrent Action : tensor([[0.6416]])\n",
      "Next State : [[ 0.32398826]\n",
      " [-0.9460611 ]\n",
      " [-3.8076067 ]]\n",
      "Reward : [-2.1621795]\n",
      "--------------------------------------------------\n",
      "Step : 59\n",
      "Current State : [ 0.09540376 -0.99543864 -4.6878605 ]\n",
      "Currrent Action : tensor([[-1.1381]])\n",
      "Next State : [[ 0.09540376]\n",
      " [-0.99543864]\n",
      " [-4.6878605 ]]\n",
      "Reward : [-2.99080129]\n",
      "--------------------------------------------------\n",
      "Step : 60\n",
      "Current State : [-0.16433898 -0.98640394 -5.2127385 ]\n",
      "Currrent Action : tensor([[1.4780]])\n",
      "Next State : [[-0.16433898]\n",
      " [-0.98640394]\n",
      " [-5.2127385 ]]\n",
      "Reward : [-4.37614274]\n",
      "--------------------------------------------------\n",
      "Step : 61\n",
      "Current State : [-0.45075172 -0.89264935 -6.0503893 ]\n",
      "Currrent Action : tensor([[-0.6523]])\n",
      "Next State : [[-0.45075172]\n",
      " [-0.89264935]\n",
      " [-6.0503893 ]]\n",
      "Reward : [-5.73098347]\n",
      "--------------------------------------------------\n",
      "Step : 62\n",
      "Current State : [-0.7163794  -0.69771093 -6.6198373 ]\n",
      "Currrent Action : tensor([[0.6669]])\n",
      "Next State : [[-0.7163794 ]\n",
      " [-0.69771093]\n",
      " [-6.6198373 ]]\n",
      "Reward : [-7.81625511]\n",
      "--------------------------------------------------\n",
      "Step : 63\n",
      "Current State : [-0.91283244 -0.4083344  -7.0313697 ]\n",
      "Currrent Action : tensor([[0.7450]])\n",
      "Next State : [[-0.91283244]\n",
      " [-0.4083344 ]\n",
      " [-7.0313697 ]]\n",
      "Reward : [-9.99681485]\n",
      "--------------------------------------------------\n",
      "Step : 64\n",
      "Current State : [-0.9976378 -0.0686935 -7.0376205]\n",
      "Currrent Action : tensor([[2.3905]])\n",
      "Next State : [[-0.9976378]\n",
      " [-0.0686935]\n",
      " [-7.0376205]]\n",
      "Reward : [-12.35166111]\n",
      "--------------------------------------------------\n",
      "Step : 65\n",
      "Current State : [-0.96358144  0.26741508 -6.7891407 ]\n",
      "Currrent Action : tensor([[2.4415]])\n",
      "Next State : [[-0.96358144]\n",
      " [ 0.26741508]\n",
      " [-6.7891407 ]]\n",
      "Reward : [-14.39918683]\n",
      "--------------------------------------------------\n",
      "Step : 66\n",
      "Current State : [-0.8276246   0.56128204 -6.5044928 ]\n",
      "Currrent Action : tensor([[0.5606]])\n",
      "Next State : [[-0.8276246 ]\n",
      " [ 0.56128204]\n",
      " [-6.5044928 ]]\n",
      "Reward : [-12.85152807]\n",
      "--------------------------------------------------\n",
      "Step : 67\n",
      "Current State : [-0.6169234   0.78702325 -6.200696  ]\n",
      "Currrent Action : tensor([[-0.7811]])\n",
      "Next State : [[-0.6169234 ]\n",
      " [ 0.78702325]\n",
      " [-6.200696  ]]\n",
      "Reward : [-10.71183065]\n",
      "--------------------------------------------------\n",
      "Step : 68\n",
      "Current State : [-0.38437614  0.9231766  -5.4059105 ]\n",
      "Currrent Action : tensor([[1.3635]])\n",
      "Next State : [[-0.38437614]\n",
      " [ 0.9231766 ]\n",
      " [-5.4059105 ]]\n",
      "Reward : [-8.84473569]\n",
      "--------------------------------------------------\n",
      "Step : 69\n",
      "Current State : [-0.14627907  0.9892434  -4.954523  ]\n",
      "Currrent Action : tensor([[-1.6066]])\n",
      "Next State : [[-0.14627907]\n",
      " [ 0.9892434 ]\n",
      " [-4.954523  ]]\n",
      "Reward : [-6.78748352]\n",
      "--------------------------------------------------\n",
      "Step : 70\n",
      "Current State : [ 0.05604807  0.99842805 -4.057666  ]\n",
      "Currrent Action : tensor([[1.0328]])\n",
      "Next State : [[ 0.05604807]\n",
      " [ 0.99842805]\n",
      " [-4.057666  ]]\n",
      "Reward : [-5.4059536]\n",
      "--------------------------------------------------\n",
      "Step : 71\n",
      "Current State : [ 0.21864219  0.9758051  -3.2869065 ]\n",
      "Currrent Action : tensor([[0.1463]])\n",
      "Next State : [[ 0.21864219]\n",
      " [ 0.9758051 ]\n",
      " [-3.2869065 ]]\n",
      "Reward : [-3.94085964]\n",
      "--------------------------------------------------\n",
      "Step : 72\n",
      "Current State : [ 0.34868237  0.93724096 -2.7148418 ]\n",
      "Currrent Action : tensor([[-1.0653]])\n",
      "Next State : [[ 0.34868237]\n",
      " [ 0.93724096]\n",
      " [-2.7148418 ]]\n",
      "Reward : [-2.90501885]\n",
      "--------------------------------------------------\n",
      "Step : 73\n",
      "Current State : [ 0.44782454  0.89412147 -2.1633182 ]\n",
      "Currrent Action : tensor([[-1.0094]])\n",
      "Next State : [[ 0.44782454]\n",
      " [ 0.89412147]\n",
      " [-2.1633182 ]]\n",
      "Reward : [-2.21338508]\n",
      "--------------------------------------------------\n",
      "Step : 74\n",
      "Current State : [ 0.51452583  0.85747486 -1.5224755 ]\n",
      "Currrent Action : tensor([[-0.1983]])\n",
      "Next State : [[ 0.51452583]\n",
      " [ 0.85747486]\n",
      " [-1.5224755 ]]\n",
      "Reward : [-1.69229985]\n",
      "--------------------------------------------------\n",
      "Step : 75\n",
      "Current State : [ 0.5391462   0.8422122  -0.57936925]\n",
      "Currrent Action : tensor([[2.0390]])\n",
      "Next State : [[ 0.5391462 ]\n",
      " [ 0.8422122 ]\n",
      " [-0.57936925]]\n",
      "Reward : [-1.29739732]\n",
      "--------------------------------------------------\n",
      "Step : 76\n",
      "Current State : [0.52902186 0.8486082  0.2395094 ]\n",
      "Currrent Action : tensor([[1.2481]])\n",
      "Next State : [[0.52902186]\n",
      " [0.8486082 ]\n",
      " [0.2395094 ]]\n",
      "Reward : [-1.03787322]\n",
      "--------------------------------------------------\n",
      "Step : 77\n",
      "Current State : [0.49064413 0.87136006 0.89237434]\n",
      "Currrent Action : tensor([[0.1094]])\n",
      "Next State : [[0.49064413]\n",
      " [0.87136006]\n",
      " [0.89237434]]\n",
      "Reward : [-1.0326242]\n",
      "--------------------------------------------------\n",
      "Step : 78\n",
      "Current State : [0.4116803 0.9113283 1.7706343]\n",
      "Currrent Action : tensor([[1.4983]])\n",
      "Next State : [[0.4116803]\n",
      " [0.9113283]\n",
      " [1.7706343]]\n",
      "Reward : [-1.20117324]\n",
      "--------------------------------------------------\n",
      "Step : 79\n",
      "Current State : [0.30265433 0.9531004  2.3364146 ]\n",
      "Currrent Action : tensor([[-0.7848]])\n",
      "Next State : [[0.30265433]\n",
      " [0.9531004 ]\n",
      " [2.3364146 ]]\n",
      "Reward : [-1.62859093]\n",
      "--------------------------------------------------\n",
      "Step : 80\n",
      "Current State : [0.15303317 0.98822105 3.07679   ]\n",
      "Currrent Action : tensor([[0.1703]])\n",
      "Next State : [[0.15303317]\n",
      " [0.98822105]\n",
      " [3.07679   ]]\n",
      "Reward : [-2.14188967]\n",
      "--------------------------------------------------\n",
      "Step : 81\n",
      "Current State : [-0.03904981  0.99923724  3.8539321 ]\n",
      "Currrent Action : tensor([[0.2398]])\n",
      "Next State : [[-0.03904981]\n",
      " [ 0.99923724]\n",
      " [ 3.8539321 ]]\n",
      "Reward : [-2.95506223]\n",
      "--------------------------------------------------\n",
      "Step : 82\n",
      "Current State : [-0.2665632  0.9638174  4.615314 ]\n",
      "Currrent Action : tensor([[0.0797]])\n",
      "Next State : [[-0.2665632]\n",
      " [ 0.9638174]\n",
      " [ 4.615314 ]]\n",
      "Reward : [-4.07692222]\n",
      "--------------------------------------------------\n",
      "Step : 83\n",
      "Current State : [-0.5177769  0.8555157  5.4884977]\n",
      "Currrent Action : tensor([[1.0021]])\n",
      "Next State : [[-0.5177769]\n",
      " [ 0.8555157]\n",
      " [ 5.4884977]]\n",
      "Reward : [-5.51900532]\n",
      "--------------------------------------------------\n",
      "Step : 84\n",
      "Current State : [-0.74783874  0.66388047  6.011005  ]\n",
      "Currrent Action : tensor([[-0.7942]])\n",
      "Next State : [[-0.74783874]\n",
      " [ 0.66388047]\n",
      " [ 6.011005  ]]\n",
      "Reward : [-7.4864137]\n",
      "--------------------------------------------------\n",
      "Step : 85\n",
      "Current State : [-0.9199544   0.39202535  6.4632726 ]\n",
      "Currrent Action : tensor([[-0.3043]])\n",
      "Next State : [[-0.9199544 ]\n",
      " [ 0.39202535]\n",
      " [ 6.4632726 ]]\n",
      "Reward : [-9.4484187]\n",
      "--------------------------------------------------\n",
      "Step : 86\n",
      "Current State : [-0.9971249   0.07577569  6.5396748 ]\n",
      "Currrent Action : tensor([[-1.4508]])\n",
      "Next State : [[-0.9971249 ]\n",
      " [ 0.07577569]\n",
      " [ 6.5396748 ]]\n",
      "Reward : [-11.68030332]\n",
      "--------------------------------------------------\n",
      "Step : 87\n",
      "Current State : [-0.9677956  -0.25173733  6.6064677 ]\n",
      "Currrent Action : tensor([[0.0664]])\n",
      "Next State : [[-0.9677956 ]\n",
      " [-0.25173733]\n",
      " [ 6.6064677 ]]\n",
      "Reward : [-13.67552664]\n",
      "--------------------------------------------------\n",
      "Step : 88\n",
      "Current State : [-0.84283155 -0.5381775   6.275965  ]\n",
      "Currrent Action : tensor([[-0.9447]])\n",
      "Next State : [[-0.84283155]\n",
      " [-0.5381775 ]\n",
      " [ 6.275965  ]]\n",
      "Reward : [-12.70088236]\n",
      "--------------------------------------------------\n",
      "Step : 89\n",
      "Current State : [-0.6588618  -0.75226396  5.6643662 ]\n",
      "Currrent Action : tensor([[-1.3864]])\n",
      "Next State : [[-0.6588618 ]\n",
      " [-0.75226396]\n",
      " [ 5.6643662 ]]\n",
      "Reward : [-10.56266889]\n",
      "--------------------------------------------------\n",
      "Step : 90\n",
      "Current State : [-0.4544997 -0.8907469  4.9498863]\n",
      "Currrent Action : tensor([[-1.0019]])\n",
      "Next State : [[-0.4544997]\n",
      " [-0.8907469]\n",
      " [ 4.9498863]]\n",
      "Reward : [-8.45407122]\n",
      "--------------------------------------------------\n",
      "Step : 91\n",
      "Current State : [-0.265152   -0.96420664  4.068978  ]\n",
      "Currrent Action : tensor([[-1.4190]])\n",
      "Next State : [[-0.265152  ]\n",
      " [-0.96420664]\n",
      " [ 4.068978  ]]\n",
      "Reward : [-6.62439359]\n",
      "--------------------------------------------------\n",
      "Step : 92\n",
      "Current State : [-0.09073532 -0.99587506  3.5500252 ]\n",
      "Currrent Action : tensor([[1.3614]])\n",
      "Next State : [[-0.09073532]\n",
      " [-0.99587506]\n",
      " [ 3.5500252 ]]\n",
      "Reward : [-5.04001296]\n",
      "--------------------------------------------------\n",
      "Step : 93\n",
      "Current State : [ 0.04070261 -0.9991713   2.6314828 ]\n",
      "Currrent Action : tensor([[-1.1442]])\n",
      "Next State : [[ 0.04070261]\n",
      " [-0.9991713 ]\n",
      " [ 2.6314828 ]]\n",
      "Reward : [-4.02268]\n",
      "--------------------------------------------------\n",
      "Step : 94\n",
      "Current State : [ 0.14295118 -0.98972976  2.0545745 ]\n",
      "Currrent Action : tensor([[1.1498]])\n",
      "Next State : [[ 0.14295118]\n",
      " [-0.98972976]\n",
      " [ 2.0545745 ]]\n",
      "Reward : [-3.03494464]\n",
      "--------------------------------------------------\n",
      "Step : 95\n",
      "Current State : [ 0.21968816 -0.97557014  1.5610446 ]\n",
      "Currrent Action : tensor([[1.6584]])\n",
      "Next State : [[ 0.21968816]\n",
      " [-0.97557014]\n",
      " [ 1.5610446 ]]\n",
      "Reward : [-2.46221681]\n",
      "--------------------------------------------------\n",
      "Step : 96\n",
      "Current State : [ 0.26469636 -0.9643318   0.9278844 ]\n",
      "Currrent Action : tensor([[0.6568]])\n",
      "Next State : [[ 0.26469636]\n",
      " [-0.9643318 ]\n",
      " [ 0.9278844 ]]\n",
      "Reward : [-2.06473198]\n",
      "--------------------------------------------------\n",
      "Step : 97\n",
      "Current State : [ 0.27614754 -0.96111524  0.23788874]\n",
      "Currrent Action : tensor([[0.2217]])\n",
      "Next State : [[ 0.27614754]\n",
      " [-0.96111524]\n",
      " [ 0.23788874]]\n",
      "Reward : [-1.78371352]\n",
      "--------------------------------------------------\n",
      "Step : 98\n",
      "Current State : [ 0.24773501 -0.96882784 -0.58883566]\n",
      "Currrent Action : tensor([[-0.7059]])\n",
      "Next State : [[ 0.24773501]\n",
      " [-0.96882784]\n",
      " [-0.58883566]]\n",
      "Reward : [-1.67287164]\n",
      "--------------------------------------------------\n",
      "Step : 99\n",
      "Current State : [ 0.1796135  -0.98373723 -1.3949625 ]\n",
      "Currrent Action : tensor([[-0.5300]])\n",
      "Next State : [[ 0.1796135 ]\n",
      " [-0.98373723]\n",
      " [-1.3949625 ]]\n",
      "Reward : [-1.77855416]\n",
      "learning iteration : 21\n",
      "--------------------------------------------------\n",
      "Step : 0\n",
      "Current State : [-0.20881398 -0.9779554  -1.8157694 ]\n",
      "Currrent Action : tensor([[-0.9392]])\n",
      "Next State : [[-0.20881398]\n",
      " [-0.9779554 ]\n",
      " [-1.8157694 ]]\n",
      "Reward : [-2.9447693]\n",
      "--------------------------------------------------\n",
      "Step : 1\n",
      "Current State : [-0.32733738 -0.94490755 -2.4624457 ]\n",
      "Currrent Action : tensor([[0.5786]])\n",
      "Next State : [[-0.32733738]\n",
      " [-0.94490755]\n",
      " [-2.4624457 ]]\n",
      "Reward : [-3.50256182]\n",
      "--------------------------------------------------\n",
      "Step : 2\n",
      "Current State : [-0.4768248  -0.87899834 -3.2710903 ]\n",
      "Currrent Action : tensor([[-0.6664]])\n",
      "Next State : [[-0.4768248 ]\n",
      " [-0.87899834]\n",
      " [-3.2710903 ]]\n",
      "Reward : [-4.2330929]\n",
      "--------------------------------------------------\n",
      "Step : 3\n",
      "Current State : [-0.6405573 -0.7679104 -3.9636977]\n",
      "Currrent Action : tensor([[-0.2224]])\n",
      "Next State : [[-0.6405573]\n",
      " [-0.7679104]\n",
      " [-3.9636977]]\n",
      "Reward : [-5.34599497]\n",
      "--------------------------------------------------\n",
      "Step : 4\n",
      "Current State : [-0.79456425 -0.6071801  -4.461316  ]\n",
      "Currrent Action : tensor([[0.5221]])\n",
      "Next State : [[-0.79456425]\n",
      " [-0.6071801 ]\n",
      " [-4.461316  ]]\n",
      "Reward : [-6.70620939]\n",
      "--------------------------------------------------\n",
      "Step : 5\n",
      "Current State : [-0.9151323  -0.40315372 -4.7509274 ]\n",
      "Currrent Action : tensor([[1.1052]])\n",
      "Next State : [[-0.9151323 ]\n",
      " [-0.40315372]\n",
      " [-4.7509274 ]]\n",
      "Reward : [-8.1871042]\n",
      "--------------------------------------------------\n",
      "Step : 6\n",
      "Current State : [-0.98612314 -0.16601555 -4.963453  ]\n",
      "Currrent Action : tensor([[0.5989]])\n",
      "Next State : [[-0.98612314]\n",
      " [-0.16601555]\n",
      " [-4.963453  ]]\n",
      "Reward : [-9.69201304]\n",
      "--------------------------------------------------\n",
      "Step : 7\n",
      "Current State : [-0.997365    0.07254667 -4.7879643 ]\n",
      "Currrent Action : tensor([[2.0785]])\n",
      "Next State : [[-0.997365  ]\n",
      " [ 0.07254667]\n",
      " [-4.7879643 ]]\n",
      "Reward : [-11.31705053]\n",
      "--------------------------------------------------\n",
      "Step : 8\n",
      "Current State : [-0.9541174   0.29943278 -4.6297526 ]\n",
      "Currrent Action : tensor([[0.6920]])\n",
      "Next State : [[-0.9541174 ]\n",
      " [ 0.29943278]\n",
      " [-4.6297526 ]]\n",
      "Reward : [-11.71159105]\n",
      "--------------------------------------------------\n",
      "Step : 9\n",
      "Current State : [-0.87266785  0.48831433 -4.1211796 ]\n",
      "Currrent Action : tensor([[1.8933]])\n",
      "Next State : [[-0.87266785]\n",
      " [ 0.48831433]\n",
      " [-4.1211796 ]]\n",
      "Reward : [-10.19842108]\n",
      "--------------------------------------------------\n",
      "Step : 10\n",
      "Current State : [-0.7693455   0.63883287 -3.656464  ]\n",
      "Currrent Action : tensor([[0.6565]])\n",
      "Next State : [[-0.7693455 ]\n",
      " [ 0.63883287]\n",
      " [-3.656464  ]]\n",
      "Reward : [-8.6232963]\n",
      "--------------------------------------------------\n",
      "Step : 11\n",
      "Current State : [-0.6655928  0.7463151 -2.990565 ]\n",
      "Currrent Action : tensor([[1.2452]])\n",
      "Next State : [[-0.6655928]\n",
      " [ 0.7463151]\n",
      " [-2.990565 ]]\n",
      "Reward : [-7.334226]\n",
      "--------------------------------------------------\n",
      "Step : 12\n",
      "Current State : [-0.56506884  0.8250438  -2.555423  ]\n",
      "Currrent Action : tensor([[-0.8306]])\n",
      "Next State : [[-0.56506884]\n",
      " [ 0.8250438 ]\n",
      " [-2.555423  ]]\n",
      "Reward : [-6.18082572]\n",
      "--------------------------------------------------\n",
      "Step : 13\n",
      "Current State : [-0.48476288  0.87464565 -1.8884927 ]\n",
      "Currrent Action : tensor([[0.3210]])\n",
      "Next State : [[-0.48476288]\n",
      " [ 0.87464565]\n",
      " [-1.8884927 ]]\n",
      "Reward : [-5.36772178]\n",
      "--------------------------------------------------\n",
      "Step : 14\n",
      "Current State : [-0.4434701   0.89628917 -0.93250847]\n",
      "Currrent Action : tensor([[2.1719]])\n",
      "Next State : [[-0.4434701 ]\n",
      " [ 0.89628917]\n",
      " [-0.93250847]]\n",
      "Reward : [-4.67410568]\n",
      "--------------------------------------------------\n",
      "Step : 15\n",
      "Current State : [-0.42958143  0.90302813 -0.3087478 ]\n",
      "Currrent Action : tensor([[-0.3230]])\n",
      "Next State : [[-0.42958143]\n",
      " [ 0.90302813]\n",
      " [-0.3087478 ]]\n",
      "Reward : [-4.20902912]\n",
      "--------------------------------------------------\n",
      "Step : 16\n",
      "Current State : [-0.44516397  0.8954491   0.3465627 ]\n",
      "Currrent Action : tensor([[-0.1464]])\n",
      "Next State : [[-0.44516397]\n",
      " [ 0.8954491 ]\n",
      " [ 0.3465627 ]]\n",
      "Reward : [-4.06907591]\n",
      "--------------------------------------------------\n",
      "Step : 17\n",
      "Current State : [-0.48525146  0.8743746   0.9058673 ]\n",
      "Currrent Action : tensor([[-0.7485]])\n",
      "Next State : [[-0.48525146]\n",
      " [ 0.8743746 ]\n",
      " [ 0.9058673 ]]\n",
      "Reward : [-4.14221946]\n",
      "--------------------------------------------------\n",
      "Step : 18\n",
      "Current State : [-0.54911125  0.83574927  1.4929951 ]\n",
      "Currrent Action : tensor([[-0.4577]])\n",
      "Next State : [[-0.54911125]\n",
      " [ 0.83574927]\n",
      " [ 1.4929951 ]]\n",
      "Reward : [-4.39805524]\n",
      "--------------------------------------------------\n",
      "Step : 19\n",
      "Current State : [-0.63412094  0.7732339   2.1114137 ]\n",
      "Currrent Action : tensor([[-0.0560]])\n",
      "Next State : [[-0.63412094]\n",
      " [ 0.7732339 ]\n",
      " [ 2.1114137 ]]\n",
      "Reward : [-4.8544272]\n",
      "--------------------------------------------------\n",
      "Step : 20\n",
      "Current State : [-0.7306379  0.6827651  2.6476932]\n",
      "Currrent Action : tensor([[-0.2910]])\n",
      "Next State : [[-0.7306379]\n",
      " [ 0.6827651]\n",
      " [ 2.6476932]]\n",
      "Reward : [-5.54295389]\n",
      "--------------------------------------------------\n",
      "Step : 21\n",
      "Current State : [-0.826063   0.5635778  3.0565991]\n",
      "Currrent Action : tensor([[-0.6878]])\n",
      "Next State : [[-0.826063 ]\n",
      " [ 0.5635778]\n",
      " [ 3.0565991]]\n",
      "Reward : [-6.41385013]\n",
      "--------------------------------------------------\n",
      "Step : 22\n",
      "Current State : [-0.9113097   0.41172147  3.487366  ]\n",
      "Currrent Action : tensor([[0.0539]])\n",
      "Next State : [[-0.9113097 ]\n",
      " [ 0.41172147]\n",
      " [ 3.487366  ]]\n",
      "Reward : [-7.40053202]\n",
      "--------------------------------------------------\n",
      "Step : 23\n",
      "Current State : [-0.9711359  0.2385269  3.669872 ]\n",
      "Currrent Action : tensor([[-0.8419]])\n",
      "Next State : [[-0.9711359]\n",
      " [ 0.2385269]\n",
      " [ 3.669872 ]]\n",
      "Reward : [-8.60033051]\n",
      "--------------------------------------------------\n",
      "Step : 24\n",
      "Current State : [-0.99896705  0.04544053  3.9078498 ]\n",
      "Currrent Action : tensor([[0.3939]])\n",
      "Next State : [[-0.99896705]\n",
      " [ 0.04544053]\n",
      " [ 3.9078498 ]]\n",
      "Reward : [-9.76126679]\n",
      "--------------------------------------------------\n",
      "Step : 25\n",
      "Current State : [-0.98823017 -0.15297416  3.9806669 ]\n",
      "Currrent Action : tensor([[0.2582]])\n",
      "Next State : [[-0.98823017]\n",
      " [-0.15297416]\n",
      " [ 3.9806669 ]]\n",
      "Reward : [-11.11325681]\n",
      "--------------------------------------------------\n",
      "Step : 26\n",
      "Current State : [-0.94128937 -0.33760086  3.815796  ]\n",
      "Currrent Action : tensor([[-0.3343]])\n",
      "Next State : [[-0.94128937]\n",
      " [-0.33760086]\n",
      " [ 3.815796  ]]\n",
      "Reward : [-10.51291915]\n",
      "--------------------------------------------------\n",
      "Step : 27\n",
      "Current State : [-0.8643605 -0.5028727  3.651042 ]\n",
      "Currrent Action : tensor([[0.5896]])\n",
      "Next State : [[-0.8643605]\n",
      " [-0.5028727]\n",
      " [ 3.651042 ]]\n",
      "Reward : [-9.28084913]\n",
      "--------------------------------------------------\n",
      "Step : 28\n",
      "Current State : [-0.7688396 -0.6394417  3.3370569]\n",
      "Currrent Action : tensor([[0.4211]])\n",
      "Next State : [[-0.7688396]\n",
      " [-0.6394417]\n",
      " [ 3.3370569]]\n",
      "Reward : [-8.1697061]\n",
      "--------------------------------------------------\n",
      "Step : 29\n",
      "Current State : [-0.6687909 -0.7434506  2.8888636]\n",
      "Currrent Action : tensor([[0.2093]])\n",
      "Next State : [[-0.6687909]\n",
      " [-0.7434506]\n",
      " [ 2.8888636]]\n",
      "Reward : [-7.10546507]\n",
      "--------------------------------------------------\n",
      "Step : 30\n",
      "Current State : [-0.5877923 -0.8090119  2.0850801]\n",
      "Currrent Action : tensor([[-1.6413]])\n",
      "Next State : [[-0.5877923]\n",
      " [-0.8090119]\n",
      " [ 2.0850801]]\n",
      "Reward : [-6.14279542]\n",
      "--------------------------------------------------\n",
      "Step : 31\n",
      "Current State : [-0.51363087 -0.85801125  1.7783213 ]\n",
      "Currrent Action : tensor([[2.1006]])\n",
      "Next State : [[-0.51363087]\n",
      " [-0.85801125]\n",
      " [ 1.7783213 ]]\n",
      "Reward : [-5.27490044]\n",
      "--------------------------------------------------\n",
      "Step : 32\n",
      "Current State : [-0.46742117 -0.88403475  1.0607954 ]\n",
      "Currrent Action : tensor([[-0.4934]])\n",
      "Next State : [[-0.46742117]\n",
      " [-0.88403475]\n",
      " [ 1.0607954 ]]\n",
      "Reward : [-4.76946189]\n",
      "--------------------------------------------------\n",
      "Step : 33\n",
      "Current State : [-0.43788004 -0.8990334   0.66264254]\n",
      "Currrent Action : tensor([[1.7658]])\n",
      "Next State : [[-0.43788004]\n",
      " [-0.8990334 ]\n",
      " [ 0.66264254]]\n",
      "Reward : [-4.34758593]\n",
      "--------------------------------------------------\n",
      "Step : 34\n",
      "Current State : [-0.42711955 -0.9041952   0.23869158]\n",
      "Currrent Action : tensor([[1.6688]])\n",
      "Next State : [[-0.42711955]\n",
      " [-0.9041952 ]\n",
      " [ 0.23869158]]\n",
      "Reward : [-4.14341466]\n",
      "--------------------------------------------------\n",
      "Step : 35\n",
      "Current State : [-0.44155306 -0.89723516 -0.32048345]\n",
      "Currrent Action : tensor([[0.7931]])\n",
      "Next State : [[-0.44155306]\n",
      " [-0.89723516]\n",
      " [-0.32048345]]\n",
      "Reward : [-4.05487702]\n",
      "--------------------------------------------------\n",
      "Step : 36\n",
      "Current State : [-0.49861422 -0.866824   -1.2934098 ]\n",
      "Currrent Action : tensor([[-2.1051]])\n",
      "Next State : [[-0.49861422]\n",
      " [-0.866824  ]\n",
      " [-1.2934098 ]]\n",
      "Reward : [-4.12756282]\n",
      "--------------------------------------------------\n",
      "Step : 37\n",
      "Current State : [-0.5752853 -0.8179528 -1.8190706]\n",
      "Currrent Action : tensor([[0.8297]])\n",
      "Next State : [[-0.5752853]\n",
      " [-0.8179528]\n",
      " [-1.8190706]]\n",
      "Reward : [-4.54777314]\n",
      "--------------------------------------------------\n",
      "Step : 38\n",
      "Current State : [-0.6609959 -0.7503895 -2.1838455]\n",
      "Currrent Action : tensor([[1.6579]])\n",
      "Next State : [[-0.6609959]\n",
      " [-0.7503895]\n",
      " [-2.1838455]]\n",
      "Reward : [-5.10241116]\n",
      "--------------------------------------------------\n",
      "Step : 39\n",
      "Current State : [-0.75220317 -0.65893126 -2.5850823 ]\n",
      "Currrent Action : tensor([[1.0770]])\n",
      "Next State : [[-0.75220317]\n",
      " [-0.65893126]\n",
      " [-2.5850823 ]]\n",
      "Reward : [-5.73565878]\n",
      "--------------------------------------------------\n",
      "Step : 40\n",
      "Current State : [-0.8432343  -0.53754616 -3.0374537 ]\n",
      "Currrent Action : tensor([[0.2788]])\n",
      "Next State : [[-0.8432343 ]\n",
      " [-0.53754616]\n",
      " [-3.0374537 ]]\n",
      "Reward : [-6.53537441]\n",
      "--------------------------------------------------\n",
      "Step : 41\n",
      "Current State : [-0.9218492  -0.38754883 -3.3910644 ]\n",
      "Currrent Action : tensor([[0.3303]])\n",
      "Next State : [[-0.9218492 ]\n",
      " [-0.38754883]\n",
      " [-3.3910644 ]]\n",
      "Reward : [-7.54854917]\n",
      "--------------------------------------------------\n",
      "Step : 42\n",
      "Current State : [-0.9748394  -0.22290845 -3.463483  ]\n",
      "Currrent Action : tensor([[1.4550]])\n",
      "Next State : [[-0.9748394 ]\n",
      " [-0.22290845]\n",
      " [-3.463483  ]]\n",
      "Reward : [-8.67950767]\n",
      "--------------------------------------------------\n",
      "Step : 43\n",
      "Current State : [-0.99882257 -0.04851304 -3.5252979 ]\n",
      "Currrent Action : tensor([[0.7024]])\n",
      "Next State : [[-0.99882257]\n",
      " [-0.04851304]\n",
      " [-3.5252979 ]]\n",
      "Reward : [-9.70776197]\n",
      "--------------------------------------------------\n",
      "Step : 44\n",
      "Current State : [-0.9907118  0.1359785 -3.6986635]\n",
      "Currrent Action : tensor([[-0.9132]])\n",
      "Next State : [[-0.9907118]\n",
      " [ 0.1359785]\n",
      " [-3.6986635]]\n",
      "Reward : [-10.81063001]\n",
      "--------------------------------------------------\n",
      "Step : 45\n",
      "Current State : [-0.9547357   0.29745552 -3.312507  ]\n",
      "Currrent Action : tensor([[1.8945]])\n",
      "Next State : [[-0.9547357 ]\n",
      " [ 0.29745552]\n",
      " [-3.312507  ]]\n",
      "Reward : [-10.40277657]\n",
      "--------------------------------------------------\n",
      "Step : 46\n",
      "Current State : [-0.89754915  0.44091436 -3.0918148 ]\n",
      "Currrent Action : tensor([[-0.0160]])\n",
      "Next State : [[-0.89754915]\n",
      " [ 0.44091436]\n",
      " [-3.0918148 ]]\n",
      "Reward : [-9.1604069]\n",
      "--------------------------------------------------\n",
      "Step : 47\n",
      "Current State : [-0.83400255  0.55176055 -2.5571315 ]\n",
      "Currrent Action : tensor([[1.3600]])\n",
      "Next State : [[-0.83400255]\n",
      " [ 0.55176055]\n",
      " [-2.5571315 ]]\n",
      "Reward : [-8.16687476]\n",
      "--------------------------------------------------\n",
      "Step : 48\n",
      "Current State : [-0.76936036  0.63881505 -2.1696665 ]\n",
      "Currrent Action : tensor([[-0.1757]])\n",
      "Next State : [[-0.76936036]\n",
      " [ 0.63881505]\n",
      " [-2.1696665 ]]\n",
      "Reward : [-7.19278021]\n",
      "--------------------------------------------------\n",
      "Step : 49\n",
      "Current State : [-0.72278506  0.6910729  -1.4003067 ]\n",
      "Currrent Action : tensor([[1.9350]])\n",
      "Next State : [[-0.72278506]\n",
      " [ 0.6910729 ]\n",
      " [-1.4003067 ]]\n",
      "Reward : [-6.4703057]\n",
      "--------------------------------------------------\n",
      "Step : 50\n",
      "Current State : [-0.69300383  0.72093385 -0.8435309 ]\n",
      "Currrent Action : tensor([[0.2565]])\n",
      "Next State : [[-0.69300383]\n",
      " [ 0.72093385]\n",
      " [-0.8435309 ]]\n",
      "Reward : [-5.85398596]\n",
      "--------------------------------------------------\n",
      "Step : 51\n",
      "Current State : [-0.69084525  0.7230027  -0.0597981 ]\n",
      "Currrent Action : tensor([[1.6202]])\n",
      "Next State : [[-0.69084525]\n",
      " [ 0.7230027 ]\n",
      " [-0.0597981 ]]\n",
      "Reward : [-5.53274872]\n",
      "--------------------------------------------------\n",
      "Step : 52\n",
      "Current State : [-0.7103894   0.70380884  0.5478773 ]\n",
      "Currrent Action : tensor([[0.4362]])\n",
      "Next State : [[-0.7103894 ]\n",
      " [ 0.70380884]\n",
      " [ 0.5478773 ]]\n",
      "Reward : [-5.44555444]\n",
      "--------------------------------------------------\n",
      "Step : 53\n",
      "Current State : [-0.7526406  0.6584316  1.2402407]\n",
      "Currrent Action : tensor([[1.0967]])\n",
      "Next State : [[-0.7526406]\n",
      " [ 0.6584316]\n",
      " [ 1.2402407]]\n",
      "Reward : [-5.60482142]\n",
      "--------------------------------------------------\n",
      "Step : 54\n",
      "Current State : [-0.80273896  0.5963306   1.5962148 ]\n",
      "Currrent Action : tensor([[-0.9190]])\n",
      "Next State : [[-0.80273896]\n",
      " [ 0.5963306 ]\n",
      " [ 1.5962148 ]]\n",
      "Reward : [-6.02491336]\n",
      "--------------------------------------------------\n",
      "Step : 55\n",
      "Current State : [-0.85423917  0.5198802   1.8442303 ]\n",
      "Currrent Action : tensor([[-1.3282]])\n",
      "Next State : [[-0.85423917]\n",
      " [ 0.5198802 ]\n",
      " [ 1.8442303 ]]\n",
      "Reward : [-6.51991366]\n",
      "--------------------------------------------------\n",
      "Step : 56\n",
      "Current State : [-0.90858454  0.41770098  2.3159442 ]\n",
      "Currrent Action : tensor([[0.5454]])\n",
      "Next State : [[-0.90858454]\n",
      " [ 0.41770098]\n",
      " [ 2.3159442 ]]\n",
      "Reward : [-7.07382829]\n",
      "--------------------------------------------------\n",
      "Step : 57\n",
      "Current State : [-0.95900005  0.2834059   2.8713946 ]\n",
      "Currrent Action : tensor([[1.6145]])\n",
      "Next State : [[-0.95900005]\n",
      " [ 0.2834059 ]\n",
      " [ 2.8713946 ]]\n",
      "Reward : [-7.88674785]\n",
      "--------------------------------------------------\n",
      "Step : 58\n",
      "Current State : [-0.9921031   0.12542485  3.2317543 ]\n",
      "Currrent Action : tensor([[0.9854]])\n",
      "Next State : [[-0.9921031 ]\n",
      " [ 0.12542485]\n",
      " [ 3.2317543 ]]\n",
      "Reward : [-8.97219843]\n",
      "--------------------------------------------------\n",
      "Step : 59\n",
      "Current State : [-0.99911565 -0.04204698  3.3563085 ]\n",
      "Currrent Action : tensor([[0.2032]])\n",
      "Next State : [[-0.99911565]\n",
      " [-0.04204698]\n",
      " [ 3.3563085 ]]\n",
      "Reward : [-10.13973537]\n",
      "--------------------------------------------------\n",
      "Step : 60\n",
      "Current State : [-0.9768047  -0.21413219  3.4748788 ]\n",
      "Currrent Action : tensor([[1.0007]])\n",
      "Next State : [[-0.9768047 ]\n",
      " [-0.21413219]\n",
      " [ 3.4748788 ]]\n",
      "Reward : [-10.73458859]\n",
      "--------------------------------------------------\n",
      "Step : 61\n",
      "Current State : [-0.9282703  -0.37190628  3.3051686 ]\n",
      "Currrent Action : tensor([[-0.0607]])\n",
      "Next State : [[-0.9282703 ]\n",
      " [-0.37190628]\n",
      " [ 3.3051686 ]]\n",
      "Reward : [-9.76772521]\n",
      "--------------------------------------------------\n",
      "Step : 62\n",
      "Current State : [-0.8618419  -0.50717705  3.0168872 ]\n",
      "Currrent Action : tensor([[-0.0623]])\n",
      "Next State : [[-0.8618419 ]\n",
      " [-0.50717705]\n",
      " [ 3.0168872 ]]\n",
      "Reward : [-8.71294868]\n",
      "--------------------------------------------------\n",
      "Step : 63\n",
      "Current State : [-0.7904785  -0.61248976  2.5460064 ]\n",
      "Currrent Action : tensor([[-0.6033]])\n",
      "Next State : [[-0.7904785 ]\n",
      " [-0.61248976]\n",
      " [ 2.5460064 ]]\n",
      "Reward : [-7.72098866]\n",
      "--------------------------------------------------\n",
      "Step : 64\n",
      "Current State : [-0.7326844 -0.6805686  1.7866391]\n",
      "Currrent Action : tensor([[-2.1770]])\n",
      "Next State : [[-0.7326844]\n",
      " [-0.6805686]\n",
      " [ 1.7866391]]\n",
      "Reward : [-6.81445622]\n",
      "--------------------------------------------------\n",
      "Step : 65\n",
      "Current State : [-0.6872975  -0.72637606  1.2899197 ]\n",
      "Currrent Action : tensor([[0.0914]])\n",
      "Next State : [[-0.6872975 ]\n",
      " [-0.72637606]\n",
      " [ 1.2899197 ]]\n",
      "Reward : [-6.04592499]\n",
      "--------------------------------------------------\n",
      "Step : 66\n",
      "Current State : [-0.6571823 -0.7537317  0.813754 ]\n",
      "Currrent Action : tensor([[0.4574]])\n",
      "Next State : [[-0.6571823]\n",
      " [-0.7537317]\n",
      " [ 0.813754 ]]\n",
      "Reward : [-5.58878218]\n",
      "--------------------------------------------------\n",
      "Step : 67\n",
      "Current State : [-0.6362684  -0.7714678   0.54845524]\n",
      "Currrent Action : tensor([[2.1179]])\n",
      "Next State : [[-0.6362684 ]\n",
      " [-0.7714678 ]\n",
      " [ 0.54845524]]\n",
      "Reward : [-5.30457132]\n",
      "--------------------------------------------------\n",
      "Step : 68\n",
      "Current State : [-0.6322161  -0.77479213  0.10482781]\n",
      "Currrent Action : tensor([[0.8998]])\n",
      "Next State : [[-0.6322161 ]\n",
      " [-0.77479213]\n",
      " [ 0.10482781]]\n",
      "Reward : [-5.1405143]\n",
      "--------------------------------------------------\n",
      "Step : 69\n",
      "Current State : [-0.6411328  -0.76743    -0.23126598]\n",
      "Currrent Action : tensor([[1.6333]])\n",
      "Next State : [[-0.6411328 ]\n",
      " [-0.76743   ]\n",
      " [-0.23126598]]\n",
      "Reward : [-5.08972267]\n",
      "--------------------------------------------------\n",
      "Step : 70\n",
      "Current State : [-0.6655018  -0.7463963  -0.64384896]\n",
      "Currrent Action : tensor([[1.0866]])\n",
      "Next State : [[-0.6655018 ]\n",
      " [-0.7463963 ]\n",
      " [-0.64384896]]\n",
      "Reward : [-5.14477405]\n",
      "--------------------------------------------------\n",
      "Step : 71\n",
      "Current State : [-0.7113159 -0.7028725 -1.2640545]\n",
      "Currrent Action : tensor([[-0.4027]])\n",
      "Next State : [[-0.7113159]\n",
      " [-0.7028725]\n",
      " [-1.2640545]]\n",
      "Reward : [-5.32684338]\n",
      "--------------------------------------------------\n",
      "Step : 72\n",
      "Current State : [-0.779021  -0.6269978 -2.0346866]\n",
      "Currrent Action : tensor([[-1.6232]])\n",
      "Next State : [[-0.779021 ]\n",
      " [-0.6269978]\n",
      " [-2.0346866]]\n",
      "Reward : [-5.74224109]\n",
      "--------------------------------------------------\n",
      "Step : 73\n",
      "Current State : [-0.8536999 -0.5207653 -2.598926 ]\n",
      "Currrent Action : tensor([[-0.6266]])\n",
      "Next State : [[-0.8536999]\n",
      " [-0.5207653]\n",
      " [-2.598926 ]]\n",
      "Reward : [-6.48518697]\n",
      "--------------------------------------------------\n",
      "Step : 74\n",
      "Current State : [-0.9176482 -0.3973938 -2.7814424]\n",
      "Currrent Action : tensor([[1.3871]])\n",
      "Next State : [[-0.9176482]\n",
      " [-0.3973938]\n",
      " [-2.7814424]]\n",
      "Reward : [-7.40540017]\n",
      "--------------------------------------------------\n",
      "Step : 75\n",
      "Current State : [-0.96591043 -0.2588764  -2.9363246 ]\n",
      "Currrent Action : tensor([[0.9544]])\n",
      "Next State : [[-0.96591043]\n",
      " [-0.2588764 ]\n",
      " [-2.9363246 ]]\n",
      "Reward : [-8.24339191]\n",
      "--------------------------------------------------\n",
      "Step : 76\n",
      "Current State : [-0.9938099  -0.11109428 -3.010694  ]\n",
      "Currrent Action : tensor([[0.7986]])\n",
      "Next State : [[-0.9938099 ]\n",
      " [-0.11109428]\n",
      " [-3.010694  ]]\n",
      "Reward : [-9.1557052]\n",
      "--------------------------------------------------\n",
      "Step : 77\n",
      "Current State : [-0.9991793   0.04050572 -3.0368178 ]\n",
      "Currrent Action : tensor([[0.3813]])\n",
      "Next State : [[-0.9991793 ]\n",
      " [ 0.04050572]\n",
      " [-3.0368178 ]]\n",
      "Reward : [-10.08910083]\n",
      "--------------------------------------------------\n",
      "Step : 78\n",
      "Current State : [-0.9829555   0.18384352 -2.887568  ]\n",
      "Currrent Action : tensor([[0.7925]])\n",
      "Next State : [[-0.9829555 ]\n",
      " [ 0.18384352]\n",
      " [-2.887568  ]]\n",
      "Reward : [-10.53952561]\n",
      "--------------------------------------------------\n",
      "Step : 79\n",
      "Current State : [-0.9517611  0.30684   -2.5395174]\n",
      "Currrent Action : tensor([[1.4011]])\n",
      "Next State : [[-0.9517611]\n",
      " [ 0.30684  ]\n",
      " [-2.5395174]]\n",
      "Reward : [-9.57782782]\n",
      "--------------------------------------------------\n",
      "Step : 80\n",
      "Current State : [-0.9071815  0.4207395 -2.447785 ]\n",
      "Currrent Action : tensor([[-0.9226]])\n",
      "Next State : [[-0.9071815]\n",
      " [ 0.4207395]\n",
      " [-2.447785 ]]\n",
      "Reward : [-8.65309037]\n",
      "--------------------------------------------------\n",
      "Step : 81\n",
      "Current State : [-0.8648114  0.5020968 -1.8352267]\n",
      "Currrent Action : tensor([[1.9800]])\n",
      "Next State : [[-0.8648114]\n",
      " [ 0.5020968]\n",
      " [-1.8352267]]\n",
      "Reward : [-7.93273393]\n",
      "--------------------------------------------------\n",
      "Step : 82\n",
      "Current State : [-0.830377   0.557202  -1.2998155]\n",
      "Currrent Action : tensor([[1.0589]])\n",
      "Next State : [[-0.830377 ]\n",
      " [ 0.557202 ]\n",
      " [-1.2998155]]\n",
      "Reward : [-7.1791387]\n",
      "--------------------------------------------------\n",
      "Step : 83\n",
      "Current State : [-0.8100558  0.5863528 -0.7107328]\n",
      "Currrent Action : tensor([[1.1412]])\n",
      "Next State : [[-0.8100558]\n",
      " [ 0.5863528]\n",
      " [-0.7107328]]\n",
      "Reward : [-6.67571389]\n",
      "--------------------------------------------------\n",
      "Step : 84\n",
      "Current State : [-0.80223936  0.5970025  -0.26420897]\n",
      "Currrent Action : tensor([[0.0451]])\n",
      "Next State : [[-0.80223936]\n",
      " [ 0.5970025 ]\n",
      " [-0.26420897]]\n",
      "Reward : [-6.37596039]\n",
      "--------------------------------------------------\n",
      "Step : 85\n",
      "Current State : [-0.814159    0.58064204  0.40484938]\n",
      "Currrent Action : tensor([[1.4754]])\n",
      "Next State : [[-0.814159  ]\n",
      " [ 0.58064204]\n",
      " [ 0.40484938]]\n",
      "Reward : [-6.26832642]\n",
      "--------------------------------------------------\n",
      "Step : 86\n",
      "Current State : [-0.84094745  0.5411168   0.9550507 ]\n",
      "Currrent Action : tensor([[0.7648]])\n",
      "Next State : [[-0.84094745]\n",
      " [ 0.5411168 ]\n",
      " [ 0.9550507 ]]\n",
      "Reward : [-6.37784058]\n",
      "--------------------------------------------------\n",
      "Step : 87\n",
      "Current State : [-0.8797648   0.47540918  1.5267086 ]\n",
      "Currrent Action : tensor([[1.1055]])\n",
      "Next State : [[-0.8797648 ]\n",
      " [ 0.47540918]\n",
      " [ 1.5267086 ]]\n",
      "Reward : [-6.69645092]\n",
      "--------------------------------------------------\n",
      "Step : 88\n",
      "Current State : [-0.9146052   0.40434796  1.5832655 ]\n",
      "Currrent Action : tensor([[-2.2305]])\n",
      "Next State : [[-0.9146052 ]\n",
      " [ 0.40434796]\n",
      " [ 1.5832655 ]]\n",
      "Reward : [-7.23926556]\n",
      "--------------------------------------------------\n",
      "Step : 89\n",
      "Current State : [-0.94857776  0.31654415  1.8836348 ]\n",
      "Currrent Action : tensor([[-0.0193]])\n",
      "Next State : [[-0.94857776]\n",
      " [ 0.31654415]\n",
      " [ 1.8836348 ]]\n",
      "Reward : [-7.67807975]\n",
      "--------------------------------------------------\n",
      "Step : 90\n",
      "Current State : [-0.9777386   0.20982645  2.213732  ]\n",
      "Currrent Action : tensor([[0.6179]])\n",
      "Next State : [[-0.9777386 ]\n",
      " [ 0.20982645]\n",
      " [ 2.213732  ]]\n",
      "Reward : [-8.30481854]\n",
      "--------------------------------------------------\n",
      "Step : 91\n",
      "Current State : [-0.9963269  0.085631   2.5132294]\n",
      "Currrent Action : tensor([[0.9475]])\n",
      "Next State : [[-0.9963269]\n",
      " [ 0.085631 ]\n",
      " [ 2.5132294]]\n",
      "Reward : [-9.07700258]\n",
      "--------------------------------------------------\n",
      "Step : 92\n",
      "Current State : [-0.9988252  -0.04845912  2.684282  ]\n",
      "Currrent Action : tensor([[0.7122]])\n",
      "Next State : [[-0.9988252 ]\n",
      " [-0.04845912]\n",
      " [ 2.684282  ]]\n",
      "Reward : [-9.97039935]\n",
      "--------------------------------------------------\n",
      "Step : 93\n",
      "Current State : [-0.9817591 -0.1901293  2.8563147]\n",
      "Currrent Action : tensor([[1.3892]])\n",
      "Next State : [[-0.9817591]\n",
      " [-0.1901293]\n",
      " [ 2.8563147]]\n",
      "Reward : [-10.28982442]\n",
      "--------------------------------------------------\n",
      "Step : 94\n",
      "Current State : [-0.9430178 -0.3327424  2.9583273]\n",
      "Currrent Action : tensor([[1.6307]])\n",
      "Next State : [[-0.9430178]\n",
      " [-0.3327424]\n",
      " [ 2.9583273]]\n",
      "Reward : [-9.52277568]\n",
      "--------------------------------------------------\n",
      "Step : 95\n",
      "Current State : [-0.89095396 -0.4540936   2.6428885 ]\n",
      "Currrent Action : tensor([[-0.4392]])\n",
      "Next State : [[-0.89095396]\n",
      " [-0.4540936 ]\n",
      " [ 2.6428885 ]]\n",
      "Reward : [-8.7287104]\n",
      "--------------------------------------------------\n",
      "Step : 96\n",
      "Current State : [-0.831385   -0.55569685  2.3569257 ]\n",
      "Currrent Action : tensor([[0.3640]])\n",
      "Next State : [[-0.831385  ]\n",
      " [-0.55569685]\n",
      " [ 2.3569257 ]]\n",
      "Reward : [-7.8287896]\n",
      "--------------------------------------------------\n",
      "Step : 97\n",
      "Current State : [-0.770983  -0.6368557  2.0242434]\n",
      "Currrent Action : tensor([[0.5606]])\n",
      "Next State : [[-0.770983 ]\n",
      " [-0.6368557]\n",
      " [ 2.0242434]]\n",
      "Reward : [-7.07052774]\n",
      "--------------------------------------------------\n",
      "Step : 98\n",
      "Current State : [-0.7227394 -0.6911207  1.452508 ]\n",
      "Currrent Action : tensor([[-0.6273]])\n",
      "Next State : [[-0.7227394]\n",
      " [-0.6911207]\n",
      " [ 1.452508 ]]\n",
      "Reward : [-6.41843086]\n",
      "--------------------------------------------------\n",
      "Step : 99\n",
      "Current State : [-0.6818421  -0.73149943  1.1495988 ]\n",
      "Currrent Action : tensor([[1.4362]])\n",
      "Next State : [[-0.6818421 ]\n",
      " [-0.73149943]\n",
      " [ 1.1495988 ]]\n",
      "Reward : [-5.87056055]\n",
      "learning iteration : 22\n",
      "--------------------------------------------------\n",
      "Step : 0\n",
      "Current State : [0.22426148 0.974529   0.32509837]\n",
      "Currrent Action : tensor([[0.6345]])\n",
      "Next State : [[0.22426148]\n",
      " [0.974529  ]\n",
      " [0.32509837]]\n",
      "Reward : [-1.78974684]\n",
      "--------------------------------------------------\n",
      "Step : 1\n",
      "Current State : [0.16993825 0.98545474 1.1083627 ]\n",
      "Currrent Action : tensor([[0.3491]])\n",
      "Next State : [[0.16993825]\n",
      " [0.98545474]\n",
      " [1.1083627 ]]\n",
      "Reward : [-1.81866999]\n",
      "--------------------------------------------------\n",
      "Step : 2\n",
      "Current State : [0.0775714 0.9969868 1.8623523]\n",
      "Currrent Action : tensor([[0.0993]])\n",
      "Next State : [[0.0775714]\n",
      " [0.9969868]\n",
      " [1.8623523]]\n",
      "Reward : [-2.08293874]\n",
      "--------------------------------------------------\n",
      "Step : 3\n",
      "Current State : [-0.05575619  0.99844444  2.6686904 ]\n",
      "Currrent Action : tensor([[0.3907]])\n",
      "Next State : [[-0.05575619]\n",
      " [ 0.99844444]\n",
      " [ 2.6686904 ]]\n",
      "Reward : [-2.57647593]\n",
      "--------------------------------------------------\n",
      "Step : 4\n",
      "Current State : [-0.22520266  0.97431195  3.4273183 ]\n",
      "Currrent Action : tensor([[0.0653]])\n",
      "Next State : [[-0.22520266]\n",
      " [ 0.97431195]\n",
      " [ 3.4273183 ]]\n",
      "Reward : [-3.35796229]\n",
      "--------------------------------------------------\n",
      "Step : 5\n",
      "Current State : [-0.40819862  0.9128931   3.8665776 ]\n",
      "Currrent Action : tensor([[-1.9432]])\n",
      "Next State : [[-0.40819862]\n",
      " [ 0.9128931 ]\n",
      " [ 3.8665776 ]]\n",
      "Reward : [-4.41104175]\n",
      "--------------------------------------------------\n",
      "Step : 6\n",
      "Current State : [-0.6093328  0.7929146  4.694778 ]\n",
      "Currrent Action : tensor([[0.9569]])\n",
      "Next State : [[-0.6093328]\n",
      " [ 0.7929146]\n",
      " [ 4.694778 ]]\n",
      "Reward : [-5.46113904]\n",
      "--------------------------------------------------\n",
      "Step : 7\n",
      "Current State : [-0.7961796   0.60506034  5.314717  ]\n",
      "Currrent Action : tensor([[0.1684]])\n",
      "Next State : [[-0.7961796 ]\n",
      " [ 0.60506034]\n",
      " [ 5.314717  ]]\n",
      "Reward : [-7.15926598]\n",
      "--------------------------------------------------\n",
      "Step : 8\n",
      "Current State : [-0.9364716  0.3507434  5.8295364]\n",
      "Currrent Action : tensor([[0.4068]])\n",
      "Next State : [[-0.9364716]\n",
      " [ 0.3507434]\n",
      " [ 5.8295364]]\n",
      "Reward : [-9.03361003]\n",
      "--------------------------------------------------\n",
      "Step : 9\n",
      "Current State : [-0.9989349   0.04614125  6.2441435 ]\n",
      "Currrent Action : tensor([[1.0103]])\n",
      "Next State : [[-0.9989349 ]\n",
      " [ 0.04614125]\n",
      " [ 6.2441435 ]]\n",
      "Reward : [-11.14572768]\n",
      "--------------------------------------------------\n",
      "Step : 10\n",
      "Current State : [-0.96536446 -0.26090515  6.2023473 ]\n",
      "Currrent Action : tensor([[-0.5094]])\n",
      "Next State : [[-0.96536446]\n",
      " [-0.26090515]\n",
      " [ 6.2023473 ]]\n",
      "Reward : [-13.4809104]\n",
      "--------------------------------------------------\n",
      "Step : 11\n",
      "Current State : [-0.84681994 -0.53187966  5.93718   ]\n",
      "Currrent Action : tensor([[-0.4633]])\n",
      "Next State : [[-0.84681994]\n",
      " [-0.53187966]\n",
      " [ 5.93718   ]]\n",
      "Reward : [-12.1278969]\n",
      "--------------------------------------------------\n",
      "Step : 12\n",
      "Current State : [-0.66709304 -0.74497443  5.5935555 ]\n",
      "Currrent Action : tensor([[0.3686]])\n",
      "Next State : [[-0.66709304]\n",
      " [-0.74497443]\n",
      " [ 5.5935555 ]]\n",
      "Reward : [-10.18554052]\n",
      "--------------------------------------------------\n",
      "Step : 13\n",
      "Current State : [-0.46290877 -0.88640594  4.980518  ]\n",
      "Currrent Action : tensor([[-0.3620]])\n",
      "Next State : [[-0.46290877]\n",
      " [-0.88640594]\n",
      " [ 4.980518  ]]\n",
      "Reward : [-8.42396094]\n",
      "--------------------------------------------------\n",
      "Step : 14\n",
      "Current State : [-0.26210636 -0.965039    4.3213944 ]\n",
      "Currrent Action : tensor([[0.0379]])\n",
      "Next State : [[-0.26210636]\n",
      " [-0.965039  ]\n",
      " [ 4.3213944 ]]\n",
      "Reward : [-6.69154983]\n",
      "--------------------------------------------------\n",
      "Step : 15\n",
      "Current State : [-0.07498479 -0.9971847   3.8029797 ]\n",
      "Currrent Action : tensor([[1.3691]])\n",
      "Next State : [[-0.07498479]\n",
      " [-0.9971847 ]\n",
      " [ 3.8029797 ]]\n",
      "Reward : [-5.24021728]\n",
      "--------------------------------------------------\n",
      "Step : 16\n",
      "Current State : [ 0.07586119 -0.9971184   3.0197878 ]\n",
      "Currrent Action : tensor([[-0.2354]])\n",
      "Next State : [[ 0.07586119]\n",
      " [-0.9971184 ]\n",
      " [ 3.0197878 ]]\n",
      "Reward : [-4.15514818]\n",
      "--------------------------------------------------\n",
      "Step : 17\n",
      "Current State : [ 0.17537175 -0.98450226  2.0069842 ]\n",
      "Currrent Action : tensor([[-1.7664]])\n",
      "Next State : [[ 0.17537175]\n",
      " [-0.98450226]\n",
      " [ 2.0069842 ]]\n",
      "Reward : [-3.14964503]\n",
      "--------------------------------------------------\n",
      "Step : 18\n",
      "Current State : [ 0.23594478 -0.9717665   1.2381464 ]\n",
      "Currrent Action : tensor([[-0.2031]])\n",
      "Next State : [[ 0.23594478]\n",
      " [-0.9717665 ]\n",
      " [ 1.2381464 ]]\n",
      "Reward : [-2.3475062]\n",
      "--------------------------------------------------\n",
      "Step : 19\n",
      "Current State : [ 0.26645923 -0.9638462   0.6305378 ]\n",
      "Currrent Action : tensor([[0.8081]])\n",
      "Next State : [[ 0.26645923]\n",
      " [-0.9638462 ]\n",
      " [ 0.6305378 ]]\n",
      "Reward : [-1.92979152]\n",
      "--------------------------------------------------\n",
      "Step : 20\n",
      "Current State : [ 0.26806304 -0.9634014   0.03328728]\n",
      "Currrent Action : tensor([[0.8376]])\n",
      "Next State : [[ 0.26806304]\n",
      " [-0.9634014 ]\n",
      " [ 0.03328728]]\n",
      "Reward : [-1.73326524]\n",
      "--------------------------------------------------\n",
      "Step : 21\n",
      "Current State : [ 0.2288913  -0.973452   -0.80886674]\n",
      "Currrent Action : tensor([[-0.7974]])\n",
      "Next State : [[ 0.2288913 ]\n",
      " [-0.973452  ]\n",
      " [-0.80886674]]\n",
      "Reward : [-1.68922435]\n",
      "--------------------------------------------------\n",
      "Step : 22\n",
      "Current State : [ 0.15923741 -0.9872403  -1.4204087 ]\n",
      "Currrent Action : tensor([[0.7903]])\n",
      "Next State : [[ 0.15923741]\n",
      " [-0.9872403 ]\n",
      " [-1.4204087 ]]\n",
      "Reward : [-1.86126988]\n",
      "--------------------------------------------------\n",
      "Step : 23\n",
      "Current State : [ 0.05598311 -0.99843174 -2.0781152 ]\n",
      "Currrent Action : tensor([[0.5515]])\n",
      "Next State : [[ 0.05598311]\n",
      " [-0.99843174]\n",
      " [-2.0781152 ]]\n",
      "Reward : [-2.19263743]\n",
      "--------------------------------------------------\n",
      "Step : 24\n",
      "Current State : [-0.09261248 -0.9957022  -2.9751556 ]\n",
      "Currrent Action : tensor([[-0.9881]])\n",
      "Next State : [[-0.09261248]\n",
      " [-0.9957022 ]\n",
      " [-2.9751556 ]]\n",
      "Reward : [-2.72740302]\n",
      "--------------------------------------------------\n",
      "Step : 25\n",
      "Current State : [-0.27699807 -0.9608705  -3.7584627 ]\n",
      "Currrent Action : tensor([[-0.2435]])\n",
      "Next State : [[-0.27699807]\n",
      " [-0.9608705 ]\n",
      " [-3.7584627 ]]\n",
      "Reward : [-3.65258536]\n",
      "--------------------------------------------------\n",
      "Step : 26\n",
      "Current State : [-0.49007043 -0.8716828  -4.630041  ]\n",
      "Currrent Action : tensor([[-1.0062]])\n",
      "Next State : [[-0.49007043]\n",
      " [-0.8716828 ]\n",
      " [-4.630041  ]]\n",
      "Reward : [-4.84153849]\n",
      "--------------------------------------------------\n",
      "Step : 27\n",
      "Current State : [-0.6942139  -0.71976876 -5.1031327 ]\n",
      "Currrent Action : tensor([[1.2045]])\n",
      "Next State : [[-0.6942139 ]\n",
      " [-0.71976876]\n",
      " [-5.1031327 ]]\n",
      "Reward : [-6.48392978]\n",
      "--------------------------------------------------\n",
      "Step : 28\n",
      "Current State : [-0.8636489 -0.5040938 -5.502735 ]\n",
      "Currrent Action : tensor([[0.9348]])\n",
      "Next State : [[-0.8636489]\n",
      " [-0.5040938]\n",
      " [-5.502735 ]]\n",
      "Reward : [-8.07189181]\n",
      "--------------------------------------------------\n",
      "Step : 29\n",
      "Current State : [-0.9702829  -0.24197318 -5.6786685 ]\n",
      "Currrent Action : tensor([[1.3476]])\n",
      "Next State : [[-0.9702829 ]\n",
      " [-0.24197318]\n",
      " [-5.6786685 ]]\n",
      "Reward : [-9.85895477]\n",
      "--------------------------------------------------\n",
      "Step : 30\n",
      "Current State : [-0.9994353   0.03360213 -5.5601482 ]\n",
      "Currrent Action : tensor([[2.1785]])\n",
      "Next State : [[-0.9994353 ]\n",
      " [ 0.03360213]\n",
      " [-5.5601482 ]]\n",
      "Reward : [-11.62245881]\n",
      "--------------------------------------------------\n",
      "Step : 31\n",
      "Current State : [-0.9514192  0.3078987 -5.587504 ]\n",
      "Currrent Action : tensor([[-0.3504]])\n",
      "Next State : [[-0.9514192]\n",
      " [ 0.3078987]\n",
      " [-5.587504 ]]\n",
      "Reward : [-12.7512134]\n",
      "--------------------------------------------------\n",
      "Step : 32\n",
      "Current State : [-0.8414084  0.5403998 -5.1585712]\n",
      "Currrent Action : tensor([[1.3201]])\n",
      "Next State : [[-0.8414084]\n",
      " [ 0.5403998]\n",
      " [-5.1585712]]\n",
      "Reward : [-11.12479126]\n",
      "--------------------------------------------------\n",
      "Step : 33\n",
      "Current State : [-0.6974262  0.7166566 -4.5616865]\n",
      "Currrent Action : tensor([[1.2772]])\n",
      "Next State : [[-0.6974262]\n",
      " [ 0.7166566]\n",
      " [-4.5616865]]\n",
      "Reward : [-9.27111485]\n",
      "--------------------------------------------------\n",
      "Step : 34\n",
      "Current State : [-0.54876304  0.835978   -3.8183193 ]\n",
      "Currrent Action : tensor([[1.3725]])\n",
      "Next State : [[-0.54876304]\n",
      " [ 0.835978  ]\n",
      " [-3.8183193 ]]\n",
      "Reward : [-7.57053884]\n",
      "--------------------------------------------------\n",
      "Step : 35\n",
      "Current State : [-0.40385693  0.91482216 -3.3030968 ]\n",
      "Currrent Action : tensor([[-0.7451]])\n",
      "Next State : [[-0.40385693]\n",
      " [ 0.91482216]\n",
      " [-3.3030968 ]]\n",
      "Reward : [-6.08823891]\n",
      "--------------------------------------------------\n",
      "Step : 36\n",
      "Current State : [-0.27680928  0.96092486 -2.7051387 ]\n",
      "Currrent Action : tensor([[-0.5877]])\n",
      "Next State : [[-0.27680928]\n",
      " [ 0.96092486]\n",
      " [-2.7051387 ]]\n",
      "Reward : [-5.03767305]\n",
      "--------------------------------------------------\n",
      "Step : 37\n",
      "Current State : [-0.1774701  0.9841262 -2.041138 ]\n",
      "Currrent Action : tensor([[-0.3780]])\n",
      "Next State : [[-0.1774701]\n",
      " [ 0.9841262]\n",
      " [-2.041138 ]]\n",
      "Reward : [-4.159115]\n",
      "--------------------------------------------------\n",
      "Step : 38\n",
      "Current State : [-0.11484126  0.9933839  -1.266399  ]\n",
      "Currrent Action : tensor([[0.2443]])\n",
      "Next State : [[-0.11484126]\n",
      " [ 0.9933839 ]\n",
      " [-1.266399  ]]\n",
      "Reward : [-3.47642491]\n",
      "--------------------------------------------------\n",
      "Step : 39\n",
      "Current State : [-0.10361706  0.9946173  -0.2258365 ]\n",
      "Currrent Action : tensor([[1.9702]])\n",
      "Next State : [[-0.10361706]\n",
      " [ 0.9946173 ]\n",
      " [-0.2258365 ]]\n",
      "Reward : [-3.00648843]\n",
      "--------------------------------------------------\n",
      "Step : 40\n",
      "Current State : [-0.13229504  0.9912104   0.577613  ]\n",
      "Currrent Action : tensor([[0.3832]])\n",
      "Next State : [[-0.13229504]\n",
      " [ 0.9912104 ]\n",
      " [ 0.577613  ]]\n",
      "Reward : [-2.80953125]\n",
      "--------------------------------------------------\n",
      "Step : 41\n",
      "Current State : [-0.20070359  0.979652    1.3878411 ]\n",
      "Currrent Action : tensor([[0.4455]])\n",
      "Next State : [[-0.20070359]\n",
      " [ 0.979652  ]\n",
      " [ 1.3878411 ]]\n",
      "Reward : [-2.93540742]\n",
      "--------------------------------------------------\n",
      "Step : 42\n",
      "Current State : [-0.2959884   0.95519155  1.9682808 ]\n",
      "Currrent Action : tensor([[-1.0287]])\n",
      "Next State : [[-0.2959884 ]\n",
      " [ 0.95519155]\n",
      " [ 1.9682808 ]]\n",
      "Reward : [-3.336745]\n",
      "--------------------------------------------------\n",
      "Step : 43\n",
      "Current State : [-0.42179868  0.9066895   2.6987615 ]\n",
      "Currrent Action : tensor([[0.0939]])\n",
      "Next State : [[-0.42179868]\n",
      " [ 0.9066895 ]\n",
      " [ 2.6987615 ]]\n",
      "Reward : [-3.88913471]\n",
      "--------------------------------------------------\n",
      "Step : 44\n",
      "Current State : [-0.57702863  0.8167239   3.5931535 ]\n",
      "Currrent Action : tensor([[1.4292]])\n",
      "Next State : [[-0.57702863]\n",
      " [ 0.8167239 ]\n",
      " [ 3.5931535 ]]\n",
      "Reward : [-4.75531067]\n",
      "--------------------------------------------------\n",
      "Step : 45\n",
      "Current State : [-0.73629016  0.67666596  4.24971   ]\n",
      "Currrent Action : tensor([[0.2934]])\n",
      "Next State : [[-0.73629016]\n",
      " [ 0.67666596]\n",
      " [ 4.24971   ]]\n",
      "Reward : [-6.06924219]\n",
      "--------------------------------------------------\n",
      "Step : 46\n",
      "Current State : [-0.8721205   0.48929116  4.638963  ]\n",
      "Currrent Action : tensor([[-0.7883]])\n",
      "Next State : [[-0.8721205 ]\n",
      " [ 0.48929116]\n",
      " [ 4.638963  ]]\n",
      "Reward : [-7.55879237]\n",
      "--------------------------------------------------\n",
      "Step : 47\n",
      "Current State : [-0.9679131  0.2512852  5.1453795]\n",
      "Currrent Action : tensor([[0.9297]])\n",
      "Next State : [[-0.9679131]\n",
      " [ 0.2512852]\n",
      " [ 5.1453795]]\n",
      "Reward : [-9.0714239]\n",
      "--------------------------------------------------\n",
      "Step : 48\n",
      "Current State : [-0.9996168 -0.0276808  5.6338434]\n",
      "Currrent Action : tensor([[2.1066]])\n",
      "Next State : [[-0.9996168]\n",
      " [-0.0276808]\n",
      " [ 5.6338434]]\n",
      "Reward : [-10.98963907]\n",
      "--------------------------------------------------\n",
      "Step : 49\n",
      "Current State : [-0.95107245 -0.30896798  5.728468  ]\n",
      "Currrent Action : tensor([[0.7692]])\n",
      "Next State : [[-0.95107245]\n",
      " [-0.30896798]\n",
      " [ 5.728468  ]]\n",
      "Reward : [-12.87103579]\n",
      "--------------------------------------------------\n",
      "Step : 50\n",
      "Current State : [-0.82846665 -0.56003845  5.606489  ]\n",
      "Currrent Action : tensor([[0.7316]])\n",
      "Next State : [[-0.82846665]\n",
      " [-0.56003845]\n",
      " [ 5.606489  ]]\n",
      "Reward : [-11.27674071]\n",
      "--------------------------------------------------\n",
      "Step : 51\n",
      "Current State : [-0.65443295 -0.75612     5.258634  ]\n",
      "Currrent Action : tensor([[0.4812]])\n",
      "Next State : [[-0.65443295]\n",
      " [-0.75612   ]\n",
      " [ 5.258634  ]]\n",
      "Reward : [-9.63153008]\n",
      "--------------------------------------------------\n",
      "Step : 52\n",
      "Current State : [-0.45297673 -0.89152235  4.866619  ]\n",
      "Currrent Action : tensor([[1.1672]])\n",
      "Next State : [[-0.45297673]\n",
      " [-0.89152235]\n",
      " [ 4.866619  ]]\n",
      "Reward : [-7.98438667]\n",
      "--------------------------------------------------\n",
      "Step : 53\n",
      "Current State : [-0.26622146 -0.9639119   4.0126114 ]\n",
      "Currrent Action : tensor([[-1.2358]])\n",
      "Next State : [[-0.26622146]\n",
      " [-0.9639119 ]\n",
      " [ 4.0126114 ]]\n",
      "Reward : [-6.53518915]\n",
      "--------------------------------------------------\n",
      "Step : 54\n",
      "Current State : [-0.09018548 -0.995925    3.5832543 ]\n",
      "Currrent Action : tensor([[1.9572]])\n",
      "Next State : [[-0.09018548]\n",
      " [-0.995925  ]\n",
      " [ 3.5832543 ]]\n",
      "Reward : [-5.00051896]\n",
      "--------------------------------------------------\n",
      "Step : 55\n",
      "Current State : [ 0.04547906 -0.99896526  2.7160585 ]\n",
      "Currrent Action : tensor([[-0.8017]])\n",
      "Next State : [[ 0.04547906]\n",
      " [-0.99896526]\n",
      " [ 2.7160585 ]]\n",
      "Reward : [-4.04388203]\n",
      "--------------------------------------------------\n",
      "Step : 56\n",
      "Current State : [ 0.14027509 -0.99011254  1.9048902 ]\n",
      "Currrent Action : tensor([[-0.4130]])\n",
      "Next State : [[ 0.14027509]\n",
      " [-0.99011254]\n",
      " [ 1.9048902 ]]\n",
      "Reward : [-3.06441286]\n",
      "--------------------------------------------------\n",
      "Step : 57\n",
      "Current State : [ 0.2122282 -0.9772201  1.4623057]\n",
      "Currrent Action : tensor([[2.1072]])\n",
      "Next State : [[ 0.2122282]\n",
      " [-0.9772201]\n",
      " [ 1.4623057]]\n",
      "Reward : [-2.41192387]\n",
      "--------------------------------------------------\n",
      "Step : 58\n",
      "Current State : [ 0.24034658 -0.97068715  0.5773668 ]\n",
      "Currrent Action : tensor([[-1.0135]])\n",
      "Next State : [[ 0.24034658]\n",
      " [-0.97068715]\n",
      " [ 0.5773668 ]]\n",
      "Reward : [-2.05615197]\n",
      "--------------------------------------------------\n",
      "Step : 59\n",
      "Current State : [ 0.24758849 -0.9688653   0.14935149]\n",
      "Currrent Action : tensor([[2.1708]])\n",
      "Next State : [[ 0.24758849]\n",
      " [-0.9688653 ]\n",
      " [ 0.14935149]]\n",
      "Reward : [-1.80111432]\n",
      "--------------------------------------------------\n",
      "Step : 60\n",
      "Current State : [ 0.21863733 -0.9758062  -0.59545314]\n",
      "Currrent Action : tensor([[-0.1210]])\n",
      "Next State : [[ 0.21863733]\n",
      " [-0.9758062 ]\n",
      " [-0.59545314]]\n",
      "Reward : [-1.7462451]\n",
      "--------------------------------------------------\n",
      "Step : 61\n",
      "Current State : [ 0.16291331 -0.9866404  -1.1355022 ]\n",
      "Currrent Action : tensor([[1.2787]])\n",
      "Next State : [[ 0.16291331]\n",
      " [-0.9866404 ]\n",
      " [-1.1355022 ]]\n",
      "Reward : [-1.86061369]\n",
      "--------------------------------------------------\n",
      "Step : 62\n",
      "Current State : [ 0.07055835 -0.99750763 -1.8605132 ]\n",
      "Currrent Action : tensor([[0.0998]])\n",
      "Next State : [[ 0.07055835]\n",
      " [-0.99750763]\n",
      " [-1.8605132 ]]\n",
      "Reward : [-2.10902785]\n",
      "--------------------------------------------------\n",
      "Step : 63\n",
      "Current State : [-0.05354465 -0.99856544 -2.483746  ]\n",
      "Currrent Action : tensor([[0.8327]])\n",
      "Next State : [[-0.05354465]\n",
      " [-0.99856544]\n",
      " [-2.483746  ]]\n",
      "Reward : [-2.59738217]\n",
      "--------------------------------------------------\n",
      "Step : 64\n",
      "Current State : [-0.21810588 -0.9759251  -3.3260586 ]\n",
      "Currrent Action : tensor([[-0.6226]])\n",
      "Next State : [[-0.21810588]\n",
      " [-0.9759251 ]\n",
      " [-3.3260586 ]]\n",
      "Reward : [-3.25585384]\n",
      "--------------------------------------------------\n",
      "Step : 65\n",
      "Current State : [-0.42202914 -0.9065823  -4.3161836 ]\n",
      "Currrent Action : tensor([[-1.7212]])\n",
      "Next State : [[-0.42202914]\n",
      " [-0.9065823 ]\n",
      " [-4.3161836 ]]\n",
      "Reward : [-4.31572651]\n",
      "--------------------------------------------------\n",
      "Step : 66\n",
      "Current State : [-0.6354714 -0.7721244 -5.058731 ]\n",
      "Currrent Action : tensor([[-0.4174]])\n",
      "Next State : [[-0.6354714]\n",
      " [-0.7721244]\n",
      " [-5.058731 ]]\n",
      "Reward : [-5.88907497]\n",
      "--------------------------------------------------\n",
      "Step : 67\n",
      "Current State : [-0.8194288  -0.57318103 -5.4358964 ]\n",
      "Currrent Action : tensor([[1.3462]])\n",
      "Next State : [[-0.8194288 ]\n",
      " [-0.57318103]\n",
      " [-5.4358964 ]]\n",
      "Reward : [-7.66584528]\n",
      "--------------------------------------------------\n",
      "Step : 68\n",
      "Current State : [-0.949908   -0.31252962 -5.850554  ]\n",
      "Currrent Action : tensor([[0.1015]])\n",
      "Next State : [[-0.949908  ]\n",
      " [-0.31252962]\n",
      " [-5.850554  ]]\n",
      "Reward : [-9.3619317]\n",
      "--------------------------------------------------\n",
      "Step : 69\n",
      "Current State : [-0.9998932  -0.01461536 -6.06478   ]\n",
      "Currrent Action : tensor([[0.1345]])\n",
      "Next State : [[-0.9998932 ]\n",
      " [-0.01461536]\n",
      " [-6.06478   ]]\n",
      "Reward : [-11.39641141]\n",
      "--------------------------------------------------\n",
      "Step : 70\n",
      "Current State : [-0.96007687  0.2797363  -5.9627066 ]\n",
      "Currrent Action : tensor([[0.7536]])\n",
      "Next State : [[-0.96007687]\n",
      " [ 0.2797363 ]\n",
      " [-5.9627066 ]]\n",
      "Reward : [-13.45670746]\n",
      "--------------------------------------------------\n",
      "Step : 71\n",
      "Current State : [-0.8386934   0.54460394 -5.847947  ]\n",
      "Currrent Action : tensor([[-0.6336]])\n",
      "Next State : [[-0.8386934 ]\n",
      " [ 0.54460394]\n",
      " [-5.847947  ]]\n",
      "Reward : [-11.72437085]\n",
      "--------------------------------------------------\n",
      "Step : 72\n",
      "Current State : [-0.65413445  0.7563783  -5.6368365 ]\n",
      "Currrent Action : tensor([[-1.3156]])\n",
      "Next State : [[-0.65413445]\n",
      " [ 0.7563783 ]\n",
      " [-5.6368365 ]]\n",
      "Reward : [-10.00427214]\n",
      "--------------------------------------------------\n",
      "Step : 73\n",
      "Current State : [-0.44231167  0.89686143 -5.097263  ]\n",
      "Currrent Action : tensor([[-0.1847]])\n",
      "Next State : [[-0.44231167]\n",
      " [ 0.89686143]\n",
      " [-5.097263  ]]\n",
      "Reward : [-8.39332433]\n",
      "--------------------------------------------------\n",
      "Step : 74\n",
      "Current State : [-0.24823742  0.9686992  -4.146285  ]\n",
      "Currrent Action : tensor([[1.8555]])\n",
      "Next State : [[-0.24823742]\n",
      " [ 0.9686992 ]\n",
      " [-4.146285  ]]\n",
      "Reward : [-6.71837469]\n",
      "--------------------------------------------------\n",
      "Step : 75\n",
      "Current State : [-0.08829693  0.9960942  -3.2489645 ]\n",
      "Currrent Action : tensor([[1.1386]])\n",
      "Next State : [[-0.08829693]\n",
      " [ 0.9960942 ]\n",
      " [-3.2489645 ]]\n",
      "Reward : [-5.0388974]\n",
      "--------------------------------------------------\n",
      "Step : 76\n",
      "Current State : [ 0.03959834  0.99921566 -2.5604153 ]\n",
      "Currrent Action : tensor([[-0.3901]])\n",
      "Next State : [[ 0.03959834]\n",
      " [ 0.99921566]\n",
      " [-2.5604153 ]]\n",
      "Reward : [-3.80870171]\n",
      "--------------------------------------------------\n",
      "Step : 77\n",
      "Current State : [ 0.11933778  0.9928537  -1.6002836 ]\n",
      "Currrent Action : tensor([[1.4048]])\n",
      "Next State : [[ 0.11933778]\n",
      " [ 0.9928537 ]\n",
      " [-1.6002836 ]]\n",
      "Reward : [-3.00208167]\n",
      "--------------------------------------------------\n",
      "Step : 78\n",
      "Current State : [ 0.1578968  0.9874556 -0.7787499]\n",
      "Currrent Action : tensor([[0.5126]])\n",
      "Next State : [[ 0.1578968]\n",
      " [ 0.9874556]\n",
      " [-0.7787499]]\n",
      "Reward : [-2.36225793]\n",
      "--------------------------------------------------\n",
      "Step : 79\n",
      "Current State : [ 0.16149452  0.9868736  -0.07289   ]\n",
      "Currrent Action : tensor([[-0.2315]])\n",
      "Next State : [[ 0.16149452]\n",
      " [ 0.9868736 ]\n",
      " [-0.07289   ]]\n",
      "Reward : [-2.05510916]\n",
      "--------------------------------------------------\n",
      "Step : 80\n",
      "Current State : [0.13728416 0.9905317  0.48971567]\n",
      "Currrent Action : tensor([[-1.1837]])\n",
      "Next State : [[0.13728416]\n",
      " [0.9905317 ]\n",
      " [0.48971567]]\n",
      "Reward : [-1.98606226]\n",
      "--------------------------------------------------\n",
      "Step : 81\n",
      "Current State : [0.07433512 0.99723333 1.2663068 ]\n",
      "Currrent Action : tensor([[0.2246]])\n",
      "Next State : [[0.07433512]\n",
      " [0.99723333]\n",
      " [1.2663068 ]]\n",
      "Reward : [-2.07774298]\n",
      "--------------------------------------------------\n",
      "Step : 82\n",
      "Current State : [-0.03027019  0.99954176  2.0935712 ]\n",
      "Currrent Action : tensor([[0.5289]])\n",
      "Next State : [[-0.03027019]\n",
      " [ 0.99954176]\n",
      " [ 2.0935712 ]]\n",
      "Reward : [-2.39982378]\n",
      "--------------------------------------------------\n",
      "Step : 83\n",
      "Current State : [-0.17993818  0.9836779   3.0129757 ]\n",
      "Currrent Action : tensor([[1.1317]])\n",
      "Next State : [[-0.17993818]\n",
      " [ 0.9836779 ]\n",
      " [ 3.0129757 ]]\n",
      "Reward : [-3.00301351]\n",
      "--------------------------------------------------\n",
      "Step : 84\n",
      "Current State : [-0.35475937  0.9349576   3.6346612 ]\n",
      "Currrent Action : tensor([[-0.7738]])\n",
      "Next State : [[-0.35475937]\n",
      " [ 0.9349576 ]\n",
      " [ 3.6346612 ]]\n",
      "Reward : [-3.97692381]\n",
      "--------------------------------------------------\n",
      "Step : 85\n",
      "Current State : [-0.55141324  0.8342322   4.4280143 ]\n",
      "Currrent Action : tensor([[0.6142]])\n",
      "Next State : [[-0.55141324]\n",
      " [ 0.8342322 ]\n",
      " [ 4.4280143 ]]\n",
      "Reward : [-5.05969401]\n",
      "--------------------------------------------------\n",
      "Step : 86\n",
      "Current State : [-0.7418952   0.67051584  5.036702  ]\n",
      "Currrent Action : tensor([[-0.1132]])\n",
      "Next State : [[-0.7418952 ]\n",
      " [ 0.67051584]\n",
      " [ 5.036702  ]]\n",
      "Reward : [-6.60413819]\n",
      "--------------------------------------------------\n",
      "Step : 87\n",
      "Current State : [-0.90350217  0.4285835   5.839589  ]\n",
      "Currrent Action : tensor([[2.3160]])\n",
      "Next State : [[-0.90350217]\n",
      " [ 0.4285835 ]\n",
      " [ 5.839589  ]]\n",
      "Reward : [-8.33298781]\n",
      "--------------------------------------------------\n",
      "Step : 88\n",
      "Current State : [-0.99013406  0.14012337  6.0467677 ]\n",
      "Currrent Action : tensor([[-0.7617]])\n",
      "Next State : [[-0.99013406]\n",
      " [ 0.14012337]\n",
      " [ 6.0467677 ]]\n",
      "Reward : [-10.6934705]\n",
      "--------------------------------------------------\n",
      "Step : 89\n",
      "Current State : [-0.98541373 -0.17017579  6.231881  ]\n",
      "Currrent Action : tensor([[0.5335]])\n",
      "Next State : [[-0.98541373]\n",
      " [-0.17017579]\n",
      " [ 6.231881  ]]\n",
      "Reward : [-12.66266548]\n",
      "--------------------------------------------------\n",
      "Step : 90\n",
      "Current State : [-0.88372105 -0.46801394  6.32068   ]\n",
      "Currrent Action : tensor([[1.4429]])\n",
      "Next State : [[-0.88372105]\n",
      " [-0.46801394]\n",
      " [ 6.32068   ]]\n",
      "Reward : [-12.71008923]\n",
      "--------------------------------------------------\n",
      "Step : 91\n",
      "Current State : [-0.7060517 -0.7081603  5.9969425]\n",
      "Currrent Action : tensor([[0.1818]])\n",
      "Next State : [[-0.7060517]\n",
      " [-0.7081603]\n",
      " [ 5.9969425]]\n",
      "Reward : [-11.0417715]\n",
      "--------------------------------------------------\n",
      "Step : 92\n",
      "Current State : [-0.47962075 -0.87747586  5.6736937 ]\n",
      "Currrent Action : tensor([[1.3858]])\n",
      "Next State : [[-0.47962075]\n",
      " [-0.87747586]\n",
      " [ 5.6736937 ]]\n",
      "Reward : [-9.14288089]\n",
      "--------------------------------------------------\n",
      "Step : 93\n",
      "Current State : [-0.24868967 -0.9685832   4.9779058 ]\n",
      "Currrent Action : tensor([[-0.2512]])\n",
      "Next State : [[-0.24868967]\n",
      " [-0.9685832 ]\n",
      " [ 4.9779058 ]]\n",
      "Reward : [-7.50826177]\n",
      "--------------------------------------------------\n",
      "Step : 94\n",
      "Current State : [-0.03593391 -0.9993542   4.3077106 ]\n",
      "Currrent Action : tensor([[0.3749]])\n",
      "Next State : [[-0.03593391]\n",
      " [-0.9993542 ]\n",
      " [ 4.3077106 ]]\n",
      "Reward : [-5.79822927]\n",
      "--------------------------------------------------\n",
      "Step : 95\n",
      "Current State : [ 0.13953047 -0.9902178   3.5185776 ]\n",
      "Currrent Action : tensor([[-0.2641]])\n",
      "Next State : [[ 0.13953047]\n",
      " [-0.9902178 ]\n",
      " [ 3.5185776 ]]\n",
      "Reward : [-4.43731382]\n",
      "--------------------------------------------------\n",
      "Step : 96\n",
      "Current State : [ 0.28071845 -0.95979017  2.8911068 ]\n",
      "Currrent Action : tensor([[0.7680]])\n",
      "Next State : [[ 0.28071845]\n",
      " [-0.95979017]\n",
      " [ 2.8911068 ]]\n",
      "Reward : [-3.28584317]\n",
      "--------------------------------------------------\n",
      "Step : 97\n",
      "Current State : [ 0.3850302  -0.92290395  2.21396   ]\n",
      "Currrent Action : tensor([[0.2846]])\n",
      "Next State : [[ 0.3850302 ]\n",
      " [-0.92290395]\n",
      " [ 2.21396   ]]\n",
      "Reward : [-2.49037963]\n",
      "--------------------------------------------------\n",
      "Step : 98\n",
      "Current State : [ 0.44587588 -0.8950948   1.3382401 ]\n",
      "Currrent Action : tensor([[-1.2236]])\n",
      "Next State : [[ 0.44587588]\n",
      " [-0.8950948 ]\n",
      " [ 1.3382401 ]]\n",
      "Reward : [-1.87359041]\n",
      "--------------------------------------------------\n",
      "Step : 99\n",
      "Current State : [ 0.48861226 -0.872501    0.966919  ]\n",
      "Currrent Action : tensor([[2.4389]])\n",
      "Next State : [[ 0.48861226]\n",
      " [-0.872501  ]\n",
      " [ 0.966919  ]]\n",
      "Reward : [-1.41217964]\n",
      "learning iteration : 23\n",
      "--------------------------------------------------\n",
      "Step : 0\n",
      "Current State : [-0.78916204  0.61418504  1.321794  ]\n",
      "Currrent Action : tensor([[0.3298]])\n",
      "Next State : [[-0.78916204]\n",
      " [ 0.61418504]\n",
      " [ 1.321794  ]]\n",
      "Reward : [-5.88807598]\n",
      "--------------------------------------------------\n",
      "Step : 1\n",
      "Current State : [-0.8408633  0.5412476  1.7886546]\n",
      "Currrent Action : tensor([[0.0415]])\n",
      "Next State : [[-0.8408633]\n",
      " [ 0.5412476]\n",
      " [ 1.7886546]]\n",
      "Reward : [-6.32630512]\n",
      "--------------------------------------------------\n",
      "Step : 2\n",
      "Current State : [-0.8936513   0.44876203  2.130811  ]\n",
      "Currrent Action : tensor([[-0.4252]])\n",
      "Next State : [[-0.8936513 ]\n",
      " [ 0.44876203]\n",
      " [ 2.130811  ]]\n",
      "Reward : [-6.92332623]\n",
      "--------------------------------------------------\n",
      "Step : 3\n",
      "Current State : [-0.94151    0.336985   2.4333365]\n",
      "Currrent Action : tensor([[-0.2270]])\n",
      "Next State : [[-0.94151  ]\n",
      " [ 0.336985 ]\n",
      " [ 2.4333365]]\n",
      "Reward : [-7.61620353]\n",
      "--------------------------------------------------\n",
      "Step : 4\n",
      "Current State : [-0.9793769   0.20204182  2.8054092 ]\n",
      "Currrent Action : tensor([[0.7956]])\n",
      "Next State : [[-0.9793769 ]\n",
      " [ 0.20204182]\n",
      " [ 2.8054092 ]]\n",
      "Reward : [-8.42087757]\n",
      "--------------------------------------------------\n",
      "Step : 5\n",
      "Current State : [-0.9987283   0.05041665  3.060085  ]\n",
      "Currrent Action : tensor([[0.6876]])\n",
      "Next State : [[-0.9987283 ]\n",
      " [ 0.05041665]\n",
      " [ 3.060085  ]]\n",
      "Reward : [-9.42023244]\n",
      "--------------------------------------------------\n",
      "Step : 6\n",
      "Current State : [-0.9950724  -0.09915065  2.9950373 ]\n",
      "Currrent Action : tensor([[-0.6857]])\n",
      "Next State : [[-0.9950724 ]\n",
      " [-0.09915065]\n",
      " [ 2.9950373 ]]\n",
      "Reward : [-10.49211914]\n",
      "--------------------------------------------------\n",
      "Step : 7\n",
      "Current State : [-0.96646833 -0.25678578  3.2076232 ]\n",
      "Currrent Action : tensor([[1.9130]])\n",
      "Next State : [[-0.96646833]\n",
      " [-0.25678578]\n",
      " [ 3.2076232 ]]\n",
      "Reward : [-10.15614485]\n",
      "--------------------------------------------------\n",
      "Step : 8\n",
      "Current State : [-0.9158139 -0.4016029  3.0714288]\n",
      "Currrent Action : tensor([[0.3760]])\n",
      "Next State : [[-0.9158139]\n",
      " [-0.4016029]\n",
      " [ 3.0714288]]\n",
      "Reward : [-9.33436013]\n",
      "--------------------------------------------------\n",
      "Step : 9\n",
      "Current State : [-0.84672576 -0.5320296   2.954587  ]\n",
      "Currrent Action : tensor([[1.2291]])\n",
      "Next State : [[-0.84672576]\n",
      " [-0.5320296 ]\n",
      " [ 2.954587  ]]\n",
      "Reward : [-8.38864205]\n",
      "--------------------------------------------------\n",
      "Step : 10\n",
      "Current State : [-0.7698378  -0.63823956  2.6242719 ]\n",
      "Currrent Action : tensor([[0.4580]])\n",
      "Next State : [[-0.7698378 ]\n",
      " [-0.63823956]\n",
      " [ 2.6242719 ]]\n",
      "Reward : [-7.53264852]\n",
      "--------------------------------------------------\n",
      "Step : 11\n",
      "Current State : [-0.6972851 -0.7167939  2.1396804]\n",
      "Currrent Action : tensor([[-0.0394]])\n",
      "Next State : [[-0.6972851]\n",
      " [-0.7167939]\n",
      " [ 2.1396804]]\n",
      "Reward : [-6.68816033]\n",
      "--------------------------------------------------\n",
      "Step : 12\n",
      "Current State : [-0.63078105 -0.77596086  1.7808715 ]\n",
      "Currrent Action : tensor([[1.1919]])\n",
      "Next State : [[-0.63078105]\n",
      " [-0.77596086]\n",
      " [ 1.7808715 ]]\n",
      "Reward : [-5.94607819]\n",
      "--------------------------------------------------\n",
      "Step : 13\n",
      "Current State : [-0.5797085 -0.8148239  1.2837692]\n",
      "Currrent Action : tensor([[0.5658]])\n",
      "Next State : [[-0.5797085]\n",
      " [-0.8148239]\n",
      " [ 1.2837692]]\n",
      "Reward : [-5.39508236]\n",
      "--------------------------------------------------\n",
      "Step : 14\n",
      "Current State : [-0.5518396  -0.8339503   0.67604864]\n",
      "Currrent Action : tensor([[0.0226]])\n",
      "Next State : [[-0.5518396 ]\n",
      " [-0.8339503 ]\n",
      " [ 0.67604864]]\n",
      "Reward : [-4.95726007]\n",
      "--------------------------------------------------\n",
      "Step : 15\n",
      "Current State : [-0.56104213 -0.8277872  -0.22151457]\n",
      "Currrent Action : tensor([[-1.8140]])\n",
      "Next State : [[-0.56104213]\n",
      " [-0.8277872 ]\n",
      " [-0.22151457]]\n",
      "Reward : [-4.69459224]\n",
      "--------------------------------------------------\n",
      "Step : 16\n",
      "Current State : [-0.59516513 -0.8036034  -0.8365381 ]\n",
      "Currrent Action : tensor([[0.0388]])\n",
      "Next State : [[-0.59516513]\n",
      " [-0.8036034 ]\n",
      " [-0.8365381 ]]\n",
      "Reward : [-4.69837297]\n",
      "--------------------------------------------------\n",
      "Step : 17\n",
      "Current State : [-0.6508776 -0.7591827 -1.4253747]\n",
      "Currrent Action : tensor([[0.0924]])\n",
      "Next State : [[-0.6508776]\n",
      " [-0.7591827]\n",
      " [-1.4253747]]\n",
      "Reward : [-4.94643322]\n",
      "--------------------------------------------------\n",
      "Step : 18\n",
      "Current State : [-0.722768   -0.69109076 -1.9811875 ]\n",
      "Currrent Action : tensor([[0.0905]])\n",
      "Next State : [[-0.722768  ]\n",
      " [-0.69109076]\n",
      " [-1.9811875 ]]\n",
      "Reward : [-5.39946267]\n",
      "--------------------------------------------------\n",
      "Step : 19\n",
      "Current State : [-0.7977669  -0.60296595 -2.315668  ]\n",
      "Currrent Action : tensor([[1.2256]])\n",
      "Next State : [[-0.7977669 ]\n",
      " [-0.60296595]\n",
      " [-2.315668  ]]\n",
      "Reward : [-6.05172923]\n",
      "--------------------------------------------------\n",
      "Step : 20\n",
      "Current State : [-0.8727309  -0.48820153 -2.7437174 ]\n",
      "Currrent Action : tensor([[0.1612]])\n",
      "Next State : [[-0.8727309 ]\n",
      " [-0.48820153]\n",
      " [-2.7437174 ]]\n",
      "Reward : [-6.75818414]\n",
      "--------------------------------------------------\n",
      "Step : 21\n",
      "Current State : [-0.93679875 -0.34986869 -3.051938  ]\n",
      "Currrent Action : tensor([[0.3862]])\n",
      "Next State : [[-0.93679875]\n",
      " [-0.34986869]\n",
      " [-3.051938  ]]\n",
      "Reward : [-7.67808115]\n",
      "--------------------------------------------------\n",
      "Step : 22\n",
      "Current State : [-0.9797887  -0.20003508 -3.1207442 ]\n",
      "Currrent Action : tensor([[1.2906]])\n",
      "Next State : [[-0.9797887 ]\n",
      " [-0.20003508]\n",
      " [-3.1207442 ]]\n",
      "Reward : [-8.68465475]\n",
      "--------------------------------------------------\n",
      "Step : 23\n",
      "Current State : [-0.99900895 -0.04450959 -3.1373885 ]\n",
      "Currrent Action : tensor([[0.8892]])\n",
      "Next State : [[-0.99900895]\n",
      " [-0.04450959]\n",
      " [-3.1373885 ]]\n",
      "Reward : [-9.61946482]\n",
      "--------------------------------------------------\n",
      "Step : 24\n",
      "Current State : [-0.9951021   0.09885254 -2.8707707 ]\n",
      "Currrent Action : tensor([[2.3488]])\n",
      "Next State : [[-0.9951021 ]\n",
      " [ 0.09885254]\n",
      " [-2.8707707 ]]\n",
      "Reward : [-10.58015312]\n",
      "--------------------------------------------------\n",
      "Step : 25\n",
      "Current State : [-0.975051    0.22198112 -2.4966314 ]\n",
      "Currrent Action : tensor([[2.2435]])\n",
      "Next State : [[-0.975051  ]\n",
      " [ 0.22198112]\n",
      " [-2.4966314 ]]\n",
      "Reward : [-10.08541588]\n",
      "--------------------------------------------------\n",
      "Step : 26\n",
      "Current State : [-0.94412255  0.32959464 -2.2405674 ]\n",
      "Currrent Action : tensor([[0.5972]])\n",
      "Next State : [[-0.94412255]\n",
      " [ 0.32959464]\n",
      " [-2.2405674 ]]\n",
      "Reward : [-9.13692009]\n",
      "--------------------------------------------------\n",
      "Step : 27\n",
      "Current State : [-0.9071516   0.42080393 -1.9691436 ]\n",
      "Currrent Action : tensor([[0.1615]])\n",
      "Next State : [[-0.9071516 ]\n",
      " [ 0.42080393]\n",
      " [-1.9691436 ]]\n",
      "Reward : [-8.37409649]\n",
      "--------------------------------------------------\n",
      "Step : 28\n",
      "Current State : [-0.86648035  0.4992112  -1.7671359 ]\n",
      "Currrent Action : tensor([[-0.7573]])\n",
      "Next State : [[-0.86648035]\n",
      " [ 0.4992112 ]\n",
      " [-1.7671359 ]]\n",
      "Reward : [-7.71758983]\n",
      "--------------------------------------------------\n",
      "Step : 29\n",
      "Current State : [-0.8244983   0.56586444 -1.5758634 ]\n",
      "Currrent Action : tensor([[-1.2209]])\n",
      "Next State : [[-0.8244983 ]\n",
      " [ 0.56586444]\n",
      " [-1.5758634 ]]\n",
      "Reward : [-7.17242825]\n",
      "--------------------------------------------------\n",
      "Step : 30\n",
      "Current State : [-0.793785    0.60819846 -1.0461553 ]\n",
      "Currrent Action : tensor([[0.7021]])\n",
      "Next State : [[-0.793785  ]\n",
      " [ 0.60819846]\n",
      " [-1.0461553 ]]\n",
      "Reward : [-6.70099296]\n",
      "--------------------------------------------------\n",
      "Step : 31\n",
      "Current State : [-0.7729162   0.63450813 -0.6716575 ]\n",
      "Currrent Action : tensor([[-0.5443]])\n",
      "Next State : [[-0.7729162 ]\n",
      " [ 0.63450813]\n",
      " [-0.6716575 ]]\n",
      "Reward : [-6.29890694]\n",
      "--------------------------------------------------\n",
      "Step : 32\n",
      "Current State : [-0.7631014   0.64627874 -0.30651745]\n",
      "Currrent Action : tensor([[-0.7383]])\n",
      "Next State : [[-0.7631014 ]\n",
      " [ 0.64627874]\n",
      " [-0.30651745]]\n",
      "Reward : [-6.06885658]\n",
      "--------------------------------------------------\n",
      "Step : 33\n",
      "Current State : [-0.7744587   0.63262445  0.3552113 ]\n",
      "Currrent Action : tensor([[1.1801]])\n",
      "Next State : [[-0.7744587 ]\n",
      " [ 0.63262445]\n",
      " [ 0.3552113 ]]\n",
      "Reward : [-5.95899589]\n",
      "--------------------------------------------------\n",
      "Step : 34\n",
      "Current State : [-0.79922676  0.60102963  0.80297005]\n",
      "Currrent Action : tensor([[-0.1781]])\n",
      "Next State : [[-0.79922676]\n",
      " [ 0.60102963]\n",
      " [ 0.80297005]]\n",
      "Reward : [-6.04780484]\n",
      "--------------------------------------------------\n",
      "Step : 35\n",
      "Current State : [-0.82984626  0.5579921   1.0564917 ]\n",
      "Currrent Action : tensor([[-1.3150]])\n",
      "Next State : [[-0.82984626]\n",
      " [ 0.5579921 ]\n",
      " [ 1.0564917 ]]\n",
      "Reward : [-6.30023493]\n",
      "--------------------------------------------------\n",
      "Step : 36\n",
      "Current State : [-0.86770386  0.49708155  1.4346421 ]\n",
      "Currrent Action : tensor([[-0.2690]])\n",
      "Next State : [[-0.86770386]\n",
      " [ 0.49708155]\n",
      " [ 1.4346421 ]]\n",
      "Reward : [-6.61229513]\n",
      "--------------------------------------------------\n",
      "Step : 37\n",
      "Current State : [-0.9129626  0.4080432  1.9984477]\n",
      "Currrent Action : tensor([[1.2733]])\n",
      "Next State : [[-0.9129626]\n",
      " [ 0.4080432]\n",
      " [ 1.9984477]]\n",
      "Reward : [-7.07897229]\n",
      "--------------------------------------------------\n",
      "Step : 38\n",
      "Current State : [-0.9553226   0.29556498  2.4052567 ]\n",
      "Currrent Action : tensor([[0.6718]])\n",
      "Next State : [[-0.9553226 ]\n",
      " [ 0.29556498]\n",
      " [ 2.4052567 ]]\n",
      "Reward : [-7.80521159]\n",
      "--------------------------------------------------\n",
      "Step : 39\n",
      "Current State : [-0.9853371   0.17061915  2.5717776 ]\n",
      "Currrent Action : tensor([[-0.3677]])\n",
      "Next State : [[-0.9853371 ]\n",
      " [ 0.17061915]\n",
      " [ 2.5717776 ]]\n",
      "Reward : [-8.65304362]\n",
      "--------------------------------------------------\n",
      "Step : 40\n",
      "Current State : [-0.99948657  0.03204045  2.788241  ]\n",
      "Currrent Action : tensor([[0.5900]])\n",
      "Next State : [[-0.99948657]\n",
      " [ 0.03204045]\n",
      " [ 2.788241  ]]\n",
      "Reward : [-9.48345198]\n",
      "--------------------------------------------------\n",
      "Step : 41\n",
      "Current State : [-0.9940348  -0.10906331  2.8265328 ]\n",
      "Currrent Action : tensor([[0.0951]])\n",
      "Next State : [[-0.9940348 ]\n",
      " [-0.10906331]\n",
      " [ 2.8265328 ]]\n",
      "Reward : [-10.44671859]\n",
      "--------------------------------------------------\n",
      "Step : 42\n",
      "Current State : [-0.9679047  -0.25131762  2.895213  ]\n",
      "Currrent Action : tensor([[1.0032]])\n",
      "Next State : [[-0.9679047 ]\n",
      " [-0.25131762]\n",
      " [ 2.895213  ]]\n",
      "Reward : [-9.99485091]\n",
      "--------------------------------------------------\n",
      "Step : 43\n",
      "Current State : [-0.9251694  -0.37955448  2.705467  ]\n",
      "Currrent Action : tensor([[-0.0084]])\n",
      "Next State : [[-0.9251694 ]\n",
      " [-0.37955448]\n",
      " [ 2.705467  ]]\n",
      "Reward : [-9.17617842]\n",
      "--------------------------------------------------\n",
      "Step : 44\n",
      "Current State : [-0.87510455 -0.4839339   2.316596  ]\n",
      "Currrent Action : tensor([[-0.6947]])\n",
      "Next State : [[-0.87510455]\n",
      " [-0.4839339 ]\n",
      " [ 2.316596  ]]\n",
      "Reward : [-8.30747176]\n",
      "--------------------------------------------------\n",
      "Step : 45\n",
      "Current State : [-0.82385874 -0.56679523  1.9493233 ]\n",
      "Currrent Action : tensor([[-0.0288]])\n",
      "Next State : [[-0.82385874]\n",
      " [-0.56679523]\n",
      " [ 1.9493233 ]]\n",
      "Reward : [-7.48752144]\n",
      "--------------------------------------------------\n",
      "Step : 46\n",
      "Current State : [-0.7789929 -0.6270327  1.5025516]\n",
      "Currrent Action : tensor([[-0.1445]])\n",
      "Next State : [[-0.7789929]\n",
      " [-0.6270327]\n",
      " [ 1.5025516]]\n",
      "Reward : [-6.82643663]\n",
      "--------------------------------------------------\n",
      "Step : 47\n",
      "Current State : [-0.7489385 -0.6626395  0.9319881]\n",
      "Currrent Action : tensor([[-0.6686]])\n",
      "Next State : [[-0.7489385]\n",
      " [-0.6626395]\n",
      " [ 0.9319881]]\n",
      "Reward : [-6.29679177]\n",
      "--------------------------------------------------\n",
      "Step : 48\n",
      "Current State : [-0.7400672  -0.672533    0.26576945]\n",
      "Currrent Action : tensor([[-1.1283]])\n",
      "Next State : [[-0.7400672 ]\n",
      " [-0.672533  ]\n",
      " [ 0.26576945]]\n",
      "Reward : [-5.93125499]\n",
      "--------------------------------------------------\n",
      "Step : 49\n",
      "Current State : [-0.74150723 -0.6709449  -0.04287509]\n",
      "Currrent Action : tensor([[1.3050]])\n",
      "Next State : [[-0.74150723]\n",
      " [-0.6709449 ]\n",
      " [-0.04287509]]\n",
      "Reward : [-5.78782162]\n",
      "--------------------------------------------------\n",
      "Step : 50\n",
      "Current State : [-0.7692191  -0.6389851  -0.84608376]\n",
      "Currrent Action : tensor([[-2.6291]])\n",
      "Next State : [[-0.7692191 ]\n",
      " [-0.6389851 ]\n",
      " [-0.84608376]]\n",
      "Reward : [-5.79355062]\n",
      "--------------------------------------------------\n",
      "Step : 51\n",
      "Current State : [-0.8051931 -0.5930127 -1.1676583]\n",
      "Currrent Action : tensor([[1.0511]])\n",
      "Next State : [[-0.8051931]\n",
      " [-0.5930127]\n",
      " [-1.1676583]]\n",
      "Reward : [-6.0674241]\n",
      "--------------------------------------------------\n",
      "Step : 52\n",
      "Current State : [-0.8540432  -0.52020216 -1.7541523 ]\n",
      "Currrent Action : tensor([[-0.9449]])\n",
      "Next State : [[-0.8540432 ]\n",
      " [-0.52020216]\n",
      " [-1.7541523 ]]\n",
      "Reward : [-6.42126865]\n",
      "--------------------------------------------------\n",
      "Step : 53\n",
      "Current State : [-0.90078825 -0.4342586  -1.9574515 ]\n",
      "Currrent Action : tensor([[1.2457]])\n",
      "Next State : [[-0.90078825]\n",
      " [-0.4342586 ]\n",
      " [-1.9574515 ]]\n",
      "Reward : [-7.04071307]\n",
      "--------------------------------------------------\n",
      "Step : 54\n",
      "Current State : [-0.94525504 -0.32633248 -2.3358803 ]\n",
      "Currrent Action : tensor([[-0.3516]])\n",
      "Next State : [[-0.94525504]\n",
      " [-0.32633248]\n",
      " [-2.3358803 ]]\n",
      "Reward : [-7.63218235]\n",
      "--------------------------------------------------\n",
      "Step : 55\n",
      "Current State : [-0.9788104  -0.20476861 -2.5238748 ]\n",
      "Currrent Action : tensor([[0.3784]])\n",
      "Next State : [[-0.9788104 ]\n",
      " [-0.20476861]\n",
      " [-2.5238748 ]]\n",
      "Reward : [-8.43722198]\n",
      "--------------------------------------------------\n",
      "Step : 56\n",
      "Current State : [-0.9968388  -0.07945056 -2.5338583 ]\n",
      "Currrent Action : tensor([[0.9573]])\n",
      "Next State : [[-0.9968388 ]\n",
      " [-0.07945056]\n",
      " [-2.5338583 ]]\n",
      "Reward : [-9.25428056]\n",
      "--------------------------------------------------\n",
      "Step : 57\n",
      "Current State : [-0.9986964   0.05104477 -2.612027  ]\n",
      "Currrent Action : tensor([[-0.1239]])\n",
      "Next State : [[-0.9986964 ]\n",
      " [ 0.05104477]\n",
      " [-2.612027  ]]\n",
      "Reward : [-10.01825988]\n",
      "--------------------------------------------------\n",
      "Step : 58\n",
      "Current State : [-0.98342     0.18134248 -2.6256888 ]\n",
      "Currrent Action : tensor([[-0.3463]])\n",
      "Next State : [[-0.98342   ]\n",
      " [ 0.18134248]\n",
      " [-2.6256888 ]]\n",
      "Reward : [-10.2337375]\n",
      "--------------------------------------------------\n",
      "Step : 59\n",
      "Current State : [-0.95534754  0.29548448 -2.3522243 ]\n",
      "Currrent Action : tensor([[0.9164]])\n",
      "Next State : [[-0.95534754]\n",
      " [ 0.29548448]\n",
      " [-2.3522243 ]]\n",
      "Reward : [-9.4473727]\n",
      "--------------------------------------------------\n",
      "Step : 60\n",
      "Current State : [-0.921475    0.38843775 -1.97946   ]\n",
      "Currrent Action : tensor([[1.0077]])\n",
      "Next State : [[-0.921475  ]\n",
      " [ 0.38843775]\n",
      " [-1.97946   ]]\n",
      "Reward : [-8.62917264]\n",
      "--------------------------------------------------\n",
      "Step : 61\n",
      "Current State : [-0.88521135  0.4651891  -1.6982523 ]\n",
      "Currrent Action : tensor([[-0.0675]])\n",
      "Next State : [[-0.88521135]\n",
      " [ 0.4651891 ]\n",
      " [-1.6982523 ]]\n",
      "Reward : [-7.91399841]\n",
      "--------------------------------------------------\n",
      "Step : 62\n",
      "Current State : [-0.84879595  0.5287206  -1.4648864 ]\n",
      "Currrent Action : tensor([[-0.7702]])\n",
      "Next State : [[-0.84879595]\n",
      " [ 0.5287206 ]\n",
      " [-1.4648864 ]]\n",
      "Reward : [-7.3526047]\n",
      "--------------------------------------------------\n",
      "Step : 63\n",
      "Current State : [-0.8199435   0.57244444 -1.0478295 ]\n",
      "Currrent Action : tensor([[0.1368]])\n",
      "Next State : [[-0.8199435 ]\n",
      " [ 0.57244444]\n",
      " [-1.0478295 ]]\n",
      "Reward : [-6.89424873]\n",
      "--------------------------------------------------\n",
      "Step : 64\n",
      "Current State : [-0.7983135  0.6022421 -0.7364534]\n",
      "Currrent Action : tensor([[-0.7864]])\n",
      "Next State : [[-0.7983135]\n",
      " [ 0.6022421]\n",
      " [-0.7364534]]\n",
      "Reward : [-6.5219872]\n",
      "--------------------------------------------------\n",
      "Step : 65\n",
      "Current State : [-0.7815686   0.6238193  -0.54626465]\n",
      "Currrent Action : tensor([[-1.7433]])\n",
      "Next State : [[-0.7815686 ]\n",
      " [ 0.6238193 ]\n",
      " [-0.54626465]]\n",
      "Reward : [-6.28372745]\n",
      "--------------------------------------------------\n",
      "Step : 66\n",
      "Current State : [-0.7826978   0.62240195  0.036243  ]\n",
      "Currrent Action : tensor([[0.7643]])\n",
      "Next State : [[-0.7826978 ]\n",
      " [ 0.62240195]\n",
      " [ 0.036243  ]]\n",
      "Reward : [-6.12131405]\n",
      "--------------------------------------------------\n",
      "Step : 67\n",
      "Current State : [-0.801287    0.59828013  0.60909545]\n",
      "Currrent Action : tensor([[0.7070]])\n",
      "Next State : [[-0.801287  ]\n",
      " [ 0.59828013]\n",
      " [ 0.60909545]]\n",
      "Reward : [-6.10046858]\n",
      "--------------------------------------------------\n",
      "Step : 68\n",
      "Current State : [-0.836371    0.54816383  1.2237151 ]\n",
      "Currrent Action : tensor([[1.1061]])\n",
      "Next State : [[-0.836371  ]\n",
      " [ 0.54816383]\n",
      " [ 1.2237151 ]]\n",
      "Reward : [-6.28952143]\n",
      "--------------------------------------------------\n",
      "Step : 69\n",
      "Current State : [-0.87736136  0.4798302   1.5941211 ]\n",
      "Currrent Action : tensor([[-0.2714]])\n",
      "Next State : [[-0.87736136]\n",
      " [ 0.4798302 ]\n",
      " [ 1.5941211 ]]\n",
      "Reward : [-6.71072168]\n",
      "--------------------------------------------------\n",
      "Step : 70\n",
      "Current State : [-0.92575777  0.37811705  2.2539937 ]\n",
      "Currrent Action : tensor([[2.0579]])\n",
      "Next State : [[-0.92575777]\n",
      " [ 0.37811705]\n",
      " [ 2.2539937 ]]\n",
      "Reward : [-7.23369763]\n",
      "--------------------------------------------------\n",
      "Step : 71\n",
      "Current State : [-0.96420914  0.2651429   2.388188  ]\n",
      "Currrent Action : tensor([[-0.9960]])\n",
      "Next State : [[-0.96420914]\n",
      " [ 0.2651429 ]\n",
      " [ 2.388188  ]]\n",
      "Reward : [-8.09262675]\n",
      "--------------------------------------------------\n",
      "Step : 72\n",
      "Current State : [-0.99096316  0.13413441  2.6762435 ]\n",
      "Currrent Action : tensor([[0.5947]])\n",
      "Next State : [[-0.99096316]\n",
      " [ 0.13413441]\n",
      " [ 2.6762435 ]]\n",
      "Reward : [-8.82620903]\n",
      "--------------------------------------------------\n",
      "Step : 73\n",
      "Current State : [-0.9999489  -0.01010454  2.8928928 ]\n",
      "Currrent Action : tensor([[0.7737]])\n",
      "Next State : [[-0.9999489 ]\n",
      " [-0.01010454]\n",
      " [ 2.8928928 ]]\n",
      "Reward : [-9.75919264]\n",
      "--------------------------------------------------\n",
      "Step : 74\n",
      "Current State : [-0.98772126 -0.15622652  2.9352875 ]\n",
      "Currrent Action : tensor([[0.3332]])\n",
      "Next State : [[-0.98772126]\n",
      " [-0.15622652]\n",
      " [ 2.9352875 ]]\n",
      "Reward : [-10.64321057]\n",
      "--------------------------------------------------\n",
      "Step : 75\n",
      "Current State : [-0.9564861  -0.29177788  2.7843194 ]\n",
      "Currrent Action : tensor([[-0.2253]])\n",
      "Next State : [[-0.9564861 ]\n",
      " [-0.29177788]\n",
      " [ 2.7843194 ]]\n",
      "Reward : [-9.77021678]\n",
      "--------------------------------------------------\n",
      "Step : 76\n",
      "Current State : [-0.90823174 -0.41846758  2.7134466 ]\n",
      "Currrent Action : tensor([[0.9864]])\n",
      "Next State : [[-0.90823174]\n",
      " [-0.41846758]\n",
      " [ 2.7134466 ]]\n",
      "Reward : [-8.87312992]\n",
      "--------------------------------------------------\n",
      "Step : 77\n",
      "Current State : [-0.8496847  -0.52729106  2.473037  ]\n",
      "Currrent Action : tensor([[0.4896]])\n",
      "Next State : [[-0.8496847 ]\n",
      " [-0.52729106]\n",
      " [ 2.473037  ]]\n",
      "Reward : [-8.07972612]\n",
      "--------------------------------------------------\n",
      "Step : 78\n",
      "Current State : [-0.7995278 -0.6006291  1.7775687]\n",
      "Currrent Action : tensor([[-2.3786]])\n",
      "Next State : [[-0.7995278]\n",
      " [-0.6006291]\n",
      " [ 1.7775687]]\n",
      "Reward : [-7.3039358]\n",
      "--------------------------------------------------\n",
      "Step : 79\n",
      "Current State : [-0.7565761  -0.65390563  1.3689514 ]\n",
      "Currrent Action : tensor([[0.2790]])\n",
      "Next State : [[-0.7565761 ]\n",
      " [-0.65390563]\n",
      " [ 1.3689514 ]]\n",
      "Reward : [-6.552585]\n",
      "--------------------------------------------------\n",
      "Step : 80\n",
      "Current State : [-0.730038   -0.68340653  0.7936702 ]\n",
      "Currrent Action : tensor([[-0.5657]])\n",
      "Next State : [[-0.730038  ]\n",
      " [-0.68340653]\n",
      " [ 0.7936702 ]]\n",
      "Reward : [-6.08707103]\n",
      "--------------------------------------------------\n",
      "Step : 81\n",
      "Current State : [-0.7296602  -0.6838099   0.01105251]\n",
      "Currrent Action : tensor([[-1.8004]])\n",
      "Next State : [[-0.7296602 ]\n",
      " [-0.6838099 ]\n",
      " [ 0.01105251]]\n",
      "Reward : [-5.77438458]\n",
      "--------------------------------------------------\n",
      "Step : 82\n",
      "Current State : [-0.7564807 -0.654016  -0.8018049]\n",
      "Currrent Action : tensor([[-2.1784]])\n",
      "Next State : [[-0.7564807]\n",
      " [-0.654016 ]\n",
      " [-0.8018049]]\n",
      "Reward : [-5.70952373]\n",
      "--------------------------------------------------\n",
      "Step : 83\n",
      "Current State : [-0.7935202 -0.6085439 -1.1731375]\n",
      "Currrent Action : tensor([[0.7945]])\n",
      "Next State : [[-0.7935202]\n",
      " [-0.6085439]\n",
      " [-1.1731375]]\n",
      "Reward : [-5.96355995]\n",
      "--------------------------------------------------\n",
      "Step : 84\n",
      "Current State : [-0.8409707  -0.54108065 -1.6500545 ]\n",
      "Currrent Action : tensor([[-0.1367]])\n",
      "Next State : [[-0.8409707 ]\n",
      " [-0.54108065]\n",
      " [-1.6500545 ]]\n",
      "Reward : [-6.3246453]\n",
      "--------------------------------------------------\n",
      "Step : 85\n",
      "Current State : [-0.89588386 -0.44428837 -2.2268355 ]\n",
      "Currrent Action : tensor([[-1.1398]])\n",
      "Next State : [[-0.89588386]\n",
      " [-0.44428837]\n",
      " [-2.2268355 ]]\n",
      "Reward : [-6.87780461]\n",
      "--------------------------------------------------\n",
      "Step : 86\n",
      "Current State : [-0.94478315 -0.32769617 -2.5303144 ]\n",
      "Currrent Action : tensor([[0.1982]])\n",
      "Next State : [[-0.94478315]\n",
      " [-0.32769617]\n",
      " [-2.5303144 ]]\n",
      "Reward : [-7.68482138]\n",
      "--------------------------------------------------\n",
      "Step : 87\n",
      "Current State : [-0.9805629  -0.19620489 -2.7275598 ]\n",
      "Currrent Action : tensor([[0.3235]])\n",
      "Next State : [[-0.9805629 ]\n",
      " [-0.19620489]\n",
      " [-2.7275598 ]]\n",
      "Reward : [-8.52369369]\n",
      "--------------------------------------------------\n",
      "Step : 88\n",
      "Current State : [-0.99800444 -0.06314357 -2.6860094 ]\n",
      "Currrent Action : tensor([[1.2580]])\n",
      "Next State : [[-0.99800444]\n",
      " [-0.06314357]\n",
      " [-2.6860094 ]]\n",
      "Reward : [-9.41330442]\n",
      "--------------------------------------------------\n",
      "Step : 89\n",
      "Current State : [-0.99636227  0.08521856 -2.9701529 ]\n",
      "Currrent Action : tensor([[-1.5786]])\n",
      "Next State : [[-0.99636227]\n",
      " [ 0.08521856]\n",
      " [-2.9701529 ]]\n",
      "Reward : [-10.20054654]\n",
      "--------------------------------------------------\n",
      "Step : 90\n",
      "Current State : [-0.97479063  0.22312152 -2.7938702 ]\n",
      "Currrent Action : tensor([[0.7491]])\n",
      "Next State : [[-0.97479063]\n",
      " [ 0.22312152]\n",
      " [-2.7938702 ]]\n",
      "Reward : [-10.22353204]\n",
      "--------------------------------------------------\n",
      "Step : 91\n",
      "Current State : [-0.93864584  0.34488255 -2.541962  ]\n",
      "Currrent Action : tensor([[0.5638]])\n",
      "Next State : [[-0.93864584]\n",
      " [ 0.34488255]\n",
      " [-2.541962  ]]\n",
      "Reward : [-9.28731087]\n",
      "--------------------------------------------------\n",
      "Step : 92\n",
      "Current State : [-0.8930203   0.45001644 -2.2934017 ]\n",
      "Currrent Action : tensor([[-0.0673]])\n",
      "Next State : [[-0.8930203 ]\n",
      " [ 0.45001644]\n",
      " [-2.2934017 ]]\n",
      "Reward : [-8.4273547]\n",
      "--------------------------------------------------\n",
      "Step : 93\n",
      "Current State : [-0.8517761   0.52390593 -1.6929278 ]\n",
      "Currrent Action : tensor([[1.7531]])\n",
      "Next State : [[-0.8517761 ]\n",
      " [ 0.52390593]\n",
      " [-1.6929278 ]]\n",
      "Reward : [-7.68364512]\n",
      "--------------------------------------------------\n",
      "Step : 94\n",
      "Current State : [-0.8137057   0.58127713 -1.3773441 ]\n",
      "Currrent Action : tensor([[-0.5156]])\n",
      "Next State : [[-0.8137057 ]\n",
      " [ 0.58127713]\n",
      " [-1.3773441 ]]\n",
      "Reward : [-6.99580824]\n",
      "--------------------------------------------------\n",
      "Step : 95\n",
      "Current State : [-0.7938042   0.60817343 -0.66920304]\n",
      "Currrent Action : tensor([[1.8146]])\n",
      "Next State : [[-0.7938042 ]\n",
      " [ 0.60817343]\n",
      " [-0.66920304]]\n",
      "Reward : [-6.54993038]\n",
      "--------------------------------------------------\n",
      "Step : 96\n",
      "Current State : [-0.7884774   0.6150637  -0.17418572]\n",
      "Currrent Action : tensor([[0.2592]])\n",
      "Next State : [[-0.7884774 ]\n",
      " [ 0.6150637 ]\n",
      " [-0.17418572]]\n",
      "Reward : [-6.23417429]\n",
      "--------------------------------------------------\n",
      "Step : 97\n",
      "Current State : [-0.79893225  0.60142106  0.34376314]\n",
      "Currrent Action : tensor([[0.3777]])\n",
      "Next State : [[-0.79893225]\n",
      " [ 0.60142106]\n",
      " [ 0.34376314]]\n",
      "Reward : [-6.14924183]\n",
      "--------------------------------------------------\n",
      "Step : 98\n",
      "Current State : [-0.81431353  0.5804253   0.520555  ]\n",
      "Currrent Action : tensor([[-1.8285]])\n",
      "Next State : [[-0.81431353]\n",
      " [ 0.5804253 ]\n",
      " [ 0.520555  ]]\n",
      "Reward : [-6.24674446]\n",
      "--------------------------------------------------\n",
      "Step : 99\n",
      "Current State : [-0.8432762  0.5374805  1.0360875]\n",
      "Currrent Action : tensor([[0.5348]])\n",
      "Next State : [[-0.8432762]\n",
      " [ 0.5374805]\n",
      " [ 1.0360875]]\n",
      "Reward : [-6.38959181]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (25, 1)) of distribution Normal(loc: torch.Size([25, 1]), scale: torch.Size([25, 1])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan]], grad_fn=<AddmmBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 70\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m episodic_rewards, episodic_losses\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(check_model_stats(model\u001b[38;5;241m=\u001b[39mmodel, iteration\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m---> 70\u001b[0m \u001b[43mtrain_the_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_learning_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_iterations\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m#check_model_stats(model=model, iteration='inf')\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[73], line 49\u001b[0m, in \u001b[0;36mtrain_the_model\u001b[0;34m(env, model, num_learning_iterations)\u001b[0m\n\u001b[1;32m     46\u001b[0m     actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(actions)\n\u001b[1;32m     47\u001b[0m     advantage \u001b[38;5;241m=\u001b[39m returns \u001b[38;5;241m-\u001b[39m values\n\u001b[0;32m---> 49\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mppo_trainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mppo_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mppo_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmini_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmini_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43madvantages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madvantage\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     episodic_losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m episodic_rewards, episodic_losses\n",
      "Cell \u001b[0;32mIn[68], line 32\u001b[0m, in \u001b[0;36mppo_trainer\u001b[0;34m(model, optimizer, ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ppo_epochs):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m state, action, old_log_probs, return_vals, advantage \u001b[38;5;129;01min\u001b[39;00m ppo_randomizer(mini_batch_size, states, actions, log_probs, returns, advantages):\n\u001b[0;32m---> 32\u001b[0m         dist, value \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Action space and value chosen by the new policy\u001b[39;00m\n\u001b[1;32m     33\u001b[0m         entropy \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mentropy()\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;66;03m#what is the entropy of my new policy distribution\u001b[39;00m\n\u001b[1;32m     34\u001b[0m         new_log_probs \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mlog_prob(action) \u001b[38;5;66;03m#what is the likelihood that my new policy is to take the old action?\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[6], line 41\u001b[0m, in \u001b[0;36mActorCritic.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m mu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(x)\n\u001b[1;32m     40\u001b[0m std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_std\u001b[38;5;241m.\u001b[39mexp()\u001b[38;5;241m.\u001b[39mexpand_as(mu)\n\u001b[0;32m---> 41\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[43mNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dist, value\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/distributions/normal.py:59\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/distributions/distribution.py:71\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     69\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m---> 71\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     72\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m             )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (25, 1)) of distribution Normal(loc: torch.Size([25, 1]), scale: torch.Size([25, 1])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan]], grad_fn=<AddmmBackward0>)"
     ]
    }
   ],
   "source": [
    "num_inputs  = env.observation_space.shape[0]\n",
    "num_outputs = env.action_space.shape[0]\n",
    "\n",
    "#Hyper params:\n",
    "hidden_size      = 256\n",
    "lr               = 3e-4\n",
    "num_steps        = 100\n",
    "mini_batch_size  = 25\n",
    "ppo_epochs       = 5\n",
    "threshold_reward = -200\n",
    "learning_iterations = 500\n",
    "\n",
    "model = ActorCritic(num_inputs, num_outputs, hidden_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def train_the_model(\n",
    "    env,\n",
    "    model,\n",
    "    num_learning_iterations\n",
    "):\n",
    "\n",
    "    episodic_rewards = []\n",
    "    episodic_losses = []\n",
    "\n",
    "    for it in range(num_learning_iterations):\n",
    "        print(f\"learning iteration : {it}\")\n",
    "\n",
    "        frames, rewards, actions, values, masks, states, log_probs, entropy, next_state = collect_experience(\n",
    "            env=env,\n",
    "            model=model,\n",
    "            num_steps=num_steps\n",
    "        )\n",
    "\n",
    "        episodic_rewards.append(sum(rewards))\n",
    "        \n",
    "        next_state = np.array(next_state).flatten()\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "        _, next_value = model(next_state)\n",
    "        returns = compute_gae(next_value, rewards, masks, values)\n",
    "\n",
    "        returns = torch.cat(returns).detach()\n",
    "        log_probs = torch.cat(log_probs).detach()\n",
    "        values = torch.cat(values).detach()\n",
    "        states = torch.cat(states)\n",
    "        actions = torch.cat(actions)\n",
    "        advantage = returns - values\n",
    "\n",
    "        loss = ppo_trainer(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            ppo_epochs=ppo_epochs,\n",
    "            mini_batch_size=mini_batch_size,\n",
    "            states=states,\n",
    "            actions=actions,\n",
    "            log_probs=log_probs,\n",
    "            returns=returns,\n",
    "            advantages=advantage\n",
    "        )\n",
    "\n",
    "        episodic_losses.append(loss)\n",
    "\n",
    "    \n",
    "    return episodic_rewards, episodic_losses\n",
    "\n",
    "\n",
    "\n",
    "print(check_model_stats(model=model, iteration=0))\n",
    "\n",
    "train_the_model(\n",
    "    env=env,\n",
    "    model=model,\n",
    "    num_learning_iterations=learning_iterations\n",
    ")\n",
    "#check_model_stats(model=model, iteration='inf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f39479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9b8269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
