{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10ff97e1",
   "metadata": {},
   "source": [
    "## Import everything that we need here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b3d2a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e2d2117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7aadf0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Using cuda\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "print(use_cuda)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75989e7",
   "metadata": {},
   "source": [
    "## Create Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "70d4b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.multiprocessing_env import SubprocVecEnv\n",
    "\n",
    "num_envs = 16\n",
    "env_name = \"Pendulum-v1\"\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name, render_mode=\"human\")\n",
    "        return env \n",
    "    return _thunk()\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "#envs = SubprocVecEnv(envs)\n",
    "\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6205948b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PassiveEnvChecker<PendulumEnv<Pendulum-v1>>>>>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86916544",
   "metadata": {},
   "source": [
    "## Defining the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "85073036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    print(f\"This is m : {m}\")\n",
    "    print(f\"This is the type of m : {type(m)}\")\n",
    "\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_inputs,\n",
    "            num_outputs,\n",
    "            hidden_size,\n",
    "            std=0.0\n",
    "    ):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_outputs),\n",
    "        )\n",
    "\n",
    "        self.log_std = nn.Parameter(torch.ones(1, num_outputs) * std)\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x\n",
    "    ):\n",
    "        value = self.critic(x)\n",
    "        mu = self.actor(x)\n",
    "        std = self.log_std.exp().expand_as(mu)\n",
    "        dist = Normal(mu, std)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e4d1ddfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1\n",
      "This is m : Linear(in_features=3, out_features=256, bias=True)\n",
      "This is the type of m : <class 'torch.nn.modules.linear.Linear'>\n",
      "This is m : ReLU()\n",
      "This is the type of m : <class 'torch.nn.modules.activation.ReLU'>\n",
      "This is m : Linear(in_features=256, out_features=1, bias=True)\n",
      "This is the type of m : <class 'torch.nn.modules.linear.Linear'>\n",
      "This is m : Sequential(\n",
      "  (0): Linear(in_features=3, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "This is the type of m : <class 'torch.nn.modules.container.Sequential'>\n",
      "This is m : Linear(in_features=3, out_features=256, bias=True)\n",
      "This is the type of m : <class 'torch.nn.modules.linear.Linear'>\n",
      "This is m : ReLU()\n",
      "This is the type of m : <class 'torch.nn.modules.activation.ReLU'>\n",
      "This is m : Linear(in_features=256, out_features=1, bias=True)\n",
      "This is the type of m : <class 'torch.nn.modules.linear.Linear'>\n",
      "This is m : Sequential(\n",
      "  (0): Linear(in_features=3, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "This is the type of m : <class 'torch.nn.modules.container.Sequential'>\n",
      "This is m : ActorCritic(\n",
      "  (critic): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (actor): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "This is the type of m : <class '__main__.ActorCritic'>\n",
      "log_std: torch.Size([1, 1])\n",
      "critic.0.weight: torch.Size([256, 3])\n",
      "critic.0.bias: torch.Size([256])\n",
      "critic.2.weight: torch.Size([1, 256])\n",
      "critic.2.bias: torch.Size([1])\n",
      "actor.0.weight: torch.Size([256, 3])\n",
      "actor.0.bias: torch.Size([256])\n",
      "actor.2.weight: torch.Size([1, 256])\n",
      "actor.2.bias: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "num_input = env.observation_space.shape[0]\n",
    "num_output = env.action_space.shape[0]\n",
    "\n",
    "print(num_input, num_output)\n",
    "\n",
    "hidden_size      = 256\n",
    "lr               = 3e-4\n",
    "num_steps        = 20\n",
    "mini_batch_size  = 5\n",
    "ppo_epochs       = 4\n",
    "threshold_reward = -200\n",
    "\n",
    "model = ActorCritic(num_input, num_output, hidden_size).to(device)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dcc92a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Authorization required, but no authorization protocol specified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.8853, -0.4650,  0.9080]], device='cuda:0'), torch.Size([1, 3]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state, info = env.reset()\n",
    "env.render()\n",
    "state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "\n",
    "state, state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e6e2a926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "\n",
    "def test_env(\n",
    "    env,\n",
    "    model,\n",
    "    total_steps = 100,\n",
    "    vis=False,\n",
    "):\n",
    "    state, info = env.reset()\n",
    "    if vis: env.render()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "\n",
    "    while ((not done) and (step < total_steps)):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state)\n",
    "        \n",
    "        action = dist.sample().cpu().numpy()[0]\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        print('-' * 50)\n",
    "        print(f\"Step : {step}\")\n",
    "        print(f\"Current State : {state}\")\n",
    "        print(f\"Currrent Action : {action}\")\n",
    "        print(f\"Next State : {next_state}\")\n",
    "        print(f\"Reward : {reward}\")\n",
    "        # The interesting part is how the reward is calculated\n",
    "        # reward is -(angle_cost + velocity_cost + action_cost)\n",
    "\n",
    "        # angle_cost = angle**2                      Penalty for being away from upright (0Â°)\n",
    "        # velocity_cost = 0.1 * angular_velocity**2  Penalty for moving too fast\n",
    "        # action_cost = 0.001 * action**2            Penalty for using large torques\n",
    "\n",
    "        state = next_state\n",
    "        if vis: env.render()\n",
    "\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "        \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "707c53b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Step : 0\n",
      "Current State : tensor([[ 0.9338, -0.3579, -0.1309]], device='cuda:0')\n",
      "Currrent Action : [0.47046965]\n",
      "Next State : [ 0.9277451  -0.37321448 -0.32879534]\n",
      "Reward : -0.1359148422056937\n",
      "--------------------------------------------------\n",
      "Step : 1\n",
      "Current State : tensor([[ 0.9277, -0.3732, -0.3288]], device='cuda:0')\n",
      "Currrent Action : [0.684273]\n",
      "Next State : [ 0.9180056  -0.3965675  -0.50606525]\n",
      "Reward : -0.15756328950725693\n",
      "--------------------------------------------------\n",
      "Step : 2\n",
      "Current State : tensor([[ 0.9180, -0.3966, -0.5061]], device='cuda:0')\n",
      "Currrent Action : [0.68767244]\n",
      "Next State : [ 0.903559   -0.42846364 -0.70034003]\n",
      "Reward : -0.19236332914947593\n",
      "--------------------------------------------------\n",
      "Step : 3\n",
      "Current State : tensor([[ 0.9036, -0.4285, -0.7003]], device='cuda:0')\n",
      "Currrent Action : [-1.769825]\n",
      "Next State : [ 0.87413234 -0.48568776 -1.2871615 ]\n",
      "Reward : -0.2482444137093595\n",
      "--------------------------------------------------\n",
      "Step : 4\n",
      "Current State : tensor([[ 0.8741, -0.4857, -1.2872]], device='cuda:0')\n",
      "Currrent Action : [-0.2882422]\n",
      "Next State : [ 0.82989156 -0.5579247  -1.6946636 ]\n",
      "Reward : -0.42296248216258814\n",
      "--------------------------------------------------\n",
      "Step : 5\n",
      "Current State : tensor([[ 0.8299, -0.5579, -1.6947]], device='cuda:0')\n",
      "Currrent Action : [0.9469776]\n",
      "Next State : [ 0.7709684 -0.6368734 -1.9710605]\n",
      "Reward : -0.6384107319617093\n",
      "--------------------------------------------------\n",
      "Step : 6\n",
      "Current State : tensor([[ 0.7710, -0.6369, -1.9711]], device='cuda:0')\n",
      "Currrent Action : [-0.7092798]\n",
      "Next State : [ 0.68354243 -0.7299108  -2.5551076 ]\n",
      "Reward : -0.86571293309847\n",
      "--------------------------------------------------\n",
      "Step : 7\n",
      "Current State : tensor([[ 0.6835, -0.7299, -2.5551]], device='cuda:0')\n",
      "Currrent Action : [2.2873626]\n",
      "Next State : [ 0.5748967 -0.818226  -2.8025405]\n",
      "Reward : -1.3262946150150197\n",
      "--------------------------------------------------\n",
      "Step : 8\n",
      "Current State : tensor([[ 0.5749, -0.8182, -2.8025]], device='cuda:0')\n",
      "Currrent Action : [0.09709199]\n",
      "Next State : [ 0.42810565 -0.90372866 -3.4016464 ]\n",
      "Reward : -1.7038070238430776\n",
      "--------------------------------------------------\n",
      "Step : 9\n",
      "Current State : tensor([[ 0.4281, -0.9037, -3.4016]], device='cuda:0')\n",
      "Currrent Action : [-1.7316159]\n",
      "Next State : [ 0.22353166 -0.9746967  -4.339185  ]\n",
      "Reward : -2.433406493214337\n",
      "--------------------------------------------------\n",
      "Step : 10\n",
      "Current State : tensor([[ 0.2235, -0.9747, -4.3392]], device='cuda:0')\n",
      "Currrent Action : [1.109159]\n",
      "Next State : [-0.01975408 -0.99980485 -4.903834  ]\n",
      "Reward : -3.6940766095837962\n",
      "--------------------------------------------------\n",
      "Step : 11\n",
      "Current State : tensor([[-0.0198, -0.9998, -4.9038]], device='cuda:0')\n",
      "Currrent Action : [0.47973108]\n",
      "Next State : [-0.2944135 -0.9556781 -5.581728 ]\n",
      "Reward : -4.93484350710915\n",
      "--------------------------------------------------\n",
      "Step : 12\n",
      "Current State : tensor([[-0.2944, -0.9557, -5.5817]], device='cuda:0')\n",
      "Currrent Action : [1.1028003]\n",
      "Next State : [-0.5691692 -0.8222204 -6.1330667]\n",
      "Reward : -6.612331348757727\n",
      "--------------------------------------------------\n",
      "Step : 13\n",
      "Current State : tensor([[-0.5692, -0.8222, -6.1331]], device='cuda:0')\n",
      "Currrent Action : [0.4201991]\n",
      "Next State : [-0.8074579  -0.58992517 -6.686702  ]\n",
      "Reward : -8.497871248986925\n",
      "--------------------------------------------------\n",
      "Step : 14\n",
      "Current State : tensor([[-0.8075, -0.5899, -6.6867]], device='cuda:0')\n",
      "Currrent Action : [-0.9908096]\n",
      "Next State : [-0.9645462  -0.26391396 -7.277767  ]\n",
      "Reward : -10.775425337199762\n",
      "--------------------------------------------------\n",
      "Step : 15\n",
      "Current State : tensor([[-0.9645, -0.2639, -7.2778]], device='cuda:0')\n",
      "Currrent Action : [0.36089483]\n",
      "Next State : [-0.9945968   0.10381325 -7.4215684 ]\n",
      "Reward : -13.559555460167521\n",
      "--------------------------------------------------\n",
      "Step : 16\n",
      "Current State : tensor([[-0.9946,  0.1038, -7.4216]], device='cuda:0')\n",
      "Currrent Action : [-2.7968469]\n",
      "Next State : [-0.88412136  0.4672573  -7.6437087 ]\n",
      "Reward : -14.738933093896321\n",
      "--------------------------------------------------\n",
      "Step : 17\n",
      "Current State : tensor([[-0.8841,  0.4673, -7.6437]], device='cuda:0')\n",
      "Currrent Action : [-0.9009384]\n",
      "Next State : [-0.6542496  0.7562787 -7.4284062]\n",
      "Reward : -12.89462384276304\n",
      "--------------------------------------------------\n",
      "Step : 18\n",
      "Current State : tensor([[-0.6542,  0.7563, -7.4284]], device='cuda:0')\n",
      "Currrent Action : [-1.0076531]\n",
      "Next State : [-0.35468084  0.9349874  -7.0123453 ]\n",
      "Reward : -10.735730687366619\n",
      "--------------------------------------------------\n",
      "Step : 19\n",
      "Current State : tensor([[-0.3547,  0.9350, -7.0123]], device='cuda:0')\n",
      "Currrent Action : [-0.08824532]\n",
      "Next State : [-0.04633902  0.99892575 -6.3243413 ]\n",
      "Reward : -8.655222111691181\n",
      "--------------------------------------------------\n",
      "Step : 20\n",
      "Current State : tensor([[-0.0463,  0.9989, -6.3243]], device='cuda:0')\n",
      "Currrent Action : [0.25822166]\n",
      "Next State : [ 0.22843032  0.9735603  -5.5364137 ]\n",
      "Reward : -6.614976577229364\n",
      "--------------------------------------------------\n",
      "Step : 21\n",
      "Current State : tensor([[ 0.2284,  0.9736, -5.5364]], device='cuda:0')\n",
      "Currrent Action : [1.198027]\n",
      "Next State : [ 0.44555315  0.8952555  -4.6265397 ]\n",
      "Reward : -4.863110997600504\n",
      "--------------------------------------------------\n",
      "Step : 22\n",
      "Current State : tensor([[ 0.4456,  0.8953, -4.6265]], device='cuda:0')\n",
      "Currrent Action : [-2.296713]\n",
      "Next State : [ 0.62454355  0.78099    -4.255098  ]\n",
      "Reward : -3.3743773669297066\n",
      "--------------------------------------------------\n",
      "Step : 23\n",
      "Current State : tensor([[ 0.6245,  0.7810, -4.2551]], device='cuda:0')\n",
      "Currrent Action : [-1.4570365]\n",
      "Next State : [ 0.7636466  0.6456345 -3.887911 ]\n",
      "Reward : -2.615971802092797\n",
      "--------------------------------------------------\n",
      "Step : 24\n",
      "Current State : tensor([[ 0.7636,  0.6456, -3.8879]], device='cuda:0')\n",
      "Currrent Action : [-0.21978314]\n",
      "Next State : [ 0.86279637  0.50555164 -3.4366527 ]\n",
      "Reward : -2.004232286670647\n",
      "--------------------------------------------------\n",
      "Step : 25\n",
      "Current State : tensor([[ 0.8628,  0.5056, -3.4367]], device='cuda:0')\n",
      "Currrent Action : [-1.2270036]\n",
      "Next State : [ 0.93306875  0.35969803 -3.2415395 ]\n",
      "Reward : -1.4634861245655952\n",
      "--------------------------------------------------\n",
      "Step : 26\n",
      "Current State : tensor([[ 0.9331,  0.3597, -3.2415]], device='cuda:0')\n",
      "Currrent Action : [0.46564263]\n",
      "Next State : [ 0.97527194  0.22100833 -2.9019196 ]\n",
      "Reward : -1.1863575603574705\n",
      "--------------------------------------------------\n",
      "Step : 27\n",
      "Current State : tensor([[ 0.9753,  0.2210, -2.9019]], device='cuda:0')\n",
      "Currrent Action : [0.5406592]\n",
      "Next State : [ 0.9959442  0.0899732 -2.6550643]\n",
      "Reward : -0.8920673434342964\n",
      "--------------------------------------------------\n",
      "Step : 28\n",
      "Current State : tensor([[ 0.9959,  0.0900, -2.6551]], device='cuda:0')\n",
      "Currrent Action : [0.11929847]\n",
      "Next State : [ 0.9992632  -0.03838003 -2.5696898 ]\n",
      "Reward : -0.7130680366310301\n",
      "--------------------------------------------------\n",
      "Step : 29\n",
      "Current State : tensor([[ 0.9993, -0.0384, -2.5697]], device='cuda:0')\n",
      "Currrent Action : [-1.1810961]\n",
      "Next State : [ 0.98434615 -0.17624597 -2.775639  ]\n",
      "Reward : -0.663199263735904\n",
      "--------------------------------------------------\n",
      "Step : 30\n",
      "Current State : tensor([[ 0.9843, -0.1762, -2.7756]], device='cuda:0')\n",
      "Currrent Action : [-1.2344089]\n",
      "Next State : [ 0.9454508  -0.32576492 -3.092985  ]\n",
      "Reward : -0.8033307436420198\n",
      "--------------------------------------------------\n",
      "Step : 31\n",
      "Current State : tensor([[ 0.9455, -0.3258, -3.0930]], device='cuda:0')\n",
      "Currrent Action : [1.2179348]\n",
      "Next State : [ 0.8825438 -0.4702302 -3.1546185]\n",
      "Reward : -1.0682439102901264\n",
      "--------------------------------------------------\n",
      "Step : 32\n",
      "Current State : tensor([[ 0.8825, -0.4702, -3.1546]], device='cuda:0')\n",
      "Currrent Action : [-0.02813265]\n",
      "Next State : [ 0.78683835 -0.6171592  -3.5115108 ]\n",
      "Reward : -1.234823275137317\n",
      "--------------------------------------------------\n",
      "Step : 33\n",
      "Current State : tensor([[ 0.7868, -0.6172, -3.5115]], device='cuda:0')\n",
      "Currrent Action : [0.2394708]\n",
      "Next State : [ 0.6508824 -0.7591785 -3.9384596]\n",
      "Reward : -1.67552235868765\n",
      "--------------------------------------------------\n",
      "Step : 34\n",
      "Current State : tensor([[ 0.6509, -0.7592, -3.9385]], device='cuda:0')\n",
      "Currrent Action : [-0.34868047]\n",
      "Next State : [ 0.46243447 -0.8866535  -4.560146  ]\n",
      "Reward : -2.294398462367127\n",
      "--------------------------------------------------\n",
      "Step : 35\n",
      "Current State : tensor([[ 0.4624, -0.8867, -4.5601]], device='cuda:0')\n",
      "Currrent Action : [-0.4787072]\n",
      "Next State : [ 0.21421865 -0.9767857  -5.2969418 ]\n",
      "Reward : -3.2679471544137706\n",
      "--------------------------------------------------\n",
      "Step : 36\n",
      "Current State : tensor([[ 0.2142, -0.9768, -5.2969]], device='cuda:0')\n",
      "Currrent Action : [1.4445446]\n",
      "Next State : [-0.07468104 -0.99720746 -5.8128495 ]\n",
      "Reward : -4.643612188010639\n",
      "--------------------------------------------------\n",
      "Step : 37\n",
      "Current State : tensor([[-0.0747, -0.9972, -5.8128]], device='cuda:0')\n",
      "Currrent Action : [-1.144046]\n",
      "Next State : [-0.39986426 -0.91657436 -6.732362  ]\n",
      "Reward : -6.088055550779155\n",
      "--------------------------------------------------\n",
      "Step : 38\n",
      "Current State : tensor([[-0.3999, -0.9166, -6.7324]], device='cuda:0')\n",
      "Currrent Action : [0.6085129]\n",
      "Next State : [-0.7017098  -0.71246284 -7.328516  ]\n",
      "Reward : -8.461818390437243\n",
      "--------------------------------------------------\n",
      "Step : 39\n",
      "Current State : tensor([[-0.7017, -0.7125, -7.3285]], device='cuda:0')\n",
      "Currrent Action : [-0.12366422]\n",
      "Next State : [-0.92147666 -0.38843367 -7.8814125 ]\n",
      "Reward : -10.88660869679827\n",
      "--------------------------------------------------\n",
      "Step : 40\n",
      "Current State : tensor([[-0.9215, -0.3884, -7.8814]], device='cuda:0')\n",
      "Currrent Action : [-0.78776026]\n",
      "Next State : [-9.9999940e-01  1.0688294e-03 -8.0000000e+00]\n",
      "Reward : -13.734479041277329\n",
      "--------------------------------------------------\n",
      "Step : 41\n",
      "Current State : tensor([[-1.0000e+00,  1.0688e-03, -8.0000e+00]], device='cuda:0')\n",
      "Currrent Action : [-0.1026074]\n",
      "Next State : [-0.9206442  0.3904026 -8.       ]\n",
      "Reward : -16.262900416838097\n",
      "--------------------------------------------------\n",
      "Step : 42\n",
      "Current State : tensor([[-0.9206,  0.3904, -8.0000]], device='cuda:0')\n",
      "Currrent Action : [0.83815026]\n",
      "Next State : [-0.7108132   0.70338076 -7.5814757 ]\n",
      "Reward : -13.911173325293094\n",
      "--------------------------------------------------\n",
      "Step : 43\n",
      "Current State : tensor([[-0.7108,  0.7034, -7.5815]], device='cuda:0')\n",
      "Currrent Action : [-0.07844482]\n",
      "Next State : [-0.42355704  0.9058694  -7.0657067 ]\n",
      "Reward : -11.324329598416883\n",
      "--------------------------------------------------\n",
      "Step : 44\n",
      "Current State : tensor([[-0.4236,  0.9059, -7.0657]], device='cuda:0')\n",
      "Currrent Action : [-0.5051695]\n",
      "Next State : [-0.11401591  0.99347895 -6.46208   ]\n",
      "Reward : -9.025401773623257\n",
      "--------------------------------------------------\n",
      "Step : 45\n",
      "Current State : tensor([[-0.1140,  0.9935, -6.4621]], device='cuda:0')\n",
      "Currrent Action : [1.1259005]\n",
      "Next State : [ 0.1624172  0.9867222 -5.5480857]\n",
      "Reward : -7.016545063627326\n",
      "--------------------------------------------------\n",
      "Step : 46\n",
      "Current State : tensor([[ 0.1624,  0.9867, -5.5481]], device='cuda:0')\n",
      "Currrent Action : [0.5656394]\n",
      "Next State : [ 0.38877332  0.92133343 -4.7231984 ]\n",
      "Reward : -5.059942099983787\n",
      "--------------------------------------------------\n",
      "Step : 47\n",
      "Current State : tensor([[ 0.3888,  0.9213, -4.7232]], device='cuda:0')\n",
      "Currrent Action : [1.3038114]\n",
      "Next State : [ 0.55730057  0.8303108  -3.8366263 ]\n",
      "Reward : -3.604964137840813\n",
      "--------------------------------------------------\n",
      "Step : 48\n",
      "Current State : tensor([[ 0.5573,  0.8303, -3.8366]], device='cuda:0')\n",
      "Currrent Action : [0.45300785]\n",
      "Next State : [ 0.6804879   0.73275936 -3.1459422 ]\n",
      "Reward : -2.4319193341714187\n",
      "--------------------------------------------------\n",
      "Step : 49\n",
      "Current State : tensor([[ 0.6805,  0.7328, -3.1459]], device='cuda:0')\n",
      "Currrent Action : [1.2766314]\n",
      "Next State : [ 0.763472   0.6458409 -2.404878 ]\n",
      "Reward : -1.6676142404482426\n",
      "--------------------------------------------------\n",
      "Step : 50\n",
      "Current State : tensor([[ 0.7635,  0.6458, -2.4049]], device='cuda:0')\n",
      "Currrent Action : [-1.3684567]\n",
      "Next State : [ 0.8276797  0.5612008 -2.1257658]\n",
      "Reward : -1.0731948324453753\n",
      "--------------------------------------------------\n",
      "Step : 51\n",
      "Current State : tensor([[ 0.8277,  0.5612, -2.1258]], device='cuda:0')\n",
      "Currrent Action : [1.5943406]\n",
      "Next State : [ 0.8665492   0.49909163 -1.465714  ]\n",
      "Reward : -0.8094503463482313\n",
      "--------------------------------------------------\n",
      "Step : 52\n",
      "Current State : tensor([[ 0.8665,  0.4991, -1.4657]], device='cuda:0')\n",
      "Currrent Action : [0.28993773]\n",
      "Next State : [ 0.8914981   0.45302445 -1.0479046 ]\n",
      "Reward : -0.48797454644046345\n",
      "--------------------------------------------------\n",
      "Step : 53\n",
      "Current State : tensor([[ 0.8915,  0.4530, -1.0479]], device='cuda:0')\n",
      "Currrent Action : [0.6883932]\n",
      "Next State : [ 0.9047895   0.42585903 -0.60487735]\n",
      "Reward : -0.33133000092786213\n",
      "--------------------------------------------------\n",
      "Step : 54\n",
      "Current State : tensor([[ 0.9048,  0.4259, -0.6049]], device='cuda:0')\n",
      "Currrent Action : [1.1154027]\n",
      "Next State : [ 0.90729     0.42050558 -0.11817264]\n",
      "Reward : -0.23135356635432616\n",
      "--------------------------------------------------\n",
      "Step : 55\n",
      "Current State : tensor([[ 0.9073,  0.4205, -0.1182]], device='cuda:0')\n",
      "Currrent Action : [1.3091848]\n",
      "Next State : [0.8988396  0.43827775 0.39358425]\n",
      "Reward : -0.1914685939174193\n",
      "--------------------------------------------------\n",
      "Step : 56\n",
      "Current State : tensor([[0.8988, 0.4383, 0.3936]], device='cuda:0')\n",
      "Currrent Action : [-1.6964293]\n",
      "Next State : [0.8883427  0.45918107 0.46782818]\n",
      "Reward : -0.22419580673425485\n",
      "--------------------------------------------------\n",
      "Step : 57\n",
      "Current State : tensor([[0.8883, 0.4592, 0.4678]], device='cuda:0')\n",
      "Currrent Action : [-0.44089776]\n",
      "Next State : [0.8705994  0.49199262 0.7460793 ]\n",
      "Reward : -0.2496794553267442\n",
      "--------------------------------------------------\n",
      "Step : 58\n",
      "Current State : tensor([[0.8706, 0.4920, 0.7461]], device='cuda:0')\n",
      "Currrent Action : [-0.47334337]\n",
      "Next State : [0.84374124 0.5367502  1.0440723 ]\n",
      "Reward : -0.32047125412830857\n",
      "--------------------------------------------------\n",
      "Step : 59\n",
      "Current State : tensor([[0.8437, 0.5368, 1.0441]], device='cuda:0')\n",
      "Currrent Action : [-0.42800194]\n",
      "Next State : [0.80465484 0.59374285 1.3824346 ]\n",
      "Reward : -0.43020554258320187\n",
      "--------------------------------------------------\n",
      "Step : 60\n",
      "Current State : tensor([[0.8047, 0.5937, 1.3824]], device='cuda:0')\n",
      "Currrent Action : [-1.4688331]\n",
      "Next State : [0.75438917 0.65642744 1.6074167 ]\n",
      "Reward : -0.5973875686380469\n",
      "--------------------------------------------------\n",
      "Step : 61\n",
      "Current State : tensor([[0.7544, 0.6564, 1.6074]], device='cuda:0')\n",
      "Currrent Action : [-0.7896741]\n",
      "Next State : [0.6857683 0.7278199 1.9812863]\n",
      "Reward : -0.7717633559973819\n",
      "--------------------------------------------------\n",
      "Step : 62\n",
      "Current State : tensor([[0.6858, 0.7278, 1.9813]], device='cuda:0')\n",
      "Currrent Action : [0.8364333]\n",
      "Next State : [0.58349687 0.8121154  2.6526163 ]\n",
      "Reward : -1.0576983868430418\n",
      "--------------------------------------------------\n",
      "Step : 63\n",
      "Current State : tensor([[0.5835, 0.8121, 2.6526]], device='cuda:0')\n",
      "Currrent Action : [-0.9840945]\n",
      "Next State : [0.4504984 0.8927772 3.1140885]\n",
      "Reward : -1.602870600321956\n",
      "--------------------------------------------------\n",
      "Step : 64\n",
      "Current State : tensor([[0.4505, 0.8928, 3.1141]], device='cuda:0')\n",
      "Currrent Action : [-1.5767025]\n",
      "Next State : [0.2859189 0.9582538 3.547166 ]\n",
      "Reward : -2.1898929682627593\n",
      "--------------------------------------------------\n",
      "Step : 65\n",
      "Current State : tensor([[0.2859, 0.9583, 3.5472]], device='cuda:0')\n",
      "Currrent Action : [0.21707568]\n",
      "Next State : [0.07497391 0.99718547 4.298418  ]\n",
      "Reward : -2.898814187393875\n",
      "--------------------------------------------------\n",
      "Step : 66\n",
      "Current State : tensor([[0.0750, 0.9972, 4.2984]], device='cuda:0')\n",
      "Currrent Action : [0.49720356]\n",
      "Next State : [-0.18001337  0.98366416  5.1208873 ]\n",
      "Reward : -4.085160833214874\n",
      "--------------------------------------------------\n",
      "Step : 67\n",
      "Current State : tensor([[-0.1800,  0.9837,  5.1209]], device='cuda:0')\n",
      "Currrent Action : [-0.40451893]\n",
      "Next State : [-0.45368668  0.89116126  5.797958  ]\n",
      "Reward : -5.6913030272427365\n",
      "--------------------------------------------------\n",
      "Step : 68\n",
      "Current State : tensor([[-0.4537,  0.8912,  5.7980]], device='cuda:0')\n",
      "Currrent Action : [-1.1228645]\n",
      "Next State : [-0.70738584  0.70682764  6.2978992 ]\n",
      "Reward : -7.531407704336333\n",
      "--------------------------------------------------\n",
      "Step : 69\n",
      "Current State : tensor([[-0.7074,  0.7068,  6.2979]], device='cuda:0')\n",
      "Currrent Action : [0.9627506]\n",
      "Next State : [-0.90628684  0.42266312  6.972432  ]\n",
      "Reward : -9.520792843622566\n",
      "--------------------------------------------------\n",
      "Step : 70\n",
      "Current State : tensor([[-0.9063,  0.4227,  6.9724]], device='cuda:0')\n",
      "Currrent Action : [-1.175266]\n",
      "Next State : [-0.9967435   0.08063719  7.1131396 ]\n",
      "Reward : -12.181028170716985\n",
      "--------------------------------------------------\n",
      "Step : 71\n",
      "Current State : tensor([[-0.9967,  0.0806,  7.1131]], device='cuda:0')\n",
      "Currrent Action : [1.5656381]\n",
      "Next State : [-0.9583301  -0.28566316  7.4084635 ]\n",
      "Reward : -14.431038806110658\n",
      "--------------------------------------------------\n",
      "Step : 72\n",
      "Current State : tensor([[-0.9583, -0.2857,  7.4085]], device='cuda:0')\n",
      "Currrent Action : [-0.48487818]\n",
      "Next State : [-0.79863507 -0.6018156   7.1214843 ]\n",
      "Reward : -13.622069296386856\n",
      "--------------------------------------------------\n",
      "Step : 73\n",
      "Current State : tensor([[-0.7986, -0.6018,  7.1215]], device='cuda:0')\n",
      "Currrent Action : [0.4435342]\n",
      "Next State : [-0.5548571 -0.8319457  6.736653 ]\n",
      "Reward : -11.300868584424256\n",
      "--------------------------------------------------\n",
      "Step : 74\n",
      "Current State : tensor([[-0.5549, -0.8319,  6.7367]], device='cuda:0')\n",
      "Currrent Action : [0.37184882]\n",
      "Next State : [-0.2761323  -0.96111965  6.168471  ]\n",
      "Reward : -9.1996141568715\n",
      "--------------------------------------------------\n",
      "Step : 75\n",
      "Current State : tensor([[-0.2761, -0.9611,  6.1685]], device='cuda:0')\n",
      "Currrent Action : [0.24387287]\n",
      "Next State : [-0.00555697 -0.99998456  5.484212  ]\n",
      "Reward : -7.229649501971687\n",
      "--------------------------------------------------\n",
      "Step : 76\n",
      "Current State : tensor([[-0.0056, -1.0000,  5.4842]], device='cuda:0')\n",
      "Currrent Action : [0.04314092]\n",
      "Next State : [ 0.2294161 -0.9733285  4.7406945]\n",
      "Reward : -5.49254978411687\n",
      "--------------------------------------------------\n",
      "Step : 77\n",
      "Current State : tensor([[ 0.2294, -0.9733,  4.7407]], device='cuda:0')\n",
      "Currrent Action : [1.5766869]\n",
      "Next State : [ 0.42940858 -0.9031103   4.2472014 ]\n",
      "Reward : -4.04367885967965\n",
      "--------------------------------------------------\n",
      "Step : 78\n",
      "Current State : tensor([[ 0.4294, -0.9031,  4.2472]], device='cuda:0')\n",
      "Currrent Action : [-0.794158]\n",
      "Next State : [ 0.5780811 -0.8159793  3.4507449]\n",
      "Reward : -3.0745381713340465\n",
      "--------------------------------------------------\n",
      "Step : 79\n",
      "Current State : tensor([[ 0.5781, -0.8160,  3.4507]], device='cuda:0')\n",
      "Currrent Action : [0.38443696]\n",
      "Next State : [ 0.6897881 -0.7240113  2.896426 ]\n",
      "Reward : -2.101831834840924\n",
      "--------------------------------------------------\n",
      "Step : 80\n",
      "Current State : tensor([[ 0.6898, -0.7240,  2.8964]], device='cuda:0')\n",
      "Currrent Action : [1.0680416]\n",
      "Next State : [ 0.7751027  -0.63183534  2.5136237 ]\n",
      "Reward : -1.495521190676668\n",
      "--------------------------------------------------\n",
      "Step : 81\n",
      "Current State : tensor([[ 0.7751, -0.6318,  2.5136]], device='cuda:0')\n",
      "Currrent Action : [0.12214999]\n",
      "Next State : [ 0.8359058 -0.5488729  2.0580697]\n",
      "Reward : -1.0995902692421435\n",
      "--------------------------------------------------\n",
      "Step : 82\n",
      "Current State : tensor([[ 0.8359, -0.5489,  2.0581]], device='cuda:0')\n",
      "Currrent Action : [0.73629653]\n",
      "Next State : [ 0.8808355  -0.47342256  1.7568595 ]\n",
      "Reward : -0.7616860248031867\n",
      "--------------------------------------------------\n",
      "Step : 83\n",
      "Current State : tensor([[ 0.8808, -0.4734,  1.7569]], device='cuda:0')\n",
      "Currrent Action : [-0.38373065]\n",
      "Next State : [ 0.9106422 -0.4131958  1.344233 ]\n",
      "Reward : -0.5520217379600453\n",
      "--------------------------------------------------\n",
      "Step : 84\n",
      "Current State : tensor([[ 0.9106, -0.4132,  1.3442]], device='cuda:0')\n",
      "Currrent Action : [1.1296777]\n",
      "Next State : [ 0.9338482  -0.35766965  1.2037878 ]\n",
      "Reward : -0.3634149098248294\n",
      "--------------------------------------------------\n",
      "Step : 85\n",
      "Current State : tensor([[ 0.9338, -0.3577,  1.2038]], device='cuda:0')\n",
      "Currrent Action : [0.28353232]\n",
      "Next State : [ 0.950216  -0.311592   0.9780655]\n",
      "Reward : -0.2787795342053311\n",
      "--------------------------------------------------\n",
      "Step : 86\n",
      "Current State : tensor([[ 0.9502, -0.3116,  0.9781]], device='cuda:0')\n",
      "Currrent Action : [-0.9982807]\n",
      "Next State : [ 0.95905876 -0.28320715  0.59462935]\n",
      "Reward : -0.19706309844760722\n",
      "--------------------------------------------------\n",
      "Step : 87\n",
      "Current State : tensor([[ 0.9591, -0.2832,  0.5946]], device='cuda:0')\n",
      "Currrent Action : [0.5303571]\n",
      "Next State : [ 0.9653415  -0.26099005  0.46177754]\n",
      "Reward : -0.11808707259810805\n",
      "--------------------------------------------------\n",
      "Step : 88\n",
      "Current State : tensor([[ 0.9653, -0.2610,  0.4618]], device='cuda:0')\n",
      "Currrent Action : [-1.3210869]\n",
      "Next State : [ 0.96622163 -0.25771257  0.06787198]\n",
      "Reward : -0.09279028484584337\n",
      "--------------------------------------------------\n",
      "Step : 89\n",
      "Current State : tensor([[ 0.9662, -0.2577,  0.0679]], device='cuda:0')\n",
      "Currrent Action : [-2.0954485]\n",
      "Next State : [ 0.96052176 -0.27820486 -0.42541245]\n",
      "Reward : -0.07240119749764415\n",
      "--------------------------------------------------\n",
      "Step : 90\n",
      "Current State : tensor([[ 0.9605, -0.2782, -0.4254]], device='cuda:0')\n",
      "Currrent Action : [-0.07309373]\n",
      "Next State : [ 0.9510513  -0.3090331  -0.64503014]\n",
      "Reward : -0.09758444242769082\n",
      "--------------------------------------------------\n",
      "Step : 91\n",
      "Current State : tensor([[ 0.9511, -0.3090, -0.6450]], device='cuda:0')\n",
      "Currrent Action : [-1.5738068]\n",
      "Next State : [ 0.9323924 -0.3614476 -1.1128759]\n",
      "Reward : -0.14278993383609231\n",
      "--------------------------------------------------\n",
      "Step : 92\n",
      "Current State : tensor([[ 0.9324, -0.3614, -1.1129]], device='cuda:0')\n",
      "Currrent Action : [0.2069053]\n",
      "Next State : [ 0.905828  -0.4236457 -1.3529259]\n",
      "Reward : -0.2606589271942842\n",
      "--------------------------------------------------\n",
      "Step : 93\n",
      "Current State : tensor([[ 0.9058, -0.4236, -1.3529]], device='cuda:0')\n",
      "Currrent Action : [-1.0156159]\n",
      "Next State : [ 0.8635057 -0.5043391 -1.8230026]\n",
      "Reward : -0.37544906639471803\n",
      "--------------------------------------------------\n",
      "Step : 94\n",
      "Current State : tensor([[ 0.8635, -0.5043, -1.8230]], device='cuda:0')\n",
      "Currrent Action : [1.0179951]\n",
      "Next State : [ 0.80741185 -0.58998823 -2.0485575 ]\n",
      "Reward : -0.6128054537562402\n",
      "--------------------------------------------------\n",
      "Step : 95\n",
      "Current State : tensor([[ 0.8074, -0.5900, -2.0486]], device='cuda:0')\n",
      "Currrent Action : [2.2833889]\n",
      "Next State : [ 0.7380661  -0.67472845 -2.1910489 ]\n",
      "Reward : -0.8218757194776904\n",
      "--------------------------------------------------\n",
      "Step : 96\n",
      "Current State : tensor([[ 0.7381, -0.6747, -2.1910]], device='cuda:0')\n",
      "Currrent Action : [0.29320765]\n",
      "Next State : [ 0.6423372  -0.76642215 -2.653114  ]\n",
      "Reward : -1.0286389647436334\n",
      "--------------------------------------------------\n",
      "Step : 97\n",
      "Current State : tensor([[ 0.6423, -0.7664, -2.6531]], device='cuda:0')\n",
      "Currrent Action : [0.11653201]\n",
      "Next State : [ 0.5115789 -0.8592363 -3.210451 ]\n",
      "Reward : -1.466484764134062\n",
      "--------------------------------------------------\n",
      "Step : 98\n",
      "Current State : tensor([[ 0.5116, -0.8592, -3.2105]], device='cuda:0')\n",
      "Currrent Action : [-0.08801401]\n",
      "Next State : [ 0.33689517 -0.94154215 -3.8680801 ]\n",
      "Reward : -2.099397856990998\n",
      "--------------------------------------------------\n",
      "Step : 99\n",
      "Current State : tensor([[ 0.3369, -0.9415, -3.8681]], device='cuda:0')\n",
      "Currrent Action : [0.08148742]\n",
      "Next State : [ 0.11525993 -0.99333537 -4.5620136 ]\n",
      "Reward : -3.002179251977231\n",
      "-389.9902728112873\n"
     ]
    }
   ],
   "source": [
    "# This is how the environment works \n",
    "final_reward = test_env(\n",
    "    env,\n",
    "    model,\n",
    "    vis=True\n",
    ")\n",
    "\n",
    "print(final_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "be8becc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for a in range(5,0,-1):\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b5727f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
