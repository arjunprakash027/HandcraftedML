{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10ff97e1",
   "metadata": {},
   "source": [
    "## Import everything that we need here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b3d2a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e2d2117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aadf0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Using cuda\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "print(use_cuda)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75989e7",
   "metadata": {},
   "source": [
    "## Create Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70d4b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.multiprocessing_env import SubprocVecEnv\n",
    "\n",
    "num_envs = 16\n",
    "env_name = \"Pendulum-v1\"\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name, render_mode=\"human\")\n",
    "        return env \n",
    "    return _thunk()\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "#envs = SubprocVecEnv(envs)\n",
    "\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6205948b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PassiveEnvChecker<PendulumEnv<Pendulum-v1>>>>>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86916544",
   "metadata": {},
   "source": [
    "## Defining the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85073036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    # print(f\"This is m : {m}\")\n",
    "    # print(f\"This is the type of m : {type(m)}\")\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_inputs,\n",
    "            num_outputs,\n",
    "            hidden_size,\n",
    "            std=0.0\n",
    "    ):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_outputs),\n",
    "        )\n",
    "\n",
    "        self.log_std = nn.Parameter(torch.ones(1, num_outputs) * std)\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x\n",
    "    ):\n",
    "        value = self.critic(x)\n",
    "        mu = self.actor(x)\n",
    "        std = self.log_std.exp().expand_as(mu)\n",
    "        dist = Normal(mu, std)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4d1ddfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1\n",
      "log_std: torch.Size([1, 1])\n",
      "critic.0.weight: torch.Size([256, 3])\n",
      "critic.0.bias: torch.Size([256])\n",
      "critic.2.weight: torch.Size([1, 256])\n",
      "critic.2.bias: torch.Size([1])\n",
      "actor.0.weight: torch.Size([256, 3])\n",
      "actor.0.bias: torch.Size([256])\n",
      "actor.2.weight: torch.Size([1, 256])\n",
      "actor.2.bias: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "num_input = env.observation_space.shape[0]\n",
    "num_output = env.action_space.shape[0]\n",
    "\n",
    "print(num_input, num_output)\n",
    "\n",
    "hidden_size      = 256\n",
    "lr               = 3e-4\n",
    "num_steps        = 20\n",
    "mini_batch_size  = 5\n",
    "ppo_epochs       = 4\n",
    "threshold_reward = -200\n",
    "\n",
    "model = ActorCritic(num_input, num_output, hidden_size).to(device)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4e04b40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_experience(\n",
    "    env,\n",
    "    model,\n",
    "    num_steps\n",
    "):\n",
    "    pass\n",
    "\n",
    "    frames = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    values  = []\n",
    "    masks = []\n",
    "    states = []\n",
    "    log_probs = []\n",
    "    entropy = 0\n",
    "\n",
    "    state, info = env.reset()\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, value = model(state)\n",
    "\n",
    "        action = dist.sample()\n",
    "        next_state, reward, done, _, _ = env.step(action.cpu().numpy()) #if you pass in action.cpy().numpy()[0] we will get a scalar value or single value tuple (3,)\n",
    "\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy += dist.entropy().mean()\n",
    "\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "        masks.append(torch.FloatTensor(1 - int(done)).unsqueeze(1).to(device))\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        \n",
    "        state = np.array(next_state).flatten()\n",
    "\n",
    "        #print('-' * 50)\n",
    "        #print(f\"Step : {step}\")\n",
    "        #print(f\"Current State : {state}\")\n",
    "        #print(f\"Currrent Action : {action}\")\n",
    "        #print(f\"Next State : {next_state}\")\n",
    "        \n",
    "        # The interesting part is how the reward is calculated\n",
    "        # reward is -(angle_cost + velocity_cost + action_cost)\n",
    "\n",
    "        # angle_cost = angle**2                      Penalty for being away from upright (0Â°)\n",
    "        # velocity_cost = 0.1 * angular_velocity**2  Penalty for moving too fast\n",
    "        # action_cost = 0.001 * action**2            Penalty for using large torques\n",
    "        #print(f\"Reward : {reward}\")\n",
    "        \n",
    "        #frame = env.render()\n",
    "        #frames.append(frame)\n",
    "    \n",
    "    return (\n",
    "        frames,\n",
    "        rewards,\n",
    "        actions,\n",
    "        values,\n",
    "        masks,\n",
    "        states,\n",
    "        log_probs,\n",
    "        entropy,\n",
    "        next_state\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "be8becc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalized advtange exstimate is the total advantage that is expected at any particular time. Advtange is the reward that will be used to train the RL network\n",
    "def compute_gae(next_value, \n",
    "            rewards,\n",
    "            values, \n",
    "            masks,\n",
    "            gamma=0.9, \n",
    "            tau=0.95\n",
    "        ):\n",
    "    \n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    \n",
    "    for step in reversed(range(len(rewards))):\n",
    "\n",
    "        if masks[step] == 0:\n",
    "            gae = rewards[step]\n",
    "        else:\n",
    "            gae = rewards[step] + gamma * (tau * gae + (1 - tau) * values[step + 1])\n",
    "\n",
    "        returns.insert(0, gae)\n",
    "    \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "87b5727f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-479.6850]])\n",
      "[tensor([[-6.6119e+29]], grad_fn=<AddBackward0>), tensor([[-7.7332e+29]], grad_fn=<AddBackward0>), tensor([[-9.0447e+29]], grad_fn=<AddBackward0>), tensor([[-7.6660e+29]], grad_fn=<AddBackward0>), tensor([[-8.9661e+29]], grad_fn=<AddBackward0>), tensor([[-1.0487e+30]], grad_fn=<AddBackward0>), tensor([[-9.2554e+29]], grad_fn=<AddBackward0>), tensor([[-7.3635e+29]], grad_fn=<AddBackward0>), tensor([[-5.3169e+29]], grad_fn=<AddBackward0>), tensor([[-6.2186e+29]], grad_fn=<AddBackward0>), tensor([[-7.2732e+29]], grad_fn=<AddBackward0>), tensor([[-8.5067e+29]], grad_fn=<AddBackward0>), tensor([[-6.0274e+29]], grad_fn=<AddBackward0>), tensor([[-7.0495e+29]], grad_fn=<AddBackward0>), tensor([[-8.2451e+29]], grad_fn=<AddBackward0>), tensor([[-6.3207e+29]], grad_fn=<AddBackward0>), tensor([[-3.9134e+29]], grad_fn=<AddBackward0>), tensor([[-4.5771e+29]], grad_fn=<AddBackward0>), tensor([[-5.3533e+29]], grad_fn=<AddBackward0>), tensor([[-6.2612e+29]], grad_fn=<AddBackward0>), tensor([[-7.3230e+29]], grad_fn=<AddBackward0>), tensor([[-6.0469e+29]], grad_fn=<AddBackward0>), tensor([[-3.2279e+29]], grad_fn=<AddBackward0>), tensor([[-3.7754e+29]], grad_fn=<AddBackward0>), tensor([[-4.4156e+29]], grad_fn=<AddBackward0>), tensor([[-5.1645e+29]], grad_fn=<AddBackward0>), tensor([[-6.0403e+29]], grad_fn=<AddBackward0>), tensor([[-7.0647e+29]], grad_fn=<AddBackward0>), tensor([[-3.5281e+29]], grad_fn=<AddBackward0>), tensor([[-4.1264e+29]], grad_fn=<AddBackward0>), tensor([[-2.9937e+28]], grad_fn=<AddBackward0>), tensor([[-3.5014e+28]], grad_fn=<AddBackward0>), tensor([[-4.0952e+28]], grad_fn=<AddBackward0>), tensor([[-4.7898e+28]], grad_fn=<AddBackward0>), tensor([[-5.6021e+28]], grad_fn=<AddBackward0>), tensor([[-6.5521e+28]], grad_fn=<AddBackward0>), tensor([[-7.6633e+28]], grad_fn=<AddBackward0>), tensor([[-8.9629e+28]], grad_fn=<AddBackward0>), tensor([[-1.0483e+29]], grad_fn=<AddBackward0>), tensor([[-1.2261e+29]], grad_fn=<AddBackward0>), tensor([[-1.4340e+29]], grad_fn=<AddBackward0>), tensor([[-1.6772e+29]], grad_fn=<AddBackward0>), tensor([[-1.9616e+29]], grad_fn=<AddBackward0>), tensor([[-2.2943e+29]], grad_fn=<AddBackward0>), tensor([[-2.6834e+29]], grad_fn=<AddBackward0>), tensor([[-3.1385e+29]], grad_fn=<AddBackward0>), tensor([[-3.6707e+29]], grad_fn=<AddBackward0>), tensor([[-4.2933e+29]], grad_fn=<AddBackward0>), tensor([[-5.0214e+29]], grad_fn=<AddBackward0>), tensor([[-5.8729e+29]], grad_fn=<AddBackward0>), tensor([[-6.8689e+29]], grad_fn=<AddBackward0>), tensor([[-3.8536e+29]], grad_fn=<AddBackward0>), tensor([[-4.5071e+29]], grad_fn=<AddBackward0>), tensor([[-5.2715e+29]], grad_fn=<AddBackward0>), tensor([[-2.8466e+29]], grad_fn=<AddBackward0>), tensor([[-3.3294e+29]], grad_fn=<AddBackward0>), tensor([[-3.8940e+29]], grad_fn=<AddBackward0>), tensor([[-4.5544e+29]], grad_fn=<AddBackward0>), tensor([[-5.3268e+29]], grad_fn=<AddBackward0>), tensor([[-6.2302e+29]], grad_fn=<AddBackward0>), tensor([[-7.2868e+29]], grad_fn=<AddBackward0>), tensor([[-8.5226e+29]], grad_fn=<AddBackward0>), tensor([[-5.4450e+29]], grad_fn=<AddBackward0>), tensor([[-2.7319e+29]], grad_fn=<AddBackward0>), tensor([[-1.6905e+29]], grad_fn=<AddBackward0>), tensor([[-1.9772e+29]], grad_fn=<AddBackward0>), tensor([[-2.3125e+29]], grad_fn=<AddBackward0>), tensor([[-2.7047e+29]], grad_fn=<AddBackward0>), tensor([[-3.1634e+29]], grad_fn=<AddBackward0>), tensor([[-3.6999e+29]], grad_fn=<AddBackward0>), tensor([[-4.3273e+29]], grad_fn=<AddBackward0>), tensor([[-5.0612e+29]], grad_fn=<AddBackward0>), tensor([[-5.9195e+29]], grad_fn=<AddBackward0>), tensor([[-6.9234e+29]], grad_fn=<AddBackward0>), tensor([[-3.4418e+29]], grad_fn=<AddBackward0>), tensor([[-4.0255e+29]], grad_fn=<AddBackward0>), tensor([[-4.7082e+29]], grad_fn=<AddBackward0>), tensor([[-1.8211e+29]], grad_fn=<AddBackward0>), tensor([[-2.1300e+29]], grad_fn=<AddBackward0>), tensor([[-2.4912e+29]], grad_fn=<AddBackward0>), tensor([[-2.9137e+29]], grad_fn=<AddBackward0>), tensor([[-3.4078e+29]], grad_fn=<AddBackward0>), tensor([[-3.9858e+29]], grad_fn=<AddBackward0>), tensor([[-54.0783]], grad_fn=<AddBackward0>), tensor([[-49.4301]], grad_fn=<AddBackward0>), tensor([[-41.0455]], grad_fn=<AddBackward0>), tensor([[-32.9874]], grad_fn=<AddBackward0>), tensor([[-25.6509]], grad_fn=<AddBackward0>), tensor([[-19.4648]], grad_fn=<AddBackward0>), tensor([[-14.4541]], grad_fn=<AddBackward0>), tensor([[-10.3861]], grad_fn=<AddBackward0>), tensor([[-7.4520]], grad_fn=<AddBackward0>), tensor([[-5.2093]], grad_fn=<AddBackward0>), tensor([[-3.5100]], grad_fn=<AddBackward0>), tensor([[-2.3551]], grad_fn=<AddBackward0>), tensor([[-1.5707]], grad_fn=<AddBackward0>), tensor([[-1.0579]], grad_fn=<AddBackward0>), tensor([[-0.6881]], grad_fn=<AddBackward0>), tensor([[-0.3961]], grad_fn=<AddBackward0>), tensor([[-0.1726]], grad_fn=<AddBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "\n",
    "def test_env(\n",
    "    env,\n",
    "    model,\n",
    "    total_steps = 100,\n",
    "    vis=False,\n",
    "):\n",
    "    \n",
    "    frames, rewards, actions, values, masks, states, log_probs, entropy, next_state = collect_experience(\n",
    "        env,\n",
    "        model,\n",
    "        total_steps\n",
    "    )\n",
    "\n",
    "    next_state = np.array(next_state).flatten()\n",
    "    total_reward = sum(rewards)\n",
    "\n",
    "    next_state = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "    _, next_value = model(next_state)\n",
    "    returns = compute_gae(next_value, rewards, masks, values)\n",
    "\n",
    "    return (\n",
    "        total_reward,\n",
    "        returns\n",
    "    )\n",
    "\n",
    "# This is how the environment works \n",
    "final_reward,returns = test_env(\n",
    "    env,\n",
    "    model,\n",
    "    vis=True\n",
    ")\n",
    "\n",
    "print(final_reward)\n",
    "print(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ed2549",
   "metadata": {},
   "source": [
    "## The real training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "10b81d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_randomizer(\n",
    "    mini_batch_size,\n",
    "    states,\n",
    "    actions,\n",
    "    log_probs,\n",
    "    returns,\n",
    "    advantages\n",
    ") :\n",
    "\n",
    "    batch_size = states.size(0)\n",
    "\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_indexes = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield states[rand_indexes], actions[rand_indexes], log_probs[rand_indexes], returns[rand_indexes], advantages[rand_indexes]\n",
    "\n",
    "def ppo_trainer(\n",
    "    model,\n",
    "    optimizer,\n",
    "    ppo_epochs,\n",
    "    mini_batch_size,\n",
    "    states,\n",
    "    actions,\n",
    "    log_probs,\n",
    "    returns,\n",
    "    advantages,\n",
    "    clip_param = 0.2\n",
    "):\n",
    "\n",
    "    batch_loss = []\n",
    "    for _ in range(ppo_epochs):\n",
    "        for state, action, old_log_probs, return_vals, advantage in ppo_randomizer(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
    "            dist, value = model(state) # Action space and value chosen by the new policy\n",
    "            entropy = dist.entropy().mean() #what is the entropy of my new policy distribution\n",
    "            new_log_probs = dist.log_prob(action) #what is the likelihood that my new policy is to take the old action?\n",
    "\n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "\n",
    "            surrogate_unclipped = ratio * advantage\n",
    "            surrogate_clipped = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
    "\n",
    "            actor_loss = -torch.min(surrogate_unclipped,surrogate_clipped).mean()\n",
    "            critic_loss = (return_vals - value).pow(2).mean()\n",
    "\n",
    "            loss = 0.5 * critic_loss + actor_loss - 0.001* entropy\n",
    "\n",
    "            print(\"Actor loss\",actor_loss)\n",
    "            print(\"Return vals\", return_vals)\n",
    "            print(\"values\", value)\n",
    "            print(\"Critic loss\",critic_loss)\n",
    "            print(\"Loss \",loss)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_loss.append(loss.item())\n",
    "    \n",
    "    return np.mean(batch_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9f196a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_stats(model, iteration):\n",
    "\n",
    "    all_params = []\n",
    "\n",
    "    for param in model.parameters():\n",
    "        all_params.append(param.data.flatten())\n",
    "    \n",
    "    all_params = torch.cat(all_params)\n",
    "\n",
    "    stats = {\n",
    "        'iteration': iteration,\n",
    "        'total_params': all_params.numel(),\n",
    "        'mean': all_params.mean().item(),\n",
    "        'std': all_params.std().item(),\n",
    "        'min': all_params.min().item(),\n",
    "        'max': all_params.max().item(),\n",
    "        'norm': all_params.norm().item()\n",
    "    }\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "07b13817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iteration': 0, 'total_params': 2563, 'mean': 0.020452477037906647, 'std': 0.09886668622493744, 'min': -0.33941397070884705, 'max': 0.33309099078178406, 'norm': 5.11025333404541}\n",
      "learning iteration : 0\n",
      "Next value tensor([[-0.3293]], grad_fn=<AddmmBackward0>)\n",
      "rewards [tensor([[-3.6470]]), tensor([[-3.6726]]), tensor([[-3.9317]])]\n",
      "Masks [tensor([[-2.9315e-19]]), tensor([[-6.6208e+30]]), tensor([[-4.6392e+30]])]\n",
      "values [tensor([[-0.1164]], grad_fn=<AddmmBackward0>), tensor([[-0.1964]], grad_fn=<AddmmBackward0>), tensor([[-0.2807]], grad_fn=<AddmmBackward0>)]\n",
      "Actor loss tensor(4.7643e+29, grad_fn=<NegBackward0>)\n",
      "Return vals tensor([[-4.7643e+29]])\n",
      "values tensor([[-0.1164]], grad_fn=<AddmmBackward0>)\n",
      "Critic loss tensor(inf, grad_fn=<MeanBackward0>)\n",
      "Loss  tensor(inf, grad_fn=<SubBackward0>)\n",
      "Actor loss tensor(2.0876e+29, grad_fn=<NegBackward0>)\n",
      "Return vals tensor([[-2.0876e+29]])\n",
      "values tensor([[-0.1964]], grad_fn=<AddmmBackward0>)\n",
      "Critic loss tensor(inf, grad_fn=<MeanBackward0>)\n",
      "Loss  tensor(inf, grad_fn=<SubBackward0>)\n",
      "Actor loss tensor(2.0876e+29, grad_fn=<NegBackward0>)\n",
      "Return vals tensor([[-2.0876e+29]])\n",
      "values tensor([[-0.1964]], grad_fn=<AddmmBackward0>)\n",
      "Critic loss tensor(inf, grad_fn=<MeanBackward0>)\n",
      "Loss  tensor(inf, grad_fn=<SubBackward0>)\n",
      "learning iteration : 1\n",
      "Next value tensor([[0.3021]], grad_fn=<AddmmBackward0>)\n",
      "rewards [tensor([[-1.2469]]), tensor([[-1.4614]]), tensor([[-1.9252]])]\n",
      "Masks [tensor([[-2.9315e-19]]), tensor([[-4.2991e+30]]), tensor([[-2.9315e-19]])]\n",
      "values [tensor([[0.1533]], grad_fn=<AddmmBackward0>), tensor([[0.1959]], grad_fn=<AddmmBackward0>), tensor([[0.2541]], grad_fn=<AddmmBackward0>)]\n",
      "Actor loss tensor(1.9346e+29, grad_fn=<NegBackward0>)\n",
      "Return vals tensor([[-1.9346e+29]])\n",
      "values tensor([[0.1533]], grad_fn=<AddmmBackward0>)\n",
      "Critic loss tensor(inf, grad_fn=<MeanBackward0>)\n",
      "Loss  tensor(inf, grad_fn=<SubBackward0>)\n",
      "Actor loss tensor(2.1658, grad_fn=<NegBackward0>)\n",
      "Return vals tensor([[-1.9116]])\n",
      "values tensor([[0.2541]], grad_fn=<AddmmBackward0>)\n",
      "Critic loss tensor(4.6905, grad_fn=<MeanBackward0>)\n",
      "Loss  tensor(4.5096, grad_fn=<SubBackward0>)\n",
      "Actor loss tensor(2.1658, grad_fn=<NegBackward0>)\n",
      "Return vals tensor([[-1.9116]])\n",
      "values tensor([[0.2541]], grad_fn=<AddmmBackward0>)\n",
      "Critic loss tensor(4.6905, grad_fn=<MeanBackward0>)\n",
      "Loss  tensor(4.5096, grad_fn=<SubBackward0>)\n",
      "learning iteration : 2\n",
      "Next value tensor([[-0.2562]], grad_fn=<AddmmBackward0>)\n",
      "rewards [tensor([[-1.3406]]), tensor([[-1.2678]]), tensor([[-1.3586]])]\n",
      "Masks [tensor([[-7.0157e+30]]), tensor([[-2.9315e-19]]), tensor([[-2.9315e-19]])]\n",
      "values [tensor([[-0.0801]], grad_fn=<AddmmBackward0>), tensor([[-0.1284]], grad_fn=<AddmmBackward0>), tensor([[-0.2126]], grad_fn=<AddmmBackward0>)]\n",
      "Actor loss tensor(3.3460, grad_fn=<NegBackward0>)\n",
      "Return vals tensor([[-3.4261]])\n",
      "values tensor([[-0.0801]], grad_fn=<AddmmBackward0>)\n",
      "Critic loss tensor(11.1957, grad_fn=<MeanBackward0>)\n",
      "Loss  tensor(8.9425, grad_fn=<SubBackward0>)\n",
      "Actor loss tensor(3.3460, grad_fn=<NegBackward0>)\n",
      "Return vals tensor([[-3.4261]])\n",
      "values tensor([[-0.0801]], grad_fn=<AddmmBackward0>)\n",
      "Critic loss tensor(11.1957, grad_fn=<MeanBackward0>)\n",
      "Loss  tensor(8.9425, grad_fn=<SubBackward0>)\n",
      "Actor loss tensor(3.3460, grad_fn=<NegBackward0>)\n",
      "Return vals tensor([[-3.4261]])\n",
      "values tensor([[-0.0801]], grad_fn=<AddmmBackward0>)\n",
      "Critic loss tensor(11.1957, grad_fn=<MeanBackward0>)\n",
      "Loss  tensor(8.9425, grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "num_inputs  = env.observation_space.shape[0]\n",
    "num_outputs = env.action_space.shape[0]\n",
    "\n",
    "#Hyper params:\n",
    "hidden_size      = 256\n",
    "lr               = 1e-4\n",
    "num_steps        = 100\n",
    "mini_batch_size  = 25\n",
    "ppo_epochs       = 5\n",
    "threshold_reward = -200\n",
    "\n",
    "hidden_size      = 256\n",
    "lr               = 3e-4\n",
    "num_steps        = 3\n",
    "mini_batch_size  = 1\n",
    "ppo_epochs       = 1\n",
    "threshold_reward = -200\n",
    "\n",
    "learning_iterations = 3\n",
    "\n",
    "model = ActorCritic(num_inputs, num_outputs, hidden_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def train_the_model(\n",
    "    env,\n",
    "    model,\n",
    "    num_learning_iterations\n",
    "):\n",
    "\n",
    "    episodic_rewards = []\n",
    "    episodic_losses = []\n",
    "\n",
    "    for it in range(num_learning_iterations):\n",
    "        print(f\"learning iteration : {it}\")\n",
    "\n",
    "        frames, rewards, actions, values, masks, states, log_probs, entropy, next_state = collect_experience(\n",
    "            env=env,\n",
    "            model=model,\n",
    "            num_steps=num_steps\n",
    "        )\n",
    "\n",
    "        episodic_rewards.append(sum(rewards))\n",
    "        \n",
    "        next_state = np.array(next_state).flatten()\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "        _, next_value = model(next_state)\n",
    "\n",
    "        print(\"Next value\", next_value)\n",
    "        print(\"rewards\", rewards)\n",
    "        print(\"Masks\", masks)\n",
    "        print(\"values\", values)\n",
    "        \n",
    "        returns = compute_gae(next_value, rewards, masks, values)\n",
    "\n",
    "        returns = torch.cat(returns).detach()\n",
    "        log_probs = torch.cat(log_probs).detach()\n",
    "        values = torch.cat(values).detach()\n",
    "        states = torch.cat(states)\n",
    "        actions = torch.cat(actions)\n",
    "        advantage = returns - values\n",
    "\n",
    "        loss = ppo_trainer(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            ppo_epochs=ppo_epochs,\n",
    "            mini_batch_size=mini_batch_size,\n",
    "            states=states,\n",
    "            actions=actions,\n",
    "            log_probs=log_probs,\n",
    "            returns=returns,\n",
    "            advantages=advantage\n",
    "        )\n",
    "\n",
    "        episodic_losses.append(loss)\n",
    "\n",
    "    \n",
    "    return episodic_rewards, episodic_losses\n",
    "\n",
    "\n",
    "\n",
    "print(check_model_stats(model=model, iteration=0))\n",
    "\n",
    "rewards,losses = train_the_model(\n",
    "    env=env,\n",
    "    model=model,\n",
    "    num_learning_iterations=learning_iterations\n",
    ")\n",
    "#check_model_stats(model=model, iteration='inf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a4f39479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(inf), np.float64(inf), np.float64(inf)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9b8269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
